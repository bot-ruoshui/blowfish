<!DOCTYPE html>
<html lang="zh-cn" dir="ltr" class="scroll-smooth" data-default-appearance="light"
  data-auto-appearance="true"><head>
  <meta charset="utf-8" />
  
  <meta http-equiv="content-language" content="zh-cn" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title> &middot; KV先生</title>
  <meta name="title" content=" &middot; KV先生" />
  
  
  
  
  
  <link rel="canonical" href="https://your_domain.com/docs/hive/hive/" />
  
  
  
  
  
  
  
  
  
  
  <link type="text/css" rel="stylesheet" href="/css/main.bundle.min.8ea60310059bae0843ca33ef1de0ed63f1367330a68d9b13d82d6c3bc474a4c1a7c705dba2592e367518d4eb2c0a82076592aeab874ff2e02034ddc7147c752f.css"
    integrity="sha512-jqYDEAWbrghDyjPvHeDtY/E2czCmjZsT2C1sO8R0pMGnxwXbolkuNnUY1OssCoIHZZKuq4dP8uAgNN3HFHx1Lw==" />
  
  
  <script type="text/javascript" src="/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js"
    integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj&#43;e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
  
  
  
  
  
  
  
  
  
  <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.b3e22525a7c62c7b8e4c7c002546ab6498031635fa2d07802d593d476d7ab1262ec61b0cec0fe9f5653e1c9e6c256cbd0b66e5a8421b4f82a7bd2c1d5c901638.js"
    integrity="sha512-s&#43;IlJafGLHuOTHwAJUarZJgDFjX6LQeALVk9R216sSYuxhsM7A/p9WU&#43;HJ5sJWy9C2blqEIbT4KnvSwdXJAWOA==" data-copy="" data-copied=""></script>
  
  
  <script src="/js/zoom.min.js"></script>
  
  
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
  <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  <meta property="og:url" content="https://your_domain.com/docs/hive/hive/">
  <meta property="og:site_name" content="KV先生">
  <meta property="og:title" content="KV先生">
  <meta property="og:description" content="Hive 3.">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="KV先生">
  <meta name="twitter:description" content="Hive 3.">

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Docs",
    "name": "",
    "headline": "",
    
    "abstract": "Hive 3.",
    "inLanguage": "zh-cn",
    "url" : "https:\/\/your_domain.com\/docs\/hive\/hive\/",
    "author" : {
      "@type": "Person",
      "name": "KV先生"
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "8804"
  }]
  </script>


  
  
  <meta name="author" content="KV先生" />
  
  
  
  <link href="mailto:hello@your_domain.com" rel="me" />
  
  
  <link href="https://github.com/bot-ruoshui" rel="me" />
  
  
  
  

<script src="/lib/jquery/jquery.slim.min.js" integrity=""></script>





















  
  


  
  
  <meta name="theme-color"/>
  
  
</head>
<body
  class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
  <div id="the-top" class="absolute flex self-center">
    <a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
      href="#main-content"><span
        class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>跳过正文</a>
  </div>
  
  
  <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0 pl-[24px] pr-[24px]" style="z-index:100">
  <div id="menu-blur" class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div>
  <div class="relative max-w-[64rem] ml-auto mr-auto">
    <div style="padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px"
    class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3">
    
    <div class="flex flex-1 items-center justify-between">
        <nav class="flex space-x-3">

            
            <a href="/" class="text-base font-medium text-gray-500 hover:text-gray-900">
	            <span style="font-weight:600;font-size:18px;">KV先生</span>
	        </a>
            

        </nav>
        <nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12">

            
            
            
  <a href="/docs/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        文档
    </p>
</a>



            
            
  <a href="/tags/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        标签
    </p>
</a>



            
            
  <a href="https://github.com/5mrhelloworld"  target="_blank"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <span >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>


    </span>
    
    <p class="text-base font-medium" title="">
        
    </p>
</a>



            
            

            


            
            <button id="search-button" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            


            
            
            <div
                class="ltr:mr-14 rtl:ml-14 flex items-center">
                <button id="appearance-switcher" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400">
                    <div class="flex items-center justify-center dark:hidden">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                    </div>
                    <div class="items-center justify-center hidden dark:flex">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                    </div>
                </button>
            </div>
            

        </nav>
        <div class="flex md:hidden items-center space-x-5 md:ml-12 h-12">

            <span></span>

            


            
            <button id="search-button-mobile" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            

            
            
            <button id="appearance-switcher-mobile" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400" style="margin-right:5px">
                <div class="flex items-center justify-center dark:hidden">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                </div>
                <div class="items-center justify-center hidden dark:flex">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                </div>
            </button>
            

        </div>
    </div>
    <div class="-my-2 -mr-2 md:hidden">

        <label id="menu-button" for="menu-controller" class="block">
            <input type="checkbox" id="menu-controller" class="hidden" />
            
            <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
                

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>

  </span>


            </div>
            <div id="menu-wrapper" style="padding-top:5px;"
                class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50">
                <ul
                    class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl">

                    <li>
                        <span
                            class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>

</span>
                    </li>

                    

                    
  <li class="mt-1">
    <a href="/docs/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            文档
        </p>
    </a>
</li>




                    

                    
  <li class="mt-1">
    <a href="/tags/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            标签
        </p>
    </a>
</li>




                    

                    
  <li class="mt-1">
    <a href="https://github.com/5mrhelloworld"  target="_blank"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <div >
            

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>


        </div>
        
        <p class="text-bg font-bg" title="">
            
        </p>
    </a>
</li>




                    

                </ul>
                
                

            </div>
        </label>
    </div>
</div>





  </div>
</div>
<script>
  window.addEventListener('scroll', function (e) {
    var scroll = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop || 0;
    var background_blur = document.getElementById('menu-blur');
    background_blur.style.opacity = (scroll / 300);
  });
</script>

  
  <div class="relative flex flex-col grow">
    <main id="main-content" class="grow">
      


<article>
  

  <header id="single_header" class="mt-5 max-w-prose">
    
    <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  
  <li class="inline hidden">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/"
      >文章</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  
  <li class="inline ">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/docs/"
      >文章</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  
  <li class="inline hidden">
    <a
      class="hover:underline decoration-neutral-300 dark:underline-neutral-600"
      href="/docs/hive/hive/"
      >文章</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


    
    <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
      
    </h1>
    <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
      





  
  







  





  



  













<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="0001-01-01 00:00:00 &#43;0000 UTC">0001-01-01</time><span class="px-2 text-primary-500">&middot;</span><span>8804 字</span><span class="px-2 text-primary-500">&middot;</span><span title="预计阅读">42 分钟</span>
  

  
  
</div>





<div class="flex flex-row flex-wrap items-center">
  
  
  
  
  
  
  
  
  
  
  
  
  
</div>



    </div>

    
    
    
    
    

    

    
      
      
        
        
<div class="flex author">
  
    
    
      
    
    
      
        
      
      <img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width="96" height="96"
      alt="KV先生" src="/img/wmz_hu0f64dbd4a3f6623dcafe1cf79b5f4bcf_435948_192x192_fill_q75_box_center.jpg" />
    
  
  <div class="place-self-center">
    
    <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
      作者
    </div>
    <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
      KV先生
    </div>
    
    
    <div class="text-sm text-neutral-700 dark:text-neutral-400">rosser</div>
    
    <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="mailto:hello@your_domain.com"
          target="_blank"
          aria-label="Email"
          rel="me noopener noreferrer"
          ><span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>

</span></a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://github.com/bot-ruoshui"
          target="_blank"
          aria-label="Github"
          rel="me noopener noreferrer"
          ><span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>

</span></a
        >
      
    
  </div>

</div>
  </div>
</div>

      

      

      
      <div class="mb-5"></div>
      

    

  </header>
  
  <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
    
     <div
      class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8">
      <div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]">

         <details open id="TOCView"
  class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    目录
  </summary>
  <div
    class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#基本概念">基本概念</a>
      <ul>
        <li><a href="#简介">简介</a></li>
        <li><a href="#hive和数据库">Hive和数据库</a></li>
        <li><a href="#hive优缺点">Hive优缺点</a></li>
        <li><a href="#hive-应用场景">Hive 应用场景</a></li>
      </ul>
    </li>
    <li><a href="#hive架构">Hive架构</a>
      <ul>
        <li><a href="#clicent">Clicent</a></li>
        <li><a href="#metastore">MetaStore</a></li>
        <li><a href="#driver">Driver</a></li>
        <li><a href="#hdfs">HDFS</a></li>
        <li><a href="#hive工作原理">Hive工作原理</a></li>
      </ul>
    </li>
    <li><a href="#hive-安装">Hive 安装</a>
      <ul>
        <li><a href="#安装方式">安装方式</a>
          <ul>
            <li><a href="#内嵌模式">内嵌模式</a></li>
            <li><a href="#本地模式">本地模式</a></li>
            <li><a href="#远程模式">远程模式</a></li>
          </ul>
        </li>
        <li><a href="#目标环境">目标环境</a></li>
        <li><a href="#安装">安装</a>
          <ul>
            <li><a href="#解压">解压</a></li>
            <li><a href="#修改配置文件">修改配置文件</a></li>
            <li><a href="#配置日志组件">配置日志组件</a></li>
            <li><a href="#添加驱动包">添加驱动包</a></li>
            <li><a href="#拷贝至其他节点">拷贝至其他节点</a></li>
            <li><a href="#配置环境变量">配置环境变量</a></li>
          </ul>
        </li>
        <li><a href="#启动">启动</a></li>
        <li><a href="#客户端连接">客户端连接</a></li>
        <li><a href="#关闭">关闭</a></li>
      </ul>
    </li>
    <li><a href="#hive-交互方式">Hive 交互方式</a>
      <ul>
        <li><a href="#第一种">第一种</a></li>
        <li><a href="#第二种">第二种*</a></li>
        <li><a href="#第三种">第三种</a></li>
        <li><a href="#总结">总结</a></li>
      </ul>
    </li>
    <li><a href="#元数据数据类型">元数据/数据类型</a>
      <ul>
        <li><a href="#元数据">元数据</a>
          <ul>
            <li><a href="#版本表">版本表</a></li>
            <li><a href="#数据库相关表">数据库相关表</a></li>
            <li><a href="#视图相关表">视图相关表</a></li>
            <li><a href="#文件存储信息相关表">文件存储信息相关表</a></li>
            <li><a href="#表字段相关表">表字段相关表</a></li>
            <li><a href="#分区相关表">分区相关表</a></li>
            <li><a href="#其他表">其他表</a></li>
          </ul>
        </li>
        <li><a href="#数据类型">数据类型</a>
          <ul>
            <li><a href="#基础数据类型">基础数据类型</a></li>
            <li><a href="#复杂数据类型">复杂数据类型</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#hive基础">Hive基础</a>
      <ul>
        <li><a href="#数据库">数据库</a>
          <ul>
            <li><a href="#创建数据库">创建数据库</a></li>
            <li><a href="#修改数据库">修改数据库</a></li>
            <li><a href="#数据库详情">数据库详情</a></li>
            <li><a href="#删除数据库">删除数据库</a></li>
          </ul>
        </li>
        <li><a href="#数据表">数据表</a>
          <ul>
            <li><a href="#语法">语法</a></li>
            <li><a href="#创建表">创建表</a></li>
            <li><a href="#表详情">表详情</a></li>
            <li><a href="#重命名">重命名</a></li>
            <li><a href="#修改列">修改列</a></li>
            <li><a href="#清空表">清空表</a></li>
            <li><a href="#删除表">删除表</a></li>
          </ul>
        </li>
        <li><a href="#内外部表">内外部表</a>
          <ul>
            <li><a href="#内部表">内部表</a></li>
            <li><a href="#外部表">外部表</a></li>
          </ul>
        </li>
        <li><a href="#载入数据">载入数据</a></li>
        <li><a href="#导出数据">导出数据</a>
          <ul>
            <li><a href="#通过sql操作">通过SQL操作</a></li>
            <li><a href="#通过hdfs操作">通过HDFS操作</a></li>
            <li><a href="#将元数据和数据同时导出">将元数据和数据同时导出</a></li>
          </ul>
        </li>
        <li><a href="#基本查询">基本查询</a></li>
      </ul>
    </li>
    <li><a href="#hive高级">Hive高级</a>
      <ul>
        <li><a href="#分区分桶">分区/分桶</a>
          <ul>
            <li><a href="#数据模型">数据模型</a></li>
            <li><a href="#分区">分区</a></li>
          </ul>
        </li>
        <li><a href="#数据抽样">数据抽样</a></li>
        <li><a href="#事务">事务</a>
          <ul>
            <li><a href="#原因">原因</a></li>
            <li><a href="#实践">实践</a></li>
            <li><a href="#压缩">压缩</a></li>
          </ul>
        </li>
        <li><a href="#索引">索引</a></li>
        <li><a href="#视图物化视图">视图/物化视图</a>
          <ul>
            <li><a href="#视图">视图</a></li>
            <li><a href="#物化视图">物化视图</a></li>
          </ul>
        </li>
        <li><a href="#高级查询">高级查询</a>
          <ul>
            <li><a href="#行转列一行变多行">行转列——一行变多行</a></li>
            <li><a href="#列转行多行变一行">列转行——多行变一行</a></li>
            <li><a href="#url解析">URL解析</a></li>
            <li><a href="#json-解析">JSON 解析</a></li>
          </ul>
        </li>
        <li><a href="#窗口函数">窗口函数</a>
          <ul>
            <li><a href="#定义">定义</a></li>
            <li><a href="#窗口函数语法">窗口函数语法</a></li>
            <li><a href="#基本使用">基本使用</a></li>
            <li><a href="#移动窗口滑动窗口">移动窗口（滑动窗口）</a></li>
            <li><a href="#分析性窗口函数">分析性窗口函数</a></li>
            <li><a href="#取值型窗口函数">取值型窗口函数</a></li>
          </ul>
        </li>
        <li><a href="#自定义函数">自定义函数</a>
          <ul>
            <li><a href="#创建项目">创建项目</a></li>
            <li><a href="#自定义-udf">自定义 UDF</a></li>
            <li><a href="#自定义-udaf">自定义 UDAF</a></li>
            <li><a href="#自定义-udtf">自定义 UDTF</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#案例练习">案例练习</a></li>
    <li><a href="#hive压缩存储">Hive压缩/存储</a>
      <ul>
        <li><a href="#压缩-1">压缩</a></li>
        <li><a href="#存储方式">存储方式</a>
          <ul>
            <li><a href="#oltp-事务处理">OLTP 事务处理</a></li>
            <li><a href="#oltp-分析处理">OLTP 分析处理</a></li>
            <li><a href="#行式存储row-oriented">行式存储（Row-oriented）</a></li>
            <li><a href="#列式存储column-oriented">列式存储（Column-oriented）</a></li>
          </ul>
        </li>
        <li><a href="#存储格式">存储格式</a>
          <ul>
            <li><a href="#text-file">Text File</a></li>
            <li><a href="#sequence-file">Sequence File</a></li>
            <li><a href="#map-file">Map File</a></li>
            <li><a href="#avro-file">Avro File</a></li>
            <li><a href="#rc-file">RC File</a></li>
            <li><a href="#orc-file">ORC File</a></li>
            <li><a href="#parquet-file">Parquet File</a></li>
            <li><a href="#hive-存储实践">Hive 存储实践</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#hive-优化">Hive 优化</a>
      <ul>
        <li><a href="#explain-执行计划">EXPLAIN 执行计划</a>
          <ul>
            <li><a href="#语法-1">语法</a></li>
            <li><a href="#组成部分">组成部分</a></li>
            <li><a href="#剖析">剖析</a></li>
            <li><a href="#实践-1">实践</a></li>
          </ul>
        </li>
        <li><a href="#sql-优化">SQL 优化</a>
          <ul>
            <li><a href="#rbo-优化">RBO 优化</a></li>
            <li><a href="#cbo-优化">CBO 优化</a></li>
            <li><a href="#join-优化">JOIN 优化</a></li>
          </ul>
        </li>
        <li><a href="#数据倾斜">数据倾斜</a>
          <ul>
            <li><a href="#定义-1">定义</a></li>
            <li><a href="#原因-1">原因</a></li>
            <li><a href="#解决">解决</a></li>
          </ul>
        </li>
        <li><a href="#资源优化">资源优化</a>
          <ul>
            <li><a href="#向量化查询">向量化查询</a></li>
            <li><a href="#存储优化">存储优化</a></li>
            <li><a href="#yarn-优化">YARN 优化</a></li>
            <li><a href="#并行执行">并行执行</a></li>
            <li><a href="#jvm-重用">JVM 重用</a></li>
          </ul>
        </li>
        <li><a href="#聚合优化">聚合优化</a></li>
        <li><a href="#job-优化">Job 优化</a></li>
        <li><a href="#其他优化">其他优化</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</details>
<details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden">
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    目录
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#基本概念">基本概念</a>
      <ul>
        <li><a href="#简介">简介</a></li>
        <li><a href="#hive和数据库">Hive和数据库</a></li>
        <li><a href="#hive优缺点">Hive优缺点</a></li>
        <li><a href="#hive-应用场景">Hive 应用场景</a></li>
      </ul>
    </li>
    <li><a href="#hive架构">Hive架构</a>
      <ul>
        <li><a href="#clicent">Clicent</a></li>
        <li><a href="#metastore">MetaStore</a></li>
        <li><a href="#driver">Driver</a></li>
        <li><a href="#hdfs">HDFS</a></li>
        <li><a href="#hive工作原理">Hive工作原理</a></li>
      </ul>
    </li>
    <li><a href="#hive-安装">Hive 安装</a>
      <ul>
        <li><a href="#安装方式">安装方式</a>
          <ul>
            <li><a href="#内嵌模式">内嵌模式</a></li>
            <li><a href="#本地模式">本地模式</a></li>
            <li><a href="#远程模式">远程模式</a></li>
          </ul>
        </li>
        <li><a href="#目标环境">目标环境</a></li>
        <li><a href="#安装">安装</a>
          <ul>
            <li><a href="#解压">解压</a></li>
            <li><a href="#修改配置文件">修改配置文件</a></li>
            <li><a href="#配置日志组件">配置日志组件</a></li>
            <li><a href="#添加驱动包">添加驱动包</a></li>
            <li><a href="#拷贝至其他节点">拷贝至其他节点</a></li>
            <li><a href="#配置环境变量">配置环境变量</a></li>
          </ul>
        </li>
        <li><a href="#启动">启动</a></li>
        <li><a href="#客户端连接">客户端连接</a></li>
        <li><a href="#关闭">关闭</a></li>
      </ul>
    </li>
    <li><a href="#hive-交互方式">Hive 交互方式</a>
      <ul>
        <li><a href="#第一种">第一种</a></li>
        <li><a href="#第二种">第二种*</a></li>
        <li><a href="#第三种">第三种</a></li>
        <li><a href="#总结">总结</a></li>
      </ul>
    </li>
    <li><a href="#元数据数据类型">元数据/数据类型</a>
      <ul>
        <li><a href="#元数据">元数据</a>
          <ul>
            <li><a href="#版本表">版本表</a></li>
            <li><a href="#数据库相关表">数据库相关表</a></li>
            <li><a href="#视图相关表">视图相关表</a></li>
            <li><a href="#文件存储信息相关表">文件存储信息相关表</a></li>
            <li><a href="#表字段相关表">表字段相关表</a></li>
            <li><a href="#分区相关表">分区相关表</a></li>
            <li><a href="#其他表">其他表</a></li>
          </ul>
        </li>
        <li><a href="#数据类型">数据类型</a>
          <ul>
            <li><a href="#基础数据类型">基础数据类型</a></li>
            <li><a href="#复杂数据类型">复杂数据类型</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#hive基础">Hive基础</a>
      <ul>
        <li><a href="#数据库">数据库</a>
          <ul>
            <li><a href="#创建数据库">创建数据库</a></li>
            <li><a href="#修改数据库">修改数据库</a></li>
            <li><a href="#数据库详情">数据库详情</a></li>
            <li><a href="#删除数据库">删除数据库</a></li>
          </ul>
        </li>
        <li><a href="#数据表">数据表</a>
          <ul>
            <li><a href="#语法">语法</a></li>
            <li><a href="#创建表">创建表</a></li>
            <li><a href="#表详情">表详情</a></li>
            <li><a href="#重命名">重命名</a></li>
            <li><a href="#修改列">修改列</a></li>
            <li><a href="#清空表">清空表</a></li>
            <li><a href="#删除表">删除表</a></li>
          </ul>
        </li>
        <li><a href="#内外部表">内外部表</a>
          <ul>
            <li><a href="#内部表">内部表</a></li>
            <li><a href="#外部表">外部表</a></li>
          </ul>
        </li>
        <li><a href="#载入数据">载入数据</a></li>
        <li><a href="#导出数据">导出数据</a>
          <ul>
            <li><a href="#通过sql操作">通过SQL操作</a></li>
            <li><a href="#通过hdfs操作">通过HDFS操作</a></li>
            <li><a href="#将元数据和数据同时导出">将元数据和数据同时导出</a></li>
          </ul>
        </li>
        <li><a href="#基本查询">基本查询</a></li>
      </ul>
    </li>
    <li><a href="#hive高级">Hive高级</a>
      <ul>
        <li><a href="#分区分桶">分区/分桶</a>
          <ul>
            <li><a href="#数据模型">数据模型</a></li>
            <li><a href="#分区">分区</a></li>
          </ul>
        </li>
        <li><a href="#数据抽样">数据抽样</a></li>
        <li><a href="#事务">事务</a>
          <ul>
            <li><a href="#原因">原因</a></li>
            <li><a href="#实践">实践</a></li>
            <li><a href="#压缩">压缩</a></li>
          </ul>
        </li>
        <li><a href="#索引">索引</a></li>
        <li><a href="#视图物化视图">视图/物化视图</a>
          <ul>
            <li><a href="#视图">视图</a></li>
            <li><a href="#物化视图">物化视图</a></li>
          </ul>
        </li>
        <li><a href="#高级查询">高级查询</a>
          <ul>
            <li><a href="#行转列一行变多行">行转列——一行变多行</a></li>
            <li><a href="#列转行多行变一行">列转行——多行变一行</a></li>
            <li><a href="#url解析">URL解析</a></li>
            <li><a href="#json-解析">JSON 解析</a></li>
          </ul>
        </li>
        <li><a href="#窗口函数">窗口函数</a>
          <ul>
            <li><a href="#定义">定义</a></li>
            <li><a href="#窗口函数语法">窗口函数语法</a></li>
            <li><a href="#基本使用">基本使用</a></li>
            <li><a href="#移动窗口滑动窗口">移动窗口（滑动窗口）</a></li>
            <li><a href="#分析性窗口函数">分析性窗口函数</a></li>
            <li><a href="#取值型窗口函数">取值型窗口函数</a></li>
          </ul>
        </li>
        <li><a href="#自定义函数">自定义函数</a>
          <ul>
            <li><a href="#创建项目">创建项目</a></li>
            <li><a href="#自定义-udf">自定义 UDF</a></li>
            <li><a href="#自定义-udaf">自定义 UDAF</a></li>
            <li><a href="#自定义-udtf">自定义 UDTF</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#案例练习">案例练习</a></li>
    <li><a href="#hive压缩存储">Hive压缩/存储</a>
      <ul>
        <li><a href="#压缩-1">压缩</a></li>
        <li><a href="#存储方式">存储方式</a>
          <ul>
            <li><a href="#oltp-事务处理">OLTP 事务处理</a></li>
            <li><a href="#oltp-分析处理">OLTP 分析处理</a></li>
            <li><a href="#行式存储row-oriented">行式存储（Row-oriented）</a></li>
            <li><a href="#列式存储column-oriented">列式存储（Column-oriented）</a></li>
          </ul>
        </li>
        <li><a href="#存储格式">存储格式</a>
          <ul>
            <li><a href="#text-file">Text File</a></li>
            <li><a href="#sequence-file">Sequence File</a></li>
            <li><a href="#map-file">Map File</a></li>
            <li><a href="#avro-file">Avro File</a></li>
            <li><a href="#rc-file">RC File</a></li>
            <li><a href="#orc-file">ORC File</a></li>
            <li><a href="#parquet-file">Parquet File</a></li>
            <li><a href="#hive-存储实践">Hive 存储实践</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#hive-优化">Hive 优化</a>
      <ul>
        <li><a href="#explain-执行计划">EXPLAIN 执行计划</a>
          <ul>
            <li><a href="#语法-1">语法</a></li>
            <li><a href="#组成部分">组成部分</a></li>
            <li><a href="#剖析">剖析</a></li>
            <li><a href="#实践-1">实践</a></li>
          </ul>
        </li>
        <li><a href="#sql-优化">SQL 优化</a>
          <ul>
            <li><a href="#rbo-优化">RBO 优化</a></li>
            <li><a href="#cbo-优化">CBO 优化</a></li>
            <li><a href="#join-优化">JOIN 优化</a></li>
          </ul>
        </li>
        <li><a href="#数据倾斜">数据倾斜</a>
          <ul>
            <li><a href="#定义-1">定义</a></li>
            <li><a href="#原因-1">原因</a></li>
            <li><a href="#解决">解决</a></li>
          </ul>
        </li>
        <li><a href="#资源优化">资源优化</a>
          <ul>
            <li><a href="#向量化查询">向量化查询</a></li>
            <li><a href="#存储优化">存储优化</a></li>
            <li><a href="#yarn-优化">YARN 优化</a></li>
            <li><a href="#并行执行">并行执行</a></li>
            <li><a href="#jvm-重用">JVM 重用</a></li>
          </ul>
        </li>
        <li><a href="#聚合优化">聚合优化</a></li>
        <li><a href="#job-优化">Job 优化</a></li>
        <li><a href="#其他优化">其他优化</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</details>

<script>

  var margin = 200;
  var marginError = 50;

  (function () {
    var $window = $(window);
    var $toc = $('#TOCView');
    var tocHeight = $toc.height();

    function onResize() {
      var windowAndMarginHeight = $window.height() - margin;
      if(tocHeight >= windowAndMarginHeight) {
        $toc.css("overflow-y", "scroll")
        $toc.css("max-height", (windowAndMarginHeight + marginError) + "px")
      } else {
        $toc.css("overflow-y", "hidden")
        $toc.css("max-height", "9999999px")
      }
    }

    $window.on('resize', onResize);
    $(document).ready(onResize);
  })();



</script>
   </div>
      </div>
      

      <div class="min-w-0 min-h-0 max-w-fit">
        
        


        <div class="article-content max-w-prose mb-20">
          

<h1 class="relative group">Hive 3.1.2 
    <div id="hive-312" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive-312" aria-label="锚点">#</a>
    </span>        
    
</h1>
<p>架构_Driver；Hive基础；Hive高级；SQL，优化压缩存储；优化面试重要</p>


<h2 class="relative group">基本概念 
    <div id="%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h3 class="relative group">简介 
    <div id="%E7%AE%80%E4%BB%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%AE%80%E4%BB%8B" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>​	Hive用于解决海量结构化数据的统计分析，Hive是一个构建在Hadoop至上的数据分析工具，没有存储数据的能力，只有使用数据的能力。底层由HDFS来提供数据存储，可以将结构化的数据文件映射为一张数据库表，并且提供类似SQL的查询功能，本质上是将HQL转化成MapReduce程序。Hive是一个将SQL转换为MapReduce程序的工具，是一个MR的客户端。</p>
<p>​	总结：交互方式采用SQL，元数据存储在Derbyy或MySQL，数据存储在HDFS，分析数据底层是现实MR，执行程序运行在YARN上。</p>


<h3 class="relative group">Hive和数据库 
    <div id="hive%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>​	Hive采用了类似SQL的查询语言HQL（Hive Query Language）。</p>
<table>
<thead>
<tr>
<th></th>
<th>Hive</th>
<th>数据库</th>
</tr>
</thead>
<tbody>
<tr>
<td>查询语言</td>
<td>HQL</td>
<td>SQL</td>
</tr>
<tr>
<td>数据存储</td>
<td>HDFS</td>
<td>块设备或本地文件系统</td>
</tr>
<tr>
<td>数据更新</td>
<td>不支持修改和添加</td>
<td>支持修改和添加</td>
</tr>
<tr>
<td>执行方式</td>
<td>MapReduce</td>
<td>Executor</td>
</tr>
<tr>
<td>执行延迟</td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td>可扩展性</td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td>数据规模</td>
<td>大</td>
<td>小</td>
</tr>
<tr>
<td>索引</td>
<td>0.8以后加入围图索引</td>
<td>支持多种索引</td>
</tr>
<tr>
<td>事务</td>
<td>0.14版本后支持事务</td>
<td>支持</td>
</tr>
</tbody>
</table>
<pre tabindex="0"><code>1、查询语言：因为SQL被广泛应用在数据仓库中，因此，针对Hive的特性设计了类SQL语言HQL。
2、数据存储：Hive是建立在Hadoop之上，所有的Hive都存储在HDFS中。而数据库则可以将数据保存在块设备或本地文件系统中
3、数据更新：Hive是针对数据仓库应用设计，数据仓库内容就是读多写少，因此Hive不支持对数据进行修改和添加，所有税局都是在加载时确定好的。数据库中数据可以进行增删改。
4、执行方式：Hive中大多数查询的执行时通过Hadoop提供的MapReduce实现，数据库有自己的执行引擎
5、执行延迟：Hive在查询数据的时候，没有索引需要扫描整个表；MR框架本身就有较高的延迟，因此Hive延迟较高。数据库执行延迟地，但是数据规模小，难以计算处理大规模数据。
6、可扩展性：Hive建立在Hadoop上，因此Hive的可扩展性和Hadoop的可扩展性一致。数据库因为ACID语义的严格限制扩展性非常有限。
7、数据规模：Hive建立在集群上，通过MR计算，可以支持大规模数据，数据库不行。
8、事务：Hive在0.14版本后开始支持事务，前提是文件格式必须为orc，必须分桶，必须显示声明transaction=true
9、索引：3之后禁用。：Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于底层实现是 MapReduce 的原因， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。值得一提的是，Hive 在 0.8 版本后加入了位图索引。Hive 索引原理为：在指定列上建立索引，生成一张索引表（Hive的一张物理表），记录以下三个字段：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。在执行索引字段查询时候，首先额外生成一个 MapReduce Job，根据对索引列的过滤条件，从索引表中过滤出索引列的值对应的 HDFS 文件路径及偏移量，输出到 HDFS 上的一个文件中，然后根据这些文件中的 HDFS 路径和偏移量，筛选原始 Input 文件，生成新的 Split 作为整个 Job 的 Split，达到不用全表扫描的目的。
</code></pre>

<h3 class="relative group">Hive优缺点 
    <div id="hive%E4%BC%98%E7%BC%BA%E7%82%B9" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive%E4%BC%98%E7%BC%BA%E7%82%B9" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>	MR面临学习成本高：复杂的查询逻辑开发难度大，约束Key和Value的类型，自定义排序规则，自定义分区器等等。
Hive操作接口采用类似SQL的语法，提供快速开发的能力，免去了写MR的过程，减少学习成本，功能扩展方便

优点：
操作接口采用类似 SQL 的语法，提供快速开发的能力（简单、容易上手）。
免去了写 MapReduce 的过程，减少开发人员的学习成本。
Hive 的执行延迟比较高，因此 Hive 常用于离线数据分析，对实时性要求不高的场合。
Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。
Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。
集群可自由拓展并且具有良好的容错性，节点出现问题 SQL 仍可完成执行。

缺点：
Hive 的 HQL 表达能力有限，当逻辑需求特别复杂的时候，还是要借助 MapReduce。如迭代式无法表达计算，数据挖掘方面不擅长。
Hive 操作默认基于 MapReduce 引擎，而 MapReduce 引擎与其它的引擎（如 Spark 引擎）相比，特点就是慢、延
迟高、不适合交互式查询，所以 Hive 也有这个缺点（这里讲的是默认引擎，Hive 是可以更改成其他引擎的）。
Hive 自动生成的 MapReduce 作业，通常情况下不够智能化。
Hive 调优比较困难，粒度较粗。
</code></pre>

<h3 class="relative group">Hive 应用场景 
    <div id="hive-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>日志分析：大部分互联网公司使用 Hive 进行日志分析，包括百度、淘宝等。统计网站一个时间段内的 PV、UV；多维度数据分析
海量结构化数据的离线分析。
</code></pre>

<h2 class="relative group">Hive架构 
    <div id="hive%E6%9E%B6%E6%9E%84" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive%E6%9E%B6%E6%9E%84" aria-label="锚点">#</a>
    </span>        
    
</h2>
<p>官网地址：https://cwiki.apache.org/confluence/display/Hive/Design</p>


<h3 class="relative group">Clicent 
    <div id="clicent" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#clicent" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>	Hive 允许 Client 连接的方式有三个 CLI（Hive Shell）、JDBC/ODBC（Java 访问 Hive）、WEBUI（浏览器访问 Hive）。JDBC/ODBC 访问中间件 Thrift 软件框架，跨语言服务开发。DDL DQL DML，整体仿写一套 SQL 语句
</code></pre>

<h3 class="relative group">MetaStore 
    <div id="metastore" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#metastore" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>	元数据，数据的数据。元数据包括表名、表结构信息、表所属的数据库（默认是 default 库）、表的拥有者（权限信息）、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等。
	元数据的存放一般需要借助于其他的数据载体（Derby 或 MySQL），默认存放在自带的 Derby 数据库（单用户局限性）中，推荐使用 MySQL 进行存储。连接数据库需要提供：uri、username、password、driver。
	元数据服务的作用是：客户端连接MetaStore服务，MetaStore服务再去连接MySql数据库来存储元数据。有了MetaStore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySql数据库的用户名和密码，只需要连接MetaStore服务即可
	
	Hive默认将元数据存储在Derby数据库中，但仅支持单线程操作，若有一个用户在操作，其他用户则无法使用，效率低；切换目录后重新进入Hive原来创建的数据库和表会找不到，所以一般用MySql数据库存储元数据。
</code></pre>

<h3 class="relative group">Driver 
    <div id="driver" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#driver" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>1、解析器（SQL Parser）：将SQL字符串转换成抽象的语法树AST，这一步一般用第三方工具完成；对AST进行语法分析，比如表是否存在，字段是否存在，SQL是有有误。
2、编译器（Complier）：将AST编译生成逻辑执行计划
3、优化器（Query Optimizer）：对逻辑执行计划进行优化
4、执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。

Parser：将HQL语句解析成抽象的语法树AST（Abstract Syntax Tree）
Semantic Analyzer：将抽象语法编译成查询块
Logic Plan Generator：将查询块转换成逻辑查询计划
Logic Optimizer：重写逻辑查询计划，优化逻辑执行计划
Physical Plan Gernerator：讲逻辑计划转化成物理计划MR job
Physical Optimizer：选择最佳的Join策略，优化物理执行计划，最后执行。
</code></pre>

<h3 class="relative group">HDFS 
    <div id="hdfs" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hdfs" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>Hive 的数据存储在 HDFS 中，计算由 MapReduce 完成。HDFS 和 MapReduce 是源码级别上的整合，两者结合最佳。解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。
</code></pre>

<h3 class="relative group">Hive工作原理 
    <div id="hive%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>当创建表的时候，需要指定 HDFS 文件路径，表和其文件路径会保存到 MetaStore，从而建立表和数据的映射关系。当
数据加载入表时，根据映射获取到对应的 HDFS 路径，将数据导入。用户输入 SQL 后，Hive 会将其转换成MapReduce 或者 Spark 任务，提交到 YARN 上执行，执行成功将返回结果。
</code></pre>

<h2 class="relative group">Hive 安装 
    <div id="hive-%E5%AE%89%E8%A3%85" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive-%E5%AE%89%E8%A3%85" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h3 class="relative group">安装方式 
    <div id="%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>Hive 环境搭建分为三种，我们搭建远程模式环境：内嵌模式；本地模式；远程模式</p>


<h4 class="relative group">内嵌模式 
    <div id="%E5%86%85%E5%B5%8C%E6%A8%A1%E5%BC%8F" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%86%85%E5%B5%8C%E6%A8%A1%E5%BC%8F" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>内嵌模式使用的是内嵌的 Derby 数据库来存储元数据，不需要额外起 MetaStore 服务。数据库和 MetaStore 服务都嵌入在主 Hive Server 进程中。内嵌模式属于默认方式，配置简单，但是一次只能一个客户端连接，适用于实验，不适用于生产环境。解压 Hive 安装包，启动 bin/hive 即可使用。
缺点：不同路径启动 Hive，每一个 Hive 都拥有一套自己的元数据，无法共享。
</code></pre>

<h4 class="relative group">本地模式 
    <div id="%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>本地模式采用外部数据库来存储元数据，目前支持的数据库有：Oracle、MySQL、PostgreSQL、SQL Server。在这里我们使用 MySQL。本地模式不需要单独起 MetaStore 服务，用的是跟 Hive 在同一个进程里的 MetaStore 服务。也就是说当你启动一个 Hive 服务，里面默认会帮我们启动一个 MetaStore 服务。Hive 会根据 hive.metastore.uris 参数值来判断，如果为空，则为本地模式。
缺点：每启动一次 Hive 服务，都内置启动了一个 MetaStore。耦合度太高
</code></pre>

<h4 class="relative group">远程模式 
    <div id="%E8%BF%9C%E7%A8%8B%E6%A8%A1%E5%BC%8F" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%BF%9C%E7%A8%8B%E6%A8%A1%E5%BC%8F" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	远程模式下，需要单独起 MetaStore 服务，然后每个客户端都在配置文件里配置连接到该 MetaStore 服务。远程模式的 MetaStore 服务和 Hive 运行在不同的进程里。在生产环境中，建议用远程模式来配置 Hive Metastore。在这种情况下，其他依赖 Hive 的软件都可以通过 MetaStore 访问 Hive。
	远程模式下需要配置 hive.metastore.uris 参数来指定 MetaStore 服务运行的机器 IP 和端口，并且需要单独手动启动 MetaStore 服务。HiveServer2 是 Hive 启动的一个服务，客户端可以使用 JDBC/ODBC 协议，通过 IP + Port 的方式对其进行访问，达到并发访问的目的。
</code></pre>

<h3 class="relative group">目标环境 
    <div id="%E7%9B%AE%E6%A0%87%E7%8E%AF%E5%A2%83" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%9B%AE%E6%A0%87%E7%8E%AF%E5%A2%83" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>注意：安装前确认当前集群已经安装了Mysql数据库和HA+YARN</p>
<table>
<thead>
<tr>
<th>节点/功能</th>
<th>MetaStore</th>
<th>HiveServer2</th>
<th>Client</th>
</tr>
</thead>
<tbody>
<tr>
<td>node01</td>
<td>√</td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>node02</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>node03</td>
<td></td>
<td></td>
<td>√</td>
</tr>
</tbody>
</table>


<h3 class="relative group">安装 
    <div id="%E5%AE%89%E8%A3%85" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AE%89%E8%A3%85" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">解压 
    <div id="%E8%A7%A3%E5%8E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A7%A3%E5%8E%8B" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>将准备好的安装包上传至 node01，然后解压：</p>
<pre tabindex="0"><code>[root@node01 ~]# tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/yjx/
[root@node01 ~]# rm apache-hive-3.1.2-bin.tar.gz -rf
</code></pre>

<h4 class="relative group">修改配置文件 
    <div id="%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>修改环境配置脚本文件 hive-env.sh ：</p>
<pre tabindex="0"><code>[root@node01 ~]# cd /opt/yjx/apache-hive-3.1.2-bin/conf/
[root@node01 conf]# cp hive-env.sh.template hive-env.sh
[root@node01 conf]# vim hive-env.sh
</code></pre><p>在文件末尾添加以下内容：</p>
<pre tabindex="0"><code>Hive基于hadoop框架
HADOOP_HOME=/opt/yjx/hadoop-3.3.4/
export HIVE_CONF_DIR=/opt/yjx/apache-hive-3.1.2-bin/conf
export HIVE_AUX_JARS_PATH=/opt/yjx/apache-hive-3.1.2-bin/lib
</code></pre><p>修改配置文件 hive-site.xml ：</p>
<pre tabindex="0"><code>[root@node01 conf]# cp hive-default.xml.template hive-site.xml
[root@node01 conf]# vim hive-site.xml
</code></pre><p>首先删除 configuration 节点中的所有内容，然后再在 configuration 节点中添加以下内容：</p>
<pre tabindex="0"><code>注意：防止 xml 解析注释出问题，用 == 表示 --。实际为：hive --service metastore -p 9083 &amp; | hive --service metastore

&lt;!-- 数据库相关配置 --&gt;
&lt;property&gt;
&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
&lt;value&gt;jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
&lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
&lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
&lt;value&gt;123456&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 自动创建表 --&gt;
&lt;property&gt;
&lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 强制 MetaStore 的 schema 一致性，开启的话会校验在 MetaStore 中存储的信息的版本和 Hive 的 jar 包中的版
本一致性，并且关闭自动 schema 迁移，用户必须手动的升级 Hive 并且迁移 schema。关闭的话只会在版本不一致时给出警
告，默认是 false 不开启 --&gt;
&lt;!-- 元数据校验 --&gt;
&lt;property&gt;
&lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;
&lt;!-- MySQL8 这里一定要设置为 true，不然后面 DROP TABLE 可能会出现卡住的情况 --&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 美化打印数据 --&gt;
&lt;!-- 是否显示表名与列名，默认值为 false --&gt;
&lt;property&gt;
&lt;name&gt;hive.cli.print.header&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 是否显示数据库名，默认值为 false --&gt;
&lt;property&gt;
&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;!-- Hive 数据仓库的位置（HDFS 中的位置） --&gt;
&lt;property&gt;
&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
&lt;value&gt;/hive/warehouse&lt;/value&gt;
&lt;/property&gt;
&lt;!-- HiveServer2 通过 Thrift 访问 MetaStore --&gt;
&lt;!-- 配置 Thrift 服务绑定的服务器地址，默认为 127.0.0.1 --&gt;
&lt;!--
&lt;property&gt;
&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;
&lt;value&gt;127.0.0.1&lt;/value&gt;
&lt;/property&gt;
--&gt;
&lt;!-- 配置 Thrift 服务监听的端口，默认为 10000 --&gt;
&lt;!--
&lt;property&gt;
&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;
&lt;value&gt;10000&lt;/value&gt;
&lt;/property&gt;
--&gt;
&lt;!-- HiveServer2 的 WEBUI --&gt;
&lt;property&gt;
&lt;name&gt;hive.server2.webui.host&lt;/name&gt;
&lt;value&gt;node01&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;hive.server2.webui.port&lt;/name&gt;
&lt;value&gt;10002&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 指定 hive.metastore.uris 的 port，为了启动 MetaStore 服务的时候不用指定端口 --&gt;
&lt;!-- hive ==service metastore -p 9083 &amp; | hive ==service metastore --&gt;
&lt;property&gt;
&lt;name&gt;hive.metastore.uris&lt;/name&gt;
&lt;value&gt;thrift://node01:9083&lt;/value&gt;
&lt;/property&gt;
</code></pre><p>前面提到 Hive 实际上底层跑的仍然是 MapReduce 程序，那么我们需要让它拥有在 Hadoop 上运行的权限，修改
Hadoop 的配置文件 core-site.xml ：</p>
<pre tabindex="0"><code>[root@node01 conf]# vim /opt/yjx/hadoop-3.3.4/etc/hadoop/core-site.xml
</code></pre><p>　在 configuration 节点中末尾处添加以下内容：</p>
<pre tabindex="0"><code>提示：这里的 root 表示安装 Hive 时的用户，实际上是为 Hive 在 Hadoop 上创建了一个代理用户。

&lt;!-- 该参数表示可以通过 httpfs 接口访问 HDFS 的 IP 地址限制 --&gt;
&lt;!-- 配置 root(超级用户) 允许通过 httpfs 方式访问 HDFS 的主机名、域名 --&gt;
&lt;property&gt;
&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 通过 httpfs 接口访问的用户获得的群组身份 --&gt;
&lt;!-- 配置允许通过 httpfs 方式访问的客户端的用户组 --&gt;
&lt;property&gt;
&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h4 class="relative group">配置日志组件 
    <div id="%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97%E7%BB%84%E4%BB%B6" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97%E7%BB%84%E4%BB%B6" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>首先创建日志目录：</p>
<pre tabindex="0"><code>[root@node01 conf]# mkdir /opt/yjx/apache-hive-3.1.2-bin/logs
[root@node01 conf]# cp hive-log4j2.properties.template hive-log4j2.properties
[root@node01 conf]# vim hive-log4j2.properties

将 property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name} 替换为：
property.hive.log.dir = /opt/yjx/apache-hive-3.1.2-bin/logs
</code></pre>

<h4 class="relative group">添加驱动包 
    <div id="%E6%B7%BB%E5%8A%A0%E9%A9%B1%E5%8A%A8%E5%8C%85" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%B7%BB%E5%8A%A0%E9%A9%B1%E5%8A%A8%E5%8C%85" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>将 MySQL 驱动包（注意自己的 MySQL 版本）添加到 Hive 的 lib 目录下：
[root@node01 ~]# mv mysql-connector-java-8.0.18.jar /opt/yjx/apache-hive-3.1.2-bin/lib/
</code></pre>

<h4 class="relative group">拷贝至其他节点 
    <div id="%E6%8B%B7%E8%B4%9D%E8%87%B3%E5%85%B6%E4%BB%96%E8%8A%82%E7%82%B9" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%8B%B7%E8%B4%9D%E8%87%B3%E5%85%B6%E4%BB%96%E8%8A%82%E7%82%B9" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	将 node01 已配置好的 Hive 拷贝至 node02 和 node03。
[root@node02 ~]# scp -r root@node01:/opt/yjx/apache-hive-3.1.2-bin /opt/yjx/
[root@node03 ~]# scp -r root@node01:/opt/yjx/apache-hive-3.1.2-bin /opt/yjx/
# 或者使用分发脚本
[root@node01 ~]# yjxrsync /opt/yjx/apache-hive-3.1.2-bin

	将 node01 修改后的 Hadoop 的 core-stie.xml 配置文件拷贝至 node02 和 node03。
[root@node01 ~]# scp /opt/yjx/hadoop-3.3.4/etc/hadoop/core-site.xml root@node02:/opt/yjx/hadoop-3.3.4/etc/hadoop/
[root@node01 ~]# scp /opt/yjx/hadoop-3.3.4/etc/hadoop/core-site.xml root@node03:/opt/yjx/hadoop-3.3.4/etc/hadoop/
# 或者使用分发脚本
[root@node01 ~]# yjxrsync /opt/yjx/hadoop-3.3.4/etc/hadoop/core-site.xml
</code></pre>

<h4 class="relative group">配置环境变量 
    <div id="%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>三个节点修改环境变量 vim /etc/profile ，在文件末尾添加以下内容：
export HIVE_HOME=/opt/yjx/apache-hive-3.1.2-bin
export PATH=$HIVE_HOME/bin:$PATH

　修改完成后 source /etc/profile 重新加载环境变量。
</code></pre>

<h3 class="relative group">启动 
    <div id="%E5%90%AF%E5%8A%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%90%AF%E5%8A%A8" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>检查 MySQL 服务是否启动。
[root@node01 ~]# systemctl status mysqld

启动 ZooKeeper（三台机器都需要执行）。
zkServer.sh start
zkServer.sh status

启动 HDFS + YARN。
[root@node01 ~]# start-all.sh

启动 JobHistory。
[root@node01 ~]# mapred --daemon start historyserver

初始化 hive 数据库（第一次启动时执行）。
[root@node01 ~]# schematool -dbType mysql -initSchema

启动 MetaStore 服务。
# 前台启动，学习期间推荐使用这种方式
[root@node01 ~]# hive --service metastore
# 后台启动
[root@node01 ~]# nohup hive --service metastore &gt; /dev/null 2&gt;&amp;1 &amp;

启动 HiveServer2 服务。
# 前台启动，学习期间推荐使用这种方式
[root@node01 ~]# hiveserver2
# 后台启动
[root@node01 ~]# nohup hiveserver2 &gt; /dev/null 2&gt;&amp;1 &amp;
</code></pre>

<h3 class="relative group">客户端连接 
    <div id="%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>客户端连接方式一。
[root@node03 ~]# hive
# 退出命令行命令：exit;

客户端连接方式二。
[root@node03 ~]# beeline -u jdbc:hive2://node01:10000 -n root
# 退出命令行命令：!exit 或者 !quit

客户端连接方式三。
方法一：直接执行 beeline -u jdbc:hive2://node01:10000 -n root
方法二：先执行 beeline ，再执行 !connect jdbc:hive2://node01:10000 ，然后输入用户名，这个用户名就是安装 Hadoop 集群的用户名，无密码的话直接回车即可

10000 为 hive.server2.thrift.port 的默认值（Thrift Server 的 TCP 端口默认为 10000）。
</code></pre>

<h3 class="relative group">关闭 
    <div id="%E5%85%B3%E9%97%AD" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%85%B3%E9%97%AD" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>先关闭 HiveServer2 服务和 MetaStore 服务（前台启动的话直接 Ctrl + C 即可）。
再关闭 JobHistory 和 Hadoop。
[root@node01 ~]# mapred --daemon stop historyserver
[root@node01 ~]# stop-all.sh

再关闭 ZooKeeper（三台机器都需要执行）。
zkServer.sh stop
</code></pre>

<h2 class="relative group">Hive 交互方式 
    <div id="hive-%E4%BA%A4%E4%BA%92%E6%96%B9%E5%BC%8F" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive-%E4%BA%A4%E4%BA%92%E6%96%B9%E5%BC%8F" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h3 class="relative group">第一种 
    <div id="%E7%AC%AC%E4%B8%80%E7%A7%8D" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%AC%AC%E4%B8%80%E7%A7%8D" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>default 是默认的数据库，路径为 /hive/warehouse 

[root@node03 ~]# hive
...
hive (default)&gt; SHOW DATABASES;
OK
database_name
default
Time taken: 0.633 seconds, Fetched: 1 row(s)
hive (default)&gt; exit;
</code></pre>

<h3 class="relative group">第二种* 
    <div id="%E7%AC%AC%E4%BA%8C%E7%A7%8D" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%AC%AC%E4%BA%8C%E7%A7%8D" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>10000 为 hive.server2.thrift.port 的默认值（Thrift Server 的 TCP 端口默认为 10000）。

[root@node03 ~]# beeline -u jdbc:hive2://node01:10000 -n root
...
Connecting to jdbc:hive2://node01:10000
Connected to: Apache Hive (version 3.1.2)
Driver: Hive JDBC (version 3.1.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://node01:10000&gt; SHOW DATABASES;
...
+----------------+
| database_name |
+----------------+
| default |
+----------------+
1 row selected (0.066 seconds)
0: jdbc:hive2://node01:10000&gt; !exit
Closing: 0: jdbc:hive2://node01:10000
</code></pre>

<h3 class="relative group">第三种 
    <div id="%E7%AC%AC%E4%B8%89%E7%A7%8D" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%AC%AC%E4%B8%89%E7%A7%8D" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>使用 -e 参数来直接执行 HQL 的语句。
bin/beeline -u &#34;jdbc:hive2://node01:10000/default&#34; hive -e &#34;SHOW DATABASES;&#34;

使用 -f 参数通过指定文本文件来执行 HQL 的语句。
bin/beeline -u &#34;jdbc:hive2://node01:10000/default&#34; hive -f &#34;default.sql&#34;
</code></pre>

<h3 class="relative group">总结 
    <div id="%E6%80%BB%E7%BB%93" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%80%BB%E7%BB%93" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>	MetaStore服务实际上就是一种Thrift服务，通过它我们可以获取到Hive元数据。通过Thriftf服务获取元数据的方式屏蔽了数据库访问需要驱动，URL，用户名，密码等细节
	HiveSver2是一个服务端接口，是远程客户端可以执行对Hive的查询并返回结果。一般来讲，我们认为HS2是用来提交查询的，而MS才是真正用来访问元数据的，所以推荐使用第二种，这种方式更加安全或合理。
	
	提示：Hive 4.0.0 版本开始 hive shell 也改为了 beeline 的连接方式。
</code></pre>

<h2 class="relative group">元数据/数据类型 
    <div id="%E5%85%83%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h3 class="relative group">元数据 
    <div id="%E5%85%83%E6%95%B0%E6%8D%AE" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%85%83%E6%95%B0%E6%8D%AE" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">版本表 
    <div id="%E7%89%88%E6%9C%AC%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%89%88%E6%9C%AC%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h4 class="relative group">数据库相关表 
    <div id="%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%B8%E5%85%B3%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%B8%E5%85%B3%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h4 class="relative group">视图相关表 
    <div id="%E8%A7%86%E5%9B%BE%E7%9B%B8%E5%85%B3%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A7%86%E5%9B%BE%E7%9B%B8%E5%85%B3%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h4 class="relative group">文件存储信息相关表 
    <div id="%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E4%BF%A1%E6%81%AF%E7%9B%B8%E5%85%B3%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E4%BF%A1%E6%81%AF%E7%9B%B8%E5%85%B3%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h4 class="relative group">表字段相关表 
    <div id="%E8%A1%A8%E5%AD%97%E6%AE%B5%E7%9B%B8%E5%85%B3%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A1%A8%E5%AD%97%E6%AE%B5%E7%9B%B8%E5%85%B3%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h4 class="relative group">分区相关表 
    <div id="%E5%88%86%E5%8C%BA%E7%9B%B8%E5%85%B3%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%86%E5%8C%BA%E7%9B%B8%E5%85%B3%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h4 class="relative group">其他表 
    <div id="%E5%85%B6%E4%BB%96%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%85%B6%E4%BB%96%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h3 class="relative group">数据类型 
    <div id="%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">基础数据类型 
    <div id="%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h4 class="relative group">复杂数据类型 
    <div id="%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h2 class="relative group">Hive基础 
    <div id="hive%E5%9F%BA%E7%A1%80" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive%E5%9F%BA%E7%A1%80" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h3 class="relative group">数据库 
    <div id="%E6%95%B0%E6%8D%AE%E5%BA%93" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%95%B0%E6%8D%AE%E5%BA%93" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">创建数据库 
    <div id="%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>创建一个数据库，数据库在 HDFS 上默认的存储路径是 /hive/warehouse/*.db 
CREATE DATABASE shop;

避免要创建的数据库已经存在错误，可以使用 IF NOT EXISTS 选项来进行判断。（标准写法）
CREATE DATABASE IF NOT EXISTS crm;

指定数据库创建的位置（数据库在 HDFS 上的存储路径）。
CREATE DATABASE IF NOT EXISTS school location &#39;/hive/school.db&#39;;
</code></pre>

<h4 class="relative group">修改数据库 
    <div id="%E4%BF%AE%E6%94%B9%E6%95%B0%E6%8D%AE%E5%BA%93" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E4%BF%AE%E6%94%B9%E6%95%B0%E6%8D%AE%E5%BA%93" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。
ALTER DATABASE school SET DBPROPERTIES(&#39;createtime&#39;=&#39;20220803&#39;);
</code></pre>

<h4 class="relative group">数据库详情 
    <div id="%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%A6%E6%83%85" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%A6%E6%83%85" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>显示所有数据库。
SHOW DATABASES;

可以通过 like 进行过滤。
SHOW DATABASES LIKE &#39;s*&#39;;

查看某个数据库的详情
DESC DATABASE school;
DESCRIBE DATABASE school;

切换数据库。
USE school;
</code></pre>

<h4 class="relative group">删除数据库 
    <div id="%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE%E5%BA%93" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE%E5%BA%93" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>最简写法。
DROP DATABASE school;

如果删除的数据库不存在，最好使用 IF EXISTS 判断数据库是否存在。否则会报错： FAILED:
SemanticException [Error 10072]: Database does not exist: school 。
DROP DATABASE IF EXISTS school;

如果数据库不为空，使用 CASCADE 命令进行强制删除。否则会报错： FAILED: Execution Error, return code
40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. InvalidOperationException(message:Databaseschool is not empty. One or more tables exist.)
DROP DATABASE IF EXISTS school CASCADE
</code></pre>

<h3 class="relative group">数据表 
    <div id="%E6%95%B0%E6%8D%AE%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%95%B0%E6%8D%AE%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">语法 
    <div id="%E8%AF%AD%E6%B3%95" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%AF%AD%E6%B3%95" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
 [(col_name data_type [column_specification] [COMMENT col_comment], ... [constraint_specification])]
 [COMMENT table_comment]
 [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
 [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_bucketsBUCKETS]
 [SKEWED BY (col_name, col_name, ...)
  ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)
  [STORED AS DIRECTORIES]
 [
 [ROW FORMAT row_format]
 [STORED AS file_format]
  | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)]
 ]
 [LOCATION hdfs_path]
 [TBLPROPERTIES (property_name=property_value, ...)]
 [AS select_statement];
 
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
 LIKE existing_table_or_view_name
 [LOCATION hdfs_path];
data_type
 : primitive_type
 | array_type
 | map_type
 | struct_type
 | union_type
 
primitive_type
 : TINYINT
 | SMALLINT
 | INT
 | BIGINT
 | BOOLEAN
 | FLOAT
 | DOUBLE
 | DOUBLE PRECISION
 | STRING
 | BINARY
 | TIMESTAMP
 | DECIMAL
 | DECIMAL(precision, scale)
 | DATE
 | VARCHAR
 | CHAR
 
array_type
 : ARRAY &lt; data_type &gt;
 
map_type
 : MAP &lt; primitive_type, data_type &gt;
struct_type
 : STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;
 
union_type
 : UNIONTYPE &lt; data_type, data_type, ... &gt; 
 
row_format
 : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]
    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
    [NULL DEFINED AS char]
 | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,
property_name=property_value, ...)]

file_format:
 : SEQUENCEFILE
 | TEXTFILE   -- (Default, depending on hive.default.fileformat configuration)
 | RCFILE    -- (Note: Available in Hive 0.6.0 and later)
 | ORC     -- (Note: Available in Hive 0.11.0 and later)
 | PARQUET   -- (Note: Available in Hive 0.13.0 and later)
 | AVRO     -- (Note: Available in Hive 0.14.0 and later)
 | JSONFILE   -- (Note: Available in Hive 4.0.0 and later)
 | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname
column_constraint_specification:
 : [ PRIMARY KEY|UNIQUE|NOT NULL|DEFAULT [default_value]|CHECK  [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY ]

default_value:
 : [ LITERAL|CURRENT_USER()|CURRENT_DATE()|CURRENT_TIMESTAMP()|NULL ]
 
constraint_specification:
 : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]
  [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]
  [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...)
DISABLE NOVALIDATE
  [, CONSTRAINT constraint_name UNIQUE (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]
  [, CONSTRAINT constraint_name CHECK [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY ]
</code></pre>

<h4 class="relative group">创建表 
    <div id="%E5%88%9B%E5%BB%BA%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
[CLUSTERED BY (col_name, col_name, ...)
[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
[ROW FORMAT row_format]
[STORED AS file_format]
[LOCATION hdfs_path]
[TBLPROPERTIER(property_name=property_value,...)]

字段解释说明：
CREATE TABLE：创建一个指定名称的表。如果表名称已经存在，抛出异常；使用IF NOT EXISTS选项进行判断。
EXTERNAL：创建一个外部表，在建表的同时指向实际数据的路径(LOCATION)。
	创建内部表时，会将数据移动到数据仓库指向的路径（默认位置）
	创建外部表：只记录数据所在路径，不对数据的位置做任何改变。
	删除表的时候：内部表的元数据会被一起删除，外部表只删除元数据，不删除数据。
COMMENT：为表和列添加注释
PARTITIONED BY：创建分区表
CLUSTERED BY：创建分桶表
SORTED BY：排序方式，不常用
ROW FORMAT ： DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]
[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH
SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 
	用户在建表的时候可以自定义SerDe或者使用自带的SerD。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED,将会使用自带的SerD（默认是LazySimpleSerDe类，只支持单字节分隔符）。
	在建表时，需要为表指定列，在指定表的列时也会自定义SerDe，Hive通过SerDe确定表的具体的列数据
	SerDe是Serilize/Deserilize的简称，用于序列化和反序列化。
STORED AS：指定存储文件类型，常用的存储文件类型有：SEQUENCEFILE——二进制文件；TEXTFILE——文本；
RCFILE——列式存储格式文件，如果文件数据是纯文本，可以用STORED AS TEXTFILE。如果诗句需要压缩，使用STORED AS SEQUENCEFILE
LOCATION：指定表在HDFS上的存储位置
LIKE：允许用户复制现有表结构，不复制数据
</code></pre>

<h5 class="relative group">案例一 
    <div id="%E6%A1%88%E4%BE%8B%E4%B8%80" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%A1%88%E4%BE%8B%E4%B8%80" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>已知一些简单的用户信息如下，将以下数据保存在 user.txt 文件中并上传至 HDFS（例如 /yjx/user.txt）：
1,admin,123456,男,18
2,zhangsan,abc123,男,23
3,lisi,654321,女,16

根据以上数据结构创建 t_user 表。
CREATE DATABASE IF NOT EXISTS test;
CREATE TABLE IF NOT EXISTS test.t_user (
id int,
username string,
password string,
gender string,
age int
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;;

最后执行以下语句将 user.txt 文件的数据载入 t_user 表。执行 LOAD DATA 语句后 user.txt 文件将会被移动
到 t_user 表所在路径下（ /hive/warehouse/t_user/user.txt ）。
LOAD DATA INPATH &#39;/yjx/user.txt&#39; INTO TABLE test.t_user;

查询结果如下：SELECT * FROM test.t_user WHERE age &lt;= 18;

简单的查询不会执行 MapReduce，复杂的查询会调用 MapReduce 模板生成 MapReduce 任务。注意：复杂的查询如果不走 MR 模板，例如 count(*) 直接返回 0，请使用 LOAD DATA 正规的方式载入数据
</code></pre>

<h5 class="relative group">案例二 
    <div id="%E6%A1%88%E4%BE%8B%E4%BA%8C" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%A1%88%E4%BE%8B%E4%BA%8C" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>已知一些复杂的用户信息如下，将以下数据保存在 person.txt 文件中并上传至 HDFS（例如 /yjx/person.txt）：
songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing
yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing

根据以上数据结构创建 t_person 表。
USE test;
CREATE TABLE IF NOT EXISTS t_person (
name string,
friends array&lt;string&gt;,
children map&lt;string,int&gt;,
address struct&lt;street:string ,city:string&gt;
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
COLLECTION ITEMS TERMINATED BY &#39;_&#39;
MAP KEYS TERMINATED BY &#39;:&#39;
LINES TERMINATED BY &#39;\n&#39;;

最后执行以下语句将 person.txt 文件的数据载入 t_person 表。执行 LOAD DATA 语句后 person.txt 文件将会被移动到 t_person 表所在路径下（ /hive/warehouse/t_person/person.txt ）。
LOAD DATA INPATH &#39;/yjx/person.txt&#39; INTO TABLE t_person;

查询结果如下：SELECT * FROM t_person;

查看数据表详细信息：0: jdbc:hive2://node01:10000&gt; DESC t_person;
0: jdbc:hive2://node01:10000&gt; SHOW CREATE TABLE t_person;
</code></pre>

<h5 class="relative group">案例三 
    <div id="%E6%A1%88%E4%BE%8B%E4%B8%89" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%A1%88%E4%BE%8B%E4%B8%89" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>Hive 默认序列化类是 LazySimpleSerDe，其只支持使用单字节分隔符(Char)来加载文本数据，例如逗号、制表符、空格等等，默认的分隔符为“\001”。根据不同文件的不同分隔符，我们可以通过在创建表时使用 ROW FORMAT DELIMITED 来指定文件中的分割符，确保其正确的将表中的每一列与文件中的每一列实现——对应的关系。
可以通过 DESC FORMATTED t_sleuth_log; 命令来查看表的格式化详细信息。

情况一：每一行数据的分隔符是多字节分隔符，例如 || 、 -- 等。
　　将以下数据保存在 singer.txt 文件中并上传至 HDFS（例如 /yjx/singer.txt）。
01||周杰伦||中国||台湾||男||稻香
02||刘德华||中国||香港||男||笨小孩
03||汪 峰||中国||北京||男||光明
04||朴 树||中国||北京||男||那些花儿
05||许 巍||中国||陕西||男||故乡
06||Taylor Swift||美国||宾夕法尼亚||女||Blank Space
07||Maroon 5||美国||加利福尼亚||男||Sugar

根据以上数据结构创建 t_singer 表。
CREATE TABLE IF NOT EXISTS t_singer (
id string,
name string,
country string,
province string,
gender string,
works string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;||&#39;
LINES TERMINATED BY &#39;\n&#39;;

　最后执行以下语句将 singer.txt 文件的数据载入 t_singer 表。执行 LOAD DATA 语句后 singer.txt 文件将会被移动到 t_singer 表所在路径下（ /hive/warehouse/t_singer/singer.txt ）。
　LOAD DATA INPATH &#39;/yjx/singer.txt&#39; INTO TABLE t_singer;
　
　.查询结果如下：SELECT * FROM t_singer;
</code></pre><pre tabindex="0"><code>情况二：数据的字段中包含了分隔符。
将以下数据保存在 gateway-server.log 文件中并上传至 HDFS（例如 /yjx/gateway-server.log）。
2022-08-11 14:15:25.326 [gateway-server,,] [reactor-http-nio-4] DEBUG
2022-08-11 14:15:25.326 [gateway-server,,] [reactor-http-nio-2] DEBUG
2022-08-11 14:15:25.339 [gateway-server,95aa725089b757f8,95aa725089b757f8] [reactor-http-nio-4] DEBUG
2022-08-11 14:15:25.692 [gateway-server,95aa725089b757f8,95aa725089b757f8] [reactor-http-nio-2] DEBUG
2022-08-11 14:15:26.242 [gateway-server,,] [PollingServerListUpdater-o] DEBUG

根据以上数据结构创建 t_sleuth_log 表。
CREATE TABLE IF NOT EXISTS t_sleuth_log (
datetime string,
trace_id string,
process string,
log_level string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39; &#39;
LINES TERMINATED BY &#39;\n&#39;;

最后执行以下语句将 singer.txt 文件的数据载入 t_singer 表。执行 LOAD DATA 语句后 singer.txt 文件将会被移动到 t_singer 表所在路径下（ /hive/warehouse/t_singer/singer.txt ）。
LOAD DATA INPATH &#39;/yjx/gateway-server.log&#39; INTO TABLE t_sleuth_log;

查询结果如下：SELECT * FROM t_sleuth_log;
</code></pre><p><strong>解决方案</strong></p>
<pre tabindex="0"><code>方案一：替换分隔符。使用 MR 程序提前将数据清洗一遍（将多字节分隔符替换为单字节分隔符）。
@Override
protected void map(LongWritable key, Text value, Context context)
  throws IOException, InterruptedException {
  // 将多字节分隔符替换为单字节分隔符
  String line = value.toString().replaceAll(&#34;\\|\\|&#34;, &#34;|&#34;);
  // 写出数据
  context.write(new Text(line), NullWritable.get());
}
提示：如果是小文件直接写个 Java 程序或者使用文本工具自带的替换功能就可以搞定，如果是大文件建议使用 MR
程序来进行处理
</code></pre><pre tabindex="0"><code>方案二：RegexSerDe。Hive 内置了很多的 SerDe 类，可以使用 RegexSerDe 正则序列化器来处理。推荐使用。
　官方文档：https://cwiki.apache.org/confluence/display/Hive/SerDe	
　Built-in SerDes：Avro (Hive 0.9.1 and later)；ORC (Hive 0.11 and later)；RegEx；
Thrift；Parquet (Hive 0.13 and later)；CSV (Hive 0.14 and later)；JsonSerDe (Hive 0.12 and later in hcatalog-core)

创建 t_singer 表并使用 RegexSerDe 序列化器
CREATE TABLE IF NOT EXISTS t_singer (
id string,
name string,
country string,
province string,
gender string,
works string
)
ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39;
WITH SERDEPROPERTIES (&#34;input.regex&#34; = &#34;(\\d*)\\|\\|([\\u4e00-\\u9fa5\\w\\x20\\-]*)\\|\\|([\\u4e00-
\\u9fa5\\w\\x20\\-]*)\\|\\|([\\u4e00-\\u9fa5\\w\\x20\\-]*)\\|\\|([\\u4e00-\\u9fa5\\w\\x20\\-]*)\\|\\|
([\\u4e00-\\u9fa5\\w\\x20\\-]*)&#34;);

注意：RegexSerDe 采用 Java 标准。由于反斜杠是 Java String 类中的转义字符，因此必须使用双反斜杠来定义单反斜杠。例如，要定义 \d ，必须在 Regex 中使用 \\d 。在 input.regex 中，以一个匹配组表示一个字段，例如 (\d+) 。
　　最后执行以下语句将 singer.txt 文件的数据载入 t_singer 表。执行 LOAD DATA 语句后 singer.txt 文件将会被移动到 t_singer 表所在路径下（ /hive/warehouse/t_singer/singer.txt ）。
　　
　　LOAD DATA INPATH &#39;/yjx/singer.txt&#39; INTO TABLE t_singer;
　　
　　查询结果如下：SELECT * FROM t_singer;
　　
　　创建 t_sleuth_log 表并使用 RegexSerDe 序列化器
　　CREATE TABLE IF NOT EXISTS t_sleuth_log (
datetime string,
trace_id string,
process string,
log_level string
)
ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39;
WITH SERDEPROPERTIES (&#34;input.regex&#34; = &#34;([\\d\\x20-:\\.]*) (.*) (.*) ([a-zA-Z]*)&#34;);

最后执行以下语句将 singer.txt 文件的数据载入 t_singer 表。执行 LOAD DATA 语句后 singer.txt 文件将会被移动到 t_singer 表所在路径下（ /hive/warehouse/t_singer/singer.txt ）。
LOAD DATA INPATH &#39;/yjx/gateway-server.log&#39; INTO TABLE t_sleuth_log;

查询结果如下：SELECT * FROM t_sleuth_log;
</code></pre><pre tabindex="0"><code>方案三：自定义 InputFormat 继承 TextInputFormat，在 RecordReader 中对分割字符进行处理。将自定义的 InputFormat 类打成 jar 包，例如 MyInputFormat.jar 。将 MyInputFormat.jar 放到 hive/lib 目录中或者使用 Hive 的 add jar 命令，然后就可以建表了。由于 Hive 是基于Hadoop 集群运行的，所以 hadoop/lib 目录中也必须放入 MyInputFormat.jar。
　　假设你的 InputFormat 类路径是 com.yjxxt.hive.input ，则建表语句为：
　　CREATE TABLE IF NOT EXISTS t_sleuth_log (
id int, name stirng, ...
)
-- 指定分隔符
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39; &#39;
-- 指定自定义的 InputFormat
STORED AS INPUTFORMAT &#39;com.yjxxt.hive.input&#39;
OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;;

 HiveIgnoreKeyTextOutputFormat 是系统自带的 OUTPUTFORMAT 类，也支持自定义。
</code></pre>

<h4 class="relative group">表详情 
    <div id="%E8%A1%A8%E8%AF%A6%E6%83%85" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A1%A8%E8%AF%A6%E6%83%85" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>显示所有数据表。		SHOW TABLES;
可以通过 like 进行过滤。		SHOW TABLES LIKE &#39;t*&#39;;
查看某个数据表的详情。		
DESC t_person;
DESC FORMATTED t_person;
DESCRIBE FORMATTED t_person;
</code></pre>

<h4 class="relative group">重命名 
    <div id="%E9%87%8D%E5%91%BD%E5%90%8D" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E9%87%8D%E5%91%BD%E5%90%8D" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>内部表会同时修改文件目录，外部表因为目录是共享的，所以不会修改目录名称。
ALTER TABLE old_table_name RENAME TO new_table_name;
</code></pre>

<h4 class="relative group">修改列 
    <div id="%E4%BF%AE%E6%94%B9%E5%88%97" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E4%BF%AE%E6%94%B9%E5%88%97" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>-- 添加列
ALTER TABLE table_name ADD COLUMNS (new_col INT);
-- 一次增加一个列(默认添加为最后一列)
ALTER TABLE table_name ADD COLUMNS (new_col INT);
-- 可以一次增加多个列
ALTER TABLE table_name ADD COLUMNS (c1 INT, c2 STRING);
-- 添加一列并增加列字段注释
ALTER TABLE table_name ADD COLUMNS (new_col INT COMMENT &#39;a comment&#39;);

-- 更新列
ALTER TABLE table_name CHANGE old_col new_col STRING;
-- 将列 a 的名称更改为 a1
ALTER TABLE table_name CHANGE a a1 INT;
-- 将列 a1 的名称更改为 a2，将其数据类型更改为字符串，并将其放在列 b 之后
ALTER TABLE table_name CHANGE a1 a2 STRING AFTER b;
-- 将 c 列的名称改为 c1，并将其作为第一列
ALTER TABLE table_name CHANGE c c1 INT FIRST
</code></pre>

<h4 class="relative group">清空表 
    <div id="%E6%B8%85%E7%A9%BA%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%B8%85%E7%A9%BA%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>TRUNCATE TABLE table_name;
注意：清空表只能删除内部表的数据（HDFS 文件），不能删除外部表中的数据。
在 SQL 中，TRUNCATE 属于 DDL 语句，主要功能是彻底删除数据，使其不能进行回滚。
</code></pre>

<h4 class="relative group">删除表 
    <div id="%E5%88%A0%E9%99%A4%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%A0%E9%99%A4%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>DROP TABLE table_name;
注意：删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。
</code></pre>

<h3 class="relative group">内外部表 
    <div id="%E5%86%85%E5%A4%96%E9%83%A8%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%86%85%E5%A4%96%E9%83%A8%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">内部表 
    <div id="%E5%86%85%E9%83%A8%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%86%85%E9%83%A8%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	内部表（managed table）即Hive管理的表，Hive内部表的管理既包含逻辑以及语法上的，也包含实际物理意义上的，即创建Hive内部表时，数据将真实存在于表所在的目录内，删除内部表时，物理数据也和文件一并删除。默认创建的是内部表。</p>
<pre tabindex="0"><code>分析以下语句
-- 创建表
CREATE TABLE IF NOT EXISTS test.t_user(
id int,
username string,
password string,
gender string,
age int
)
ROW FORMAT DELIMITED FILEDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;;
——载入数据
LOAD DATA INPATH &#39;/yjx/user/user.txt&#39; INTO TABLE test.t_user;

执行LOAD DATA语句后，user.txt文件会被移动到t_user表所在路径下（ /hive/warehouse/test.db/t_user/user.txt ）。此时如果使用DROP语句删除t_user后，其数据和表的元数据都会被删除
</code></pre>

<h4 class="relative group">外部表 
    <div id="%E5%A4%96%E9%83%A8%E8%A1%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%A4%96%E9%83%A8%E8%A1%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	外部表（external table），仅仅只是在逻辑和语法意义上的，即新建表只是指向一个外部目录。同样，删除时不物理删除外部目录，仅仅是将引用和定义删除。</p>
<p>​	一般情况下，企业内部都是使用外部表，因为会有多人操作数据仓库，坑你产生数据表误删操作，为了数据安全性，一般使用外部表，方便达到数据共享。外部表数据创建和删除完全由自己控制，Hive不管理这些数据。</p>
<pre tabindex="0"><code>方案一：创建时指定数据位置*
数据的位置可以在创建时指定，在 Hive 的数据仓库 /hive/warehouse/test.db 目录下不会创建 t_user 目录。
CREATE EXTERNAL TABLE IF NOT EXISTS test_t_user(
id int,
username string,
password string,
gender string,
age int
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;
-- 可以指定到某个目录（该目录下的文件都会被扫描到），也可以指定到某个具体的文件
LOCATION &#39;/yjx/user&#39;;

提示：如果 /yjx/user 目录不存在 Hive 会帮我们自动创建，我们只需要将 t_user 表所需的数据上传
至 /yjx/user 目录即可。或者数据已经存在于 HDFS 某个目录，Hive 创建外部表时直接指定数据位置即可。
</code></pre><pre tabindex="0"><code>方案二：先建表再导入.也可以先单独建立外部表，建立时不指定数据位置，然后通过 LOAD DATA 命令载入数据。
CREATE EXTERNAL TABLE IF NOT EXISTS text.t_user2(
id int,
username string,
password string,
gender string,
age int
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;;

在这种情况下，Hive的数据仓库 /hive/warehouse/test.db目录下会创建t_user2目录，然后通过LOAD DATA命令载入数据。
LOAD DATA INPATH &#39;/yjx/user/user.txt&#39; INTO TABLE text.t_user2；

注意：在这种情况下，执行 LOAD DATA 语句后 user.txt 文件将会被移动到 t_user2 表所在路径下
（ /hive/warehouse/test.db/t_user2/user.txt ）。唯一的区别是，外部表此时如果使用 DROP 语句删除
t_user2 后，只会删除元数据，也就是说 t_user2 目录和 user.txt 数据并不会被删除。
</code></pre>

<h3 class="relative group">载入数据 
    <div id="%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>基本语法：
LOAD DATA [LOCAL] INPATH &#39;datapath&#39; [OVERWRITE] INTO TABLE student [PARTITION (partcol1=val1, ...)];
LOAD DATA：加载数据；
[LOCAL]：本地，不加则从HDFS中获取
INPATH：数据的路径
&#39;datapath&#39;：具体的路径，要参考本地还是HDFS
[OBERWRITE]：覆盖，不加则是追加数据
INTO TABLE：加入到表
student：表名
[PARTITION (partcol1=val1, ...)]分区

加载本地数据：LOAD DATA LOCAL INPATH &#39;/root/user.txt&#39; INTO TABLE t_user;
要加载的文件必须和 HiveServer2 在同一个节点，否则会报错： SemanticException Line 1:23 Invalid path&#39;&#39;/root/test.txt&#39;&#39;: No files matching path file

加载 HDFS 数据：LOAD DATA INPATH &#39;/yjx/user.txt&#39; INTO TABLE t_user;

加载并覆盖已有数据：LOAD DATA INPATH &#39;/yjx/user.txt&#39; OVERWRITE INTO TABLE t_user;
</code></pre><pre tabindex="0"><code>通过查询插入数据：
-- 创建表
CREATE TABLE t_user1 (
id int,
username string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;;
-- 创建表
CREATE TABLE t_user2 (
id int,
password string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;-&#39;
LINES TERMINATED BY &#39;\n&#39;;

-- 插入查询结果
-- 将查询结果插入一张表
-- 追加
INSERT INTO t_user1 SELECT id, username FROM t_user;
-- 覆盖
INSERT OVERWRITE TABLE t_user1 SELECT id, username FROM t_user;
INSERT OVERWRITE TABLE t_user2 SELECT id, password FROM t_user;
-- 将查询结果一次性存放到多张表（多重模式）
-- 追加
FROM t_user
INSERT INTO t_user1 SELECT id, username
INSERT INTO t_user2 SELECT id, password;
-- 覆盖
FROM t_user
INSERT OVERWRITE TABLE t_user1 SELECT id, username
INSERT OVERWRITE TABLE t_user2 SELECT id, password;
</code></pre>

<h3 class="relative group">导出数据 
    <div id="%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">通过SQL操作 
    <div id="%E9%80%9A%E8%BF%87sql%E6%93%8D%E4%BD%9C" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E9%80%9A%E8%BF%87sql%E6%93%8D%E4%BD%9C" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h5 class="relative group">将查询结果导出到本地 
    <div id="%E5%B0%86%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E5%AF%BC%E5%87%BA%E5%88%B0%E6%9C%AC%E5%9C%B0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%B0%86%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E5%AF%BC%E5%87%BA%E5%88%B0%E6%9C%AC%E5%9C%B0" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>首先在 HiveServer2 的节点上创建一个存储导出数据的目录。
mkdir -p /root/user
执行以下命令将查询结果导出到本地。
-- 将查询结果导出到本地
INSERT OVERWRITE LOCAL DIRECTORY &#39;/root/user&#39; SELECT * FROM t_user;
导出结果如下：
[root@node01 ~]# cat /root/user/000000_0
1admin123456男18
2zhangsanabc123男23
3lisi654321女16

按指定的格式将数据导出到本地
　首先在 HiveServer2 的节点上创建一个存储导出数据的目录
　mkdir -p /root/person
　执行以下命令将查询结果按指定的格式导出到本地
　-- 按指定的格式将数据导出到本地
INSERT OVERWRITE LOCAL DIRECTORY &#39;/root/person&#39;
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
COLLECTION ITEMS TERMINATED BY &#39;-&#39;
MAP KEYS TERMINATED BY &#39;:&#39;
LINES TERMINATED BY &#39;\n&#39;
SELECT * FROM t_person;
导出结果如下：
[root@node01 ~]# cat /root/person/000000_0
songsong,bingbing-lili,xiao song:18-xiaoxiao song:19,hui long guan-beijing
yangyang,caicai-susu,xiao yang:18-xiaoxiao yang:19,chao yang-beijing
</code></pre>

<h5 class="relative group">查询结果输出到HDFS 
    <div id="%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E8%BE%93%E5%87%BA%E5%88%B0hdfs" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E8%BE%93%E5%87%BA%E5%88%B0hdfs" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>首先在 HDFS 上创建一个存储导出数据的目录。
hdfs dfs -mkdir -p /yjx/export/user
执行以下命令将查询结果导出到 HDFS。-- 将查询结果导出到 HDFS
INSERT OVERWRITE DIRECTORY &#39;/yjx/export/user&#39;
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
SELECT * FROM t_user;
导出结果如下 /yjx/export/user/000000_0 ：
1,admin,123456,男,18
2,zhangsan,abc123,男,23
3,lisi,654321,女,16
</code></pre>

<h4 class="relative group">通过HDFS操作 
    <div id="%E9%80%9A%E8%BF%87hdfs%E6%93%8D%E4%BD%9C" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E9%80%9A%E8%BF%87hdfs%E6%93%8D%E4%BD%9C" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>首先在 HDFS 上创建一个存储导出数据的目录。
hdfs dfs -mkdir -p /yjx/export/person
使用 HDFS 命令拷贝文件到其他目录
hdfs dfs -cp /hive/warehouse/t_person/* /yjx/export/person
</code></pre>

<h4 class="relative group">将元数据和数据同时导出 
    <div id="%E5%B0%86%E5%85%83%E6%95%B0%E6%8D%AE%E5%92%8C%E6%95%B0%E6%8D%AE%E5%90%8C%E6%97%B6%E5%AF%BC%E5%87%BA" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%B0%86%E5%85%83%E6%95%B0%E6%8D%AE%E5%92%8C%E6%95%B0%E6%8D%AE%E5%90%8C%E6%97%B6%E5%AF%BC%E5%87%BA" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>首先在 HDFS 上创建一个存储导出数据的目录。
hdfs dfs -mkdir -p /yjx/export/person
将元数据和数据同时导出。
-- 将表结构和数据同时导出
EXPORT TABLE t_person TO &#39;/yjx/export/person&#39;;
注意：时间不同步，会导致导入导出失败。
</code></pre>

<h3 class="relative group">基本查询 
    <div id="%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h2 class="relative group">Hive高级 
    <div id="hive%E9%AB%98%E7%BA%A7" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive%E9%AB%98%E7%BA%A7" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h3 class="relative group">分区/分桶 
    <div id="%E5%88%86%E5%8C%BA%E5%88%86%E6%A1%B6" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%86%E5%8C%BA%E5%88%86%E6%A1%B6" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">数据模型 
    <div id="%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>Hive 的数据模型主要有以下四种：单独一个表；对表分区；对表分区再分桶；对表直接分桶。

在大数据中，最常见的一种思想就是分治，我们可以把大文件切割成一个个的小文件，这样每次操作小文件时就会容
易许多。同样的道理，在 Hive 中也是支持的，我们可以把大的数据，按照每天或者每小时切分成一个个的小文件，这样
去操作小文件就会容易许多，这就是分区、分桶的意思。
</code></pre>

<h4 class="relative group">分区 
    <div id="%E5%88%86%E5%8C%BA" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%86%E5%8C%BA" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	使用分区技术，可以避免Hive全表扫描，提升查询效率，同时能够减少数据冗余进而提高特定分区查询分析的效率
	注意，逻辑上分区表和未分区表没有区别，物理上分区表会将数据按照分区键的键值对存储在表目录的子目录中，目录名为“分区间=键值”。可以把建立分区想象成建立文件夹，把一些相似的数据存放在文件夹中。
	使用分区表时，尽量利用分区字段进行查询，如果不是用分区字段查询就会全扫描，这样就失去了分区的意义。
</code></pre>

<h5 class="relative group">静态分区 
    <div id="%E9%9D%99%E6%80%81%E5%88%86%E5%8C%BA" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E9%9D%99%E6%80%81%E5%88%86%E5%8C%BA" aria-label="锚点">#</a>
    </span>        
    
</h5>
<p>​	分区表分为静态分区和动态分区，区别在于前者是手动指定，后者是通过数据来判断分区。根据分区的深度又分为单分区和多分区</p>


<h6 class="relative group">单分区 
    <div id="%E5%8D%95%E5%88%86%E5%8C%BA" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%8D%95%E5%88%86%E5%8C%BA" aria-label="锚点">#</a>
    </span>        
    
</h6>
<pre tabindex="0"><code>创建静态分区表语法（静态分区和动态分区的建表语句是一样的）：
-- 单分区：创建分区表 PARTITIONED BY (分区字段名 分区字段类型)
-- 多分区：创建分区表 PARTITIONED BY (分区字段名 分区字段类型, 分区字段名2 分区字段类型2)
CREATE TABLE IF NOT EXISTS t_student (
sno int,
sname string
) PARTITIONED BY (grade int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;
注意： PARTITIONED BY () 括号中指定的分区字段名不能和表中的字段名一样。

载入数据：LOAD DATA INPATH &#39;/yjx/s1.txt&#39; INTO TABLE t_student PARTITION (grade=1);
查询数据：SELECT * FROM t_student WHERE grade=1;

需要注意的是，分区查询会将分区中所有的数据都查询出来，所以如果文件中的数据本身已经出问题了，那么查询的
结果也会出问题，请看以下案例。

添加分区（也可以在载入数据时添加分区）：
ALTER TABLE t_student ADD IF NOT EXISTS PARTITION (grade=1);
查看分区：SHOW PARTITIONS t_student;
删除分区：
-- ALTER TABLE 表名 DROP PARTITION(分区字段名=键值)
ALTER TABLE t_student DROP PARTITION (grade=1);
</code></pre>

<h6 class="relative group">多分区 
    <div id="%E5%A4%9A%E5%88%86%E5%8C%BA" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%A4%9A%E5%88%86%E5%8C%BA" aria-label="锚点">#</a>
    </span>        
    
</h6>
<pre tabindex="0"><code>创建静态分区表语法（静态分区和动态分区的建表语句是一样的）：
-- 单分区：创建分区表 PARTITIONED BY (分区字段名 分区字段类型)
-- 多分区：创建分区表 PARTITIONED BY (分区字段名 分区字段类型, 分区字段名2 分区字段类型2)
CREATE TABLE IF NOT EXISTS t_teacher (
tno int,
tname string
) PARTITIONED BY (grade int, clazz int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;
注意：前后两个分区的关系为父子关系，也就是 grade 文件夹下面有多个 clazz 子文件夹。

分区表载入数据：
LOAD DATA INPATH &#39;/yjx/t1.txt&#39; INTO TABLE t_teacher PARTITION (grade=1, clazz=1);
LOAD DATA INPATH &#39;/yjx/t2.txt&#39; INTO TABLE t_teacher PARTITION (grade=1, clazz=2);
LOAD DATA INPATH &#39;/yjx/t3.txt&#39; INTO TABLE t_teacher PARTITION (grade=1, clazz=3);
LOAD DATA INPATH &#39;/yjx/t4.txt&#39; INTO TABLE t_teacher PARTITION (grade=2, clazz=1);
LOAD DATA INPATH &#39;/yjx/t5.txt&#39; INTO TABLE t_teacher PARTITION (grade=2, clazz=2);
也可以使用分区表的 INSERT 语句插入数据（会执行 MR 任务）：
INSERT INTO TABLE t_teacher PARTITION (grade=2, clazz=3) VALUES (10, &#39;jueyuan10&#39;);

查询数据：SELECT * FROM t_teacher WHERE grade=1 AND clazz=1;
添加分区：ALTER TABLE t_teacher ADD IF NOT EXISTS PARTITION (grade=1, clazz=1);
查看分区：SHOW PARTITIONS t_teacher;
删除分区：
-- ALTER TABLE 表名 DROP PARTITION(分区字段名=键值)
ALTER TABLE t_teacher DROP PARTITION (grade=1, clazz=1);
</code></pre>

<h5 class="relative group">动态分区 
    <div id="%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA" aria-label="锚点">#</a>
    </span>        
    
</h5>
<p>​	主要区别在于静态分区是手动指定，静态分区是通过数据来判断分区。静态分区实在编译时期通过用书传递来决定；动态分区只有在SQL执行时才能决定。</p>
<pre tabindex="0"><code>开启动态分区首先要在 Hive 会话中设置以下参数：
-- 开启动态分区支持（默认 true）
SET hive.exec.dynamic.partition=true;
-- 是否允许所有分区都是动态的，strict 要求至少包含一个静态分区列，nonstrict 则无此要求（默认 strict）
SET hive.exec.dynamic.partition.mode=nonstrict;

其余参数的详细配置如下：
-- 每个 Mapper 或 Reducer 可以创建的最大动态分区个数（默认为 100）
-- 比如：源数据中包含了一年的数据，如果按天分区，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错
hive.exec.max.dynamic.partition.pernode=100;

-- 一个动态分区创建可以创建的最大动态分区个数（默认为 1000）
hive.exec.max.dynamic.partitions=1000;

-- 全局可以创建的最大文件个数（默认为 100000）
hive.exec.max.created.files=100000;

-- 当有空分区产生时，是否抛出异常（默认为 false）
hive.error.on.empty.partition=false;

-- 是否开启严格模式 strict（严格模式）和 nostrict（非严格模式，默认）
hive.mapred.mode=nostrict;
</code></pre><pre tabindex="0"><code>严格模式：
	Hive通过参数hive.mapred.omde来设置是否开启严格模式。参数只有两个：strict严格模式和默认的nostrict非严格模式。
	开启严格模式，主要是为了禁止某些查询（这些船可能会造成意想不到的结果），目前主要禁止三种类型：
		分取差表时必须在WHERE语句后面指定分区段，否则不予魂虚限制性。
</code></pre>

<h3 class="relative group">数据抽样 
    <div id="%E6%95%B0%E6%8D%AE%E6%8A%BD%E6%A0%B7" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%95%B0%E6%8D%AE%E6%8A%BD%E6%A0%B7" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h3 class="relative group">事务 
    <div id="%E4%BA%8B%E5%8A%A1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E4%BA%8B%E5%8A%A1" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">原因 
    <div id="%E5%8E%9F%E5%9B%A0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%8E%9F%E5%9B%A0" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Hive在设计之初，是不支持事务操作的，因为Hive的核心目标是将已存在的结构化数据文件映射成表，然后提供基于表的SQL分析处理器；是一款面向分析的工具。并且映射的文件存在HDFS中，其本身也不支持随机修改文件的数据。这个定位意味着HQL起初不支持UPDATE、DELETE语法，也就没有事务支持。
	从 Hive 0.14 版本开始，引入了事务特性，能够在 Hive 表上实现 ACID 语义，包括 INSERT/UPDATE/DELETE/MERGE 语句，以解决缓慢变化维表或部分数据不正确，需要更正的情况。Hive 3.0 又对该特性进行了优化，包括改进了底层的文件组织方式，减少了对表结构的限制，以及支持谓词下推和向量化查询。
	最终 Hive 支持了具有 ACID 语义的事务，但做不到和传统关系型数据库那样的事务级别，仍有很多局限如：
		1、不支持BEGIN、COMMIT、ROLLBACK所有操作自动提交
		2、事务表仅支持ORC文件格式
		3、标参数transactional必须为true
		4、外部表不能成为ACID表，不允许非ACID会话读取/写入表
		5、默认事务关闭，需要额外配置
</code></pre>

<h4 class="relative group">实践 
    <div id="%E5%AE%9E%E8%B7%B5" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AE%9E%E8%B7%B5" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>事务功能相关参数如下：
# 开启 hive 并发
SET hive.support.concurrency=true;
# 配置事务管理类
SET hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

# 如果事务表配合分区分桶一起使用建议开启以下参数
# 开启分桶功能
SET hive.enforce.bucketing=true;
# 启用自动压缩
SET hive.compactor.initiator.on=true;
# 这里的压缩线程数必须大于 0，理想状态和分桶数一致
SET hive.compactor.worker.threads=2;
# 是否允许所有分区都是动态的，strict 要求至少包含一个静态分区列，nonstrict 则无此要求（默认 strict）
SET hive.exec.dynamic.partition.mode=nonstrict;
</code></pre><pre tabindex="0"><code>创建事务表：
CREATE TABLE IF NOT EXISTS test.t_user(
id int,
name string,
age int
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY
STORED AS ORC
TBLPROPERTIES(&#34;transactional&#34;=&#34;true&#34;);

-- 事务表并分区分桶
CREATE TABLE IF NOT EXISTS test.t_user(
id int,
name string,
age int
)
PARTITIONED BY (time string)
CLUSTERED BY (id) INTO 2 BUCKETS
ROW FORMAT DELIMITED FIELDS TERMINATED BY
STORED AS ORD

-- 开启事务
TBLPROPERTIES(&#34;transactional&#34;=&#34;true&#34;);
</code></pre><pre tabindex="0"><code>测试：
INSERT INTO test.t_user VALUES (1，&#34;张三&#34;,18),(2，&#34;张三&#34;,18),(3，&#34;张三&#34;,18);
	因为使用的是事务表，所以写入数据时，INSERT 语句会在一个事务中运行。它会创建名为 delta 的目录，存放事务的信息和表的数据。
　　目录名称的格式为 delta_minWID_maxWID_stmtID ，即 delta 前缀、写事务的 ID 范围、以及语句 ID。
　　
　　所有INSERT语句都会创建delta目录。UPDATE语句也会创建delta目录，但会先创建一个delete目录，即先删除后插入。delete目录的前缀是delete_delta。Hive会为所有的事务生成一个全局唯一的ID，包括读写操作。针对写事务，Hive还会创建一个写事物ID（WriteID），该ID在表范围内唯一。写事务ID会编码到delta和delete目录的名称中。语句ID（StatementID）则是当一个事务中有多条写入语句时使用，作为唯一标识。
　　
　　测试 UPDATE：UPDATE test.t_user SET name = &#34;老张&#34;, age = 58 WHERE id = 1;
　　测试 DELETE：DELETE FROM test.t_user WHERE id = 1;
</code></pre>

<h4 class="relative group">压缩 
    <div id="%E5%8E%8B%E7%BC%A9" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%8E%8B%E7%BC%A9" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	随着写操作的积累，表中的 delta 和 delete 文件会越来越多。事务表的读取过程中需要合并所有文件，数量一多势必会影响效率。此外，小文件对 HDFS 这样的文件系统也是不够友好的。因此，Hive 引入了压实（Compaction）的概念，分为 Minor 和 Major 两类。
	
	Minor Compaction会将金所有的delta文件压实为一个文件，delete也压实为一个。压实后的结果文件名中会包含写事务ID范围，同事省略掉语句ID。压实过程是在 Hive Metastore 中运行的，会根据一定阈值自动触发。我们也可以使用如下语句人工触发：
	ALTER TABLE test.t_user COMPACT &#39;minor&#39;;
	
	Major Compaction会将所有文件合并为一个文件，以base_N的形式命名，其中 N 表示最新的写事务 ID。已删除的数据将在这个过程中被剔除。需要注意的是，在 Minor 或 Major Compaction 执行之后，原来的文件不会被立刻删除。这是因为删除的动作是在另一个名为 Cleaner 的线程中执行的。因此，表中可能同时存在不同事务 ID 的文件组合，这在读取过程中需要做特殊处理。
</code></pre>

<h3 class="relative group">索引 
    <div id="%E7%B4%A2%E5%BC%95" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%B4%A2%E5%BC%95" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>提示：Hive 3.0.0 版本中索引已被废弃

原理
	Hive 在指定列上建立索引时，会产生一张索引表（Hive 的一张物理表），里面的字段包括，索引列的值、该值对应的HDFS 文件路径、该值在文件中的偏移量。
　　在执行索引字段查询时候，首先额外生成一个 MapReduce Job，根据对索引列的过滤条件，从索引表中过滤出索引列对应的 HDFS 文件路径及偏移量，输出到 HDFS 上的一个文件中，然后根据这些文件中的 HDFS 路径和偏移量，筛选原始Input 文件，生成新的 Split，作为整个 Job 的 Split，达到不用全表扫描的目的。
　　Hive 索引使用过程繁杂，而且性能一般，在 Hive 3.0 中已被移除，在工作环境中不推荐优先使用，在分区数量过多或查询字段不是分区字段时，索引可以作为补充方案同时使用。推荐使用 ORC 文件格式的索引类型进行查询。
　　提示：工作中优先考虑使用物化视图和列式存储文件格式来加快查询速度，大表则分区分桶，使用 SMB Join。
　　
废弃
	由于 Hive 是针对海量数据存储的，创建索引需要占用大量的空间，最主要的是 Hive 索引无法自动进行刷新，也就是当新的数据加入时候，无法为这些数据自动加入索引；
	Hive 索引使用过程繁杂，且性能一般；
	在可以预见到分区数据非常庞大的情况下，分桶和索引常常是优于分区的。而分桶由于 SMB Join 对关联键要求严格，所以并不是总能生效；
	Hive 的索引与关系型数据库中的索引并不相同，比如，Hive 不支持主键或者外键；
	很多时候会优先考虑使用物化视图和列式存储文件格式来加快查询速度，大表则分区分桶，使用 SMB Join。
</code></pre>

<h3 class="relative group">视图/物化视图 
    <div id="%E8%A7%86%E5%9B%BE%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A7%86%E5%9B%BE%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>补：SQL没敲</p>


<h4 class="relative group">视图 
    <div id="%E8%A7%86%E5%9B%BE" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A7%86%E5%9B%BE" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	视图是一个虚拟的表，只保存定义，不实际存储数据，实际查询的时候改写SQL去问实际的数据表。不同于直接操作数据表，视图是依据SELECT来创建的，所以操作视图会根据创建视图的SELECT语句生成一张虚拟的表，然后在表上做SQL操作。</p>
<pre tabindex="0"><code>创建视图语法：
-- 创建视图
CREATE VIEW [IF NOT EXISTS] [db_name.]view_name (&lt;列名1&gt;, &lt;列名2&gt;, ...) AS &lt;SELECT语句&gt;;
-- 查看视图
SHOW TABLES;
-- 查看某个视图
DESC view_name;
-- 查看某个视图详细信息
DESC FORMATTED view_name;
-- 修改视图
ALTER VIEW [db_name.]view_name AS &lt;SELECT语句&gt;;
-- 删除视图
DROP VIEW [IF EXISTS] [db_name.]view_name [, &lt;视图名2&gt;, ...];

视图实践：
-- 查询部门经理人中薪水最低的部门名称
-- 不使用视图，推导过程
-- 查询部门经理人，处理NULL和去重
SELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr;
-- 查询部门经理人的薪水和部门编号
SELECT empno, sal, deptno FROM emp WHERE empno IN (
SELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr
);
-- 查询部门经理人中的最低薪水
SELECT empno, sal, deptno FROM emp WHERE empno IN (
SELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr
) ORDER BY sal LIMIT 1;
-- 查询部门经理人中薪水最低的部门名称
SELECT e.empno, e.sal, e.deptno, d.dname
FROM (
SELECT empno, sal, deptno FROM emp WHERE empno IN (
SELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr
) ORDER BY sal LIMIT 1
) e
INNER JOIN dept d ON e.deptno = d.deptno;

-- 使用视图
-- 创建视图：查询部门经理人，处理NULL和去重
CREATE VIEW IF NOT EXISTS vw_emp_mgr (mgr) AS
SELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr;
-- 创建视图：查询部门经理人的薪水和部门编号
CREATE VIEW IF NOT EXISTS vw_emp_mgr_sal (empno, sal, deptno) AS
SELECT empno, sal, deptno FROM emp WHERE empno IN (
SELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr
);
-- 创建视图：查询部门经理人中的最低薪水
CREATE VIEW IF NOT EXISTS vw_emp_mgr_minsal (empno, minsal, deptno) AS
SELECT empno, sal minsal, deptno FROM emp WHERE empno IN (
SELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr
) ORDER BY sal LIMIT 1;
-- 使用视图查询部门经理人中薪水最低的部门名称
SELECT e.empno, e.minsal, e.deptno, d.dname
FROM vw_emp_mgr_minsal e
INNER JOIN dept d ON e.deptno = d.deptno;
</code></pre><pre tabindex="0"><code>总结：
Hive 中的视图是一种虚拟表，只保存定义，不实际存储数据；
通常从真实的物理表查询中创建生成视图，也可以从已经存在的视图上创建新视图；
创建视图时，将冻结视图的结构，如果删除或更改基础表，则视图将失败；
视图是用来简化操作的，不缓冲记录，也不会提高查询性能。

优点：
通过视图可以提高数据的安全性，将真实表中特定的列提供给用户，保护数据隐私；
上层的业务查询和底层数据表解耦，业务上可以查的一张表，但是底层可能映射的是三张或多张表的数据；
修改底层数据模型只需要重建视图即可，不需要上层业务修改业务逻辑；
降低查询复杂度，优化查询语句（注意不是提高查询效率）。

缺点：
无法再对视图进行优化，而且并没有提升查询速度，只是使上层的业务逻辑变得更清晰简洁。
</code></pre>

<h4 class="relative group">物化视图 
    <div id="%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	物化视图 Materialized View，是一个包括查询结果的数据库对象，可以用预先计算并保存表链接或聚集等耗时较多的操作结果。在执行查询时，可以避免进行这些耗时的操作，从而快速得到结果。</p>
<p>​	使用物化视图的目的就是通过预计算，提高查询性能，所以要占用一定的存储空间。Hive还提供了物化视图的查询自动重写（基于 Apache Calcite 实现）和物化视图存储选择机制，可以本地存储在 Hive，也可以通过用户自定义Storage Handlers 存储在其他系统（如 Apache Druid）。</p>
<p>​	Hive引进物化视图的目的就是为了优化数据查询访问的效率，相当于从数据预处理的角度优化数据访问。</p>
<p>​	注意：物化事务图只可以在事务表上创建。</p>
<pre tabindex="0"><code>物化视图语法：
-- 创建物化视图
CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name
 [DISABLE REWRITE]
 [COMMENT materialized_view_comment]
 [PARTITIONED ON (col_name, ...)]
 [CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)]
 [
  [ROW FORMAT row_format]
  [STORED AS file_format] | STORED BY &#39;storage.handler.class.name&#39; [WITH SERDEPROPERTIES (...)]
 ]
 [LOCATION hdfs_path]
 [TBLPROPERTIES (property_name=property_value, ...)]
AS
&lt;query&gt;;

-- 物化视图是一种特殊的数据表，可以使用 SHOW TABLES 等语法
-- 删除物化视图
DROP MATERIALIZED VIEW [db_name.]materialized_view_name;
</code></pre><pre tabindex="0"><code>物化视图实践：
-- 创建事务表
CREATE TABLE IF NOT EXISTS test.emp (
EMPNO int,
ENAME varchar(255),
JOB varchar(255),
MGR int,
HIREDATE date,
SAL decimal(10,0),
COMM decimal(10,0),
DEPTNO int
)
STORED AS ORC
TBLPROPERTIES(&#34;transactional&#34;=&#39;true&#39;);
-- 载入数据
INSERT INTO test.emp SELECT * FROM scott.emp;
-- 创建物化视图
CREATE MATERIALIZED VIEW test.emp_analysis
AS
SELECT
deptno,
COUNT(*) cnt,
AVG(sal) avg_sal,
MAX(sal) max_sal,
MIN(sal) min_sal
FROM test.emp GROUP BY deptno;

　　物化视图的创建过程会比较慢，因为 SELECT 查询执行的数据会自动落地。“自动”也即在 Query 的执行期间，任何用户对该物化视图是不可见的，执行完毕之后物化视图可用。默认情况下，创建好的物化视图可被用于查询优化器
Optimizer 查询重写，在物化视图创建期间可以通过 Disable Rewrite 参数设置禁止使用。
</code></pre>

<h5 class="relative group">刷新物化视图 
    <div id="%E5%88%B7%E6%96%B0%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%B7%E6%96%B0%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>增量刷新：
当物化视图满足一定条件时，默认会执行增加刷新，即只刷新原始源表中的变动会影响到的数据，增量刷新会减少重
建步骤的执行时间。要执行增量刷新，物化视图的创建语句和更新源表的方式都须满足一定条件：
	物化视图只使用了事务表；
	如果物化视图中包含 GROUP BY，则该物化视图必须存储在 ACID 表中，因为它需要支持 MERGE 操作。对于由 Scan-Project-Filter-Join 组成的物化视图，不存在该限制。
	
定时刷新:
可以通过 SET hive.materializedview.rewriting.time.window=10min; 设置定期刷新，默认为 0min。该参数也可以作为建表语句的一个属性，在建表时设置。

全量刷新：
若只用 INSERT 更新了源表数据，可以对物化视图进行增量刷新。若使用 UPDATE、INSERT 更新了源表数据，那么只
能进行重建，即全量刷新(REBUILD)。
当数据源变更（新数据插入 Inserted、数据被修改 Modified），物化视图也需要更新以保持数据一致性，需要用户主
动触发 Rebuild，命令如下：ALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD;
注意：如果一张表创建了许多物化视图，那么在数据写入这张表时，可能会消耗许多机器的资源，比如数据带宽占
满、存储增加等等
</code></pre>

<h5 class="relative group">查询重写 
    <div id="%E6%9F%A5%E8%AF%A2%E9%87%8D%E5%86%99" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%9F%A5%E8%AF%A2%E9%87%8D%E5%86%99" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>	物化视图创建后即可用于相关查询的加速，并且优化器能够利用其定义语义来使用物化视图自动重写传入的查询，从
而加快查询的执行速度。即：用户提交查询 Query，若该 Query 经过重写后可以命中已经存在的物化视图，则直接通过物化视图查询数据返回结果，以实现查询加速。
	物化视图是否开启重写查询功能可以通过全局参数控制 set hive.materializedview.rewriting=true; ，默认为true。除此之外，用户还可以选择性的控制指定的物化视图的查询重写机制，命令如下：
	ALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE;
	
查询重写案例：
-- 创建事务表
CREATE TABLE IF NOT EXISTS test.emp (
EMPNO int,
ENAME varchar(255),
JOB varchar(255),
MGR int,
HIREDATE date,
SAL decimal(10,0),
COMM decimal(10,0),
DEPTNO int
)
STORED AS ORC
TBLPROPERTIES(&#34;transactional&#34;=&#39;true&#39;);
-- 载入数据
INSERT INTO test.emp SELECT * FROM scott.emp;
-- 创建事务表
CREATE TABLE IF NOT EXISTS test.dept (
DEPTNO int,
DNAME varchar(255),
LOC varchar(255)
)
STORED AS ORC
TBLPROPERTIES(&#34;transactional&#34;=&#39;true&#39;);
-- 载入数据
INSERT INTO test.dept SELECT * FROM scott.dept;

假设我们要经常获取有关 1981 年之后按不同时期聘用的员工及其部门的信息，我们可以创建以下物化视图：
CREATE MATERIALIZED VIEW test.mv1
AS
SELECT empno, dname, hiredate
FROM test.emp AS e
INNER JOIN test.dept AS d ON e.deptno = d.deptno
WHERE hiredate &gt;= &#39;1981-01-01&#39;;

然后，以下查询提取有关 1982 年第一季度雇用的员工的信息（该查询会触发查询重写）：
SELECT empno, dname
FROM test.emp AS e
INNER JOIN test.dept AS d ON e.deptno = d.deptno
WHERE hiredate &gt;= &#39;1982-01-01&#39; AND hiredate &lt;= &#39;1982-03-31&#39;;

在实际查询时，Hive 将使用物化视图重写传入的查询，包括实例化扫描之上的补偿谓词。查询重写的等价 SQL 如下：
SELECT empno, dname
FROM test.mv1
WHERE hiredate &gt;= &#39;1982-01-01&#39; AND hiredate &lt;= &#39;1982-03-31&#39;;
</code></pre>

<h3 class="relative group">高级查询 
    <div id="%E9%AB%98%E7%BA%A7%E6%9F%A5%E8%AF%A2" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E9%AB%98%E7%BA%A7%E6%9F%A5%E8%AF%A2" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>:补SQL</p>


<h4 class="relative group">行转列——一行变多行 
    <div id="%E8%A1%8C%E8%BD%AC%E5%88%97%E4%B8%80%E8%A1%8C%E5%8F%98%E5%A4%9A%E8%A1%8C" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A1%8C%E8%BD%AC%E5%88%97%E4%B8%80%E8%A1%8C%E5%8F%98%E5%A4%9A%E8%A1%8C" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>创建表：
CREATE TABLE IF NOT EXISTS t_movie1 (
id int,
name string,
types string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;;

载入数据：
LOAD DATA INPATH &#39;/yjx/movie1.txt&#39; INTO TABLE t_movie1;

EXPLODE爆发
 EXPLODE() 可以将 Hive 一行中复杂的 Array 或者 Map 结构拆分成多行，那如何将某个列的数据转为数组呢？可以配置 SPLIT 函数一起使用。
 SELECT EXPLODE(SPLIT(types, &#34;-&#34;)) FROM t_movie1;
 如果我还想查看一下数组中这些电影类型隶属于哪个电影，需要配合侧视图 LATERAL VIEW 一起使用。
 -- movie_type 是侧视图别名
SELECT id, name, type
FROM t_movie1,
-- 生成侧视图（表）AS 后面是侧视图的字段
LATERAL VIEW EXPLODE(SPLIT(types, &#34;-&#34;)) movie_type AS type;
</code></pre>

<h4 class="relative group">列转行——多行变一行 
    <div id="%E5%88%97%E8%BD%AC%E8%A1%8C%E5%A4%9A%E8%A1%8C%E5%8F%98%E4%B8%80%E8%A1%8C" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%97%E8%BD%AC%E8%A1%8C%E5%A4%9A%E8%A1%8C%E5%8F%98%E4%B8%80%E8%A1%8C" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>创建表：
CREATE TABLE IF NOT EXISTS t_movie2 (
id int,
name string,
type string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;;

载入数据：LOAD DATA INPATH &#39;/yjx/movie2.txt&#39; INTO TABLE t_movie2;

COLLECT_SET/COLLECT_LIST
 COLLECT_SET() 和 COLLECT_LIST() 可以将多行数据转成一行数据，区别就是 LIST 的元素可重复而 SET 的元素是去重的。
SELECT id, name,
CONCAT_WS(&#39;:&#39;, COLLECT_SET(type)) AS type_set,
CONCAT_WS(&#39;:&#39;, COLLECT_LIST(type)) AS type_list
FROM t_movie2
GROUP BY id, name;

MySQL 实现方式： GROUP_CONCAT([DISTINCT] 要连接的字段 [ORDER BY 排序字段 ASC/DESC] [SEPARATOR
‘分隔符’])
</code></pre>

<h4 class="relative group">URL解析 
    <div id="url%E8%A7%A3%E6%9E%90" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#url%E8%A7%A3%E6%9E%90" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>侧视图 LATERAL VIEW 配合 PARSE_URL_TUPLE 函数可以实现 URL 字段的一列变多列。

创建表：
CREATE TABLE IF NOT EXISTS t_mall (
id int,
name string,
url string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;;

载入数据：LOAD DATA INPATH &#39;/yjx/mall.txt&#39; INTO TABLE t_mall;
实践：
SELECT * FROM t_mall;
SELECT PARSE_URL_TUPLE(url, &#39;PROTOCOL&#39;, &#39;HOST&#39;, &#39;PATH&#39;, &#39;QUERY&#39;) FROM t_mall;
SELECT a.id, a.name, b.protocol, b.host, b.path, b.query
FROM t_mall a,
LATERAL VIEW
PARSE_URL_TUPLE(url, &#39;PROTOCOL&#39;, &#39;HOST&#39;, &#39;PATH&#39;, &#39;QUERY&#39;) b AS protocol, host, path, query;
</code></pre>

<h4 class="relative group">JSON 解析 
    <div id="json-%E8%A7%A3%E6%9E%90" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#json-%E8%A7%A3%E6%9E%90" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	JSON数据格式是数据存储及数据处理中最常见的结构化数据格式之一，一般公司会将数据以JSON格式存储在HDFS中，当构建数据仓库是时，需要对JSON格式的税局进行处理和分析，那么就需要在Hive中对JSON格式的数据进行解析读取</p>
<p>​	Hive提供了两种解析JSON数据的方式，可以根据不同需求选择合适的方式对JSON数据进行处理。</p>
<pre tabindex="0"><code>使用JSON函数处理：
GET_JSON_OBJECT(json_txt,path)
	第一个参数：指定要解析的 JSON 字符串
	第二个参数：指定要返回的字段，通过 $.column_name 的方式来指定
JSON_TUPLE(jsonStr,p1,p2,...pn)
	第一个参数：指定要解析的 JSON 字符串
	第二个参数：指定要返回的第 1 个字段
	……
	
使用 JsonSerDe：建表时指定 JSON 序列化器，加载 JSON 文件到表中时会自动解析为对应的表格式
</code></pre><pre tabindex="0"><code>JSON 函数
创建表：
CREATE TABLE IF NOT EXISTS t_user_json (
user_json string
);

载入数据：LOAD DATA INPATH &#39;/yjx/user.json&#39; INTO TABLE t_user_json;

GET_JSON_OBJECT(JSON_TXT, PATH) 实践：
 SELECT * FROM t_user_json;
 SELECT
 GET_JSON_OBJECT(user_json, &#39;$.id&#39;) AS id,
 GET_JSON_OBJECT(user_json, &#39;$.username&#39;) AS username,
 GET_JSON_OBJECT(user_json, &#39;$.gender&#39;) AS gender,
 GET_JSON_OBJECT(user_json, &#39;$.age&#39;) AS age
 FROM t_user_json;

JSON_TUPLE(jsonStr, p1, p2, ..., pn) 实践：
 SELECT
 JSON_TUPLE(user_json, &#39;id&#39;, &#39;username&#39;, &#39;gender&#39;, &#39;age&#39;) AS (id, username, gender, age)
 FROM t_user_json;
</code></pre><pre tabindex="0"><code>JsonSerDe	官方文档：https://cwiki.apache.org/confluence/display/Hive/SerDe
Hive 内置了很多的 SerDe 类，可以使用 JsonSerDe 序列化器来处理。建表时指定 JSON 序列化器，加载 JSON 文件到表中时会自动解析为对应的表格式。

Built-in SerDes：
Avro (Hive 0.9.1 and later)
ORC (Hive 0.11 and later)
RegEx
Thrift
Parquet (Hive 0.13 and later)
CSV (Hive 0.14 and later)
JsonSerDe (Hive 0.12 and later in hcatalog-core)

创建 t_user_json2 表并使用 JsonSerDe 序列化器。
CREATE TABLE IF NOT EXISTS t_user_json2 (
id int,
username string,
gender string,
age int
)
ROW FORMAT SERDE &#39;org.apache.hive.hcatalog.data.JsonSerDe&#39;
STORED AS TEXTFILE;

载入数据：LOAD DATA INPATH &#39;/yjx/user.json&#39; INTO TABLE t_user_json2;

实践：SELECT * FROM t_user_json2;
</code></pre>

<h3 class="relative group">窗口函数 
    <div id="%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>补：SQL</p>


<h4 class="relative group">定义 
    <div id="%E5%AE%9A%E4%B9%89" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AE%9A%E4%B9%89" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	官方文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</p>
<pre tabindex="0"><code>	窗口函数是用于分析的一类函数，理解窗口函数要从聚合函数说起。聚合函数是将某列中多行的值合并为一行，比如SUM,COUNT等。这列函数往往无法单独与列一起查询。
	而窗口函数可为窗口中的每行都返回一个值。就是在查询的结果上多出一列，这一列可以使聚合的值，也可以是排序值。比如：SELECT ename, COUNT(*) OVER() FROM emp;
	窗口函数和GROUP BY聚合函数区别在于：窗口函数仅仅只会将结果附加到当前的结果上，不会对已有的行或列做任何修改。GROUP BY对于各个组它仅仅只会保留一行聚合结果。
</code></pre>

<h4 class="relative group">窗口函数语法 
    <div id="%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E8%AF%AD%E6%B3%95" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E8%AF%AD%E6%B3%95" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	窗口函数是指OVER()函数，窗口是由一个OVER子句定义的多行记录。窗口函数分为三类：聚合型窗口函数、分析性窗口函数和去执行窗口函数</p>
<pre tabindex="0"><code>语法：
SELECT XX函数（）OVER（PARTITION BY 用于分组的列ORDER BY 用于排序的列ROWS/RANGE BETWEEN 开始位置AND结束位置）；

XX函数() ：聚合型窗口函数/分析型窗口函数/取值型窗口函数
OVER() ：窗口函数
	PARTITION BY ：后跟分组的字段，划分的范围被称为窗口
	ORDER BY ：决定窗口范围内数据的排序方式
移动窗口 ：
	移动方向：
		CURRENT ROW ：当前行
		PRECENDING ：向当前行之前移动
		FOLLOWING ：向当前行之后移动
		UNBOUNDED ：起点或终点（一般结合 PRECEDING，FOLLOWING 使用）
			UNBOUNDED PRECEDING ：表示该窗口第一行（起点）
			UNBOUNDED FOLLOWING ：表示该窗口最后一行（终点）
	移动范围： ROWS 和 RANGE
</code></pre>

<h4 class="relative group">基本使用 
    <div id="%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>如果 OVER 不提供分组方法，则将所有数据分为一组，如下：
SELECT ename, deptno,
AVG(sal) OVER()
FROM emp;
以上将所有员工的薪水进行了平均计算，然后显示在每行数据后边。

PARTITION BY：PARTITION BY 的作用和 GROUP BY 是类似，用于分组。
SELECT ename, deptno,
AVG(sal) OVER(PARTITION BY deptno) AS avgsal
FROM emp;

ORDER BY：
在每个窗口（分组）内，如果我们想按每个人的薪水进行排序，可以使用 ORDER BY 子句，这里我们用 RANK() 指定序
号，相同的薪水序号是一样的：
SELECT ename, deptno, sal,
RANK() OVER(PARTITION BY deptno ORDER BY sal) AS salorder
FROM emp;

当 ORDER BY 与聚合函数一起使用时，会形成顺序聚合，如 SUM 聚合与 ORDER BY 结合使用时，就实现类似于累计和的效果：
SELECT ename, deptno, sal,
SUM(sal) OVER(PARTITION BY deptno ORDER BY sal) AS sumsal
FROM emp;
</code></pre><pre tabindex="0"><code>总结：
与 GROUP BY 的区别：
	结果数据形式	
		窗口函数可以在保留原表中的全部数据
		GROUP BY 只能保留与分组字段聚合的结果
	排序范围不同
		窗口函数中的 ORDER BY 只是决定着窗口里的数据的排序方式
		普通的 ORDER BY 决定查询出的数据以什么样的方式整体排序
	SQL 顺序
		GROUP BY 先进行计算
		窗口函数在 GROUP BY 后进行计算
</code></pre>

<h4 class="relative group">移动窗口（滑动窗口） 
    <div id="%E7%A7%BB%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%A7%BB%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	有时候需要根据数据的前后重新分配窗口，比如在股票、气温数据等场景下，数据的前后会有影响，就适用移动窗口。</p>
<pre tabindex="0"><code>移动方向：
CURRENT ROW ：当前行
PRECENDING ：向当前行之前移动
FOLLOWING ：向当前行之后移动
UNBOUNDED ：起点或终点（一般结合 PRECEDING，FOLLOWING 使用）
	UNBOUNDED PRECEDING ：表示该窗口第一行（起点）
	UNBOUNDED FOLLOWING ：表示该窗口最后一行（终点）
	
	
移动范围：
	ROWS ：ROWS 后定义窗口从哪里开始（当前行也参与计算），与 BETWEEN 搭配可以表示范围。如果省略 BETWEEN仅指定一个端点，那么将该端点视为起点，终点默认为当前行。ROWS 会根据 ORDER BY 子句排序后，按分组后排序列的顺序取前 N 行或后 N 行进行计算（当前行也参与计算）。物理行。
ROWS 2 PRECEDING ：窗口从当前行的前两行开始计算，计算到当前行；
ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ：等同于上一句；
ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING ：窗口从当前行开始计算，计算到当前行的后两行；
ROWS BETWEEN 2 PRECEDING AND 1 FOLLOWING ：窗口从当前行的前两行开始计算，计算到当前行的下一
行，当前行也参与计算；
ROWS UNBOUNDED PRECEDING ：窗口从第一行（起点）计算到当前行；
ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ：等同于上一句；
ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING ：窗口从当前行计算到最后一行（终点）；
ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING ：窗口从第一行（起点）计算到当前行下一行；
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ：窗口从第一行（起点）计算到最后一
行（终点）。

	RANGE ：RANGE 后定义窗口从哪里开始（当前行也参与计算），与 BETWEEN 搭配可以表示范围。如果省略
BETWEEN 仅指定一个端点，那么将该端点视为起点，终点默认为当前行。RANGE 会根据 ORDER BY 子句排序后，按
分组后排序列的值的整数区间取前 N 行或后 N 行进行计算（相同排序值的行都会被算进来，当前行也参与计算）。
	RANGE 的窗口范围子句语法与 ROWS 一模一样，唯一的区别就在于 RANGE 会根据 ORDER BY 子句排序后，按分组后排序列的值的整数区间取前 N 行或后 N 行进行计算（相同排序值的行都会被算进来，当前行也参与计算）
	整数区间解释：如果窗口范围子句为 RANGE BETWEEN 2 PRECEDING AND CURRENT ROW ，假设排序列的值为
1 2 3 4 5 7 8 11，计算规则如下： 
	1 =&gt; 1，因为前面没有任何行，所以只有自己
	2 =&gt; 1 + 2，因为前面只有一行，所以只加了 1
	3 =&gt; 1 + 2 + 3，前面两行加当前行
	4 =&gt; 2 + 3 + 4，前面两行加当前行
	5 =&gt; 3 + 4 + 5，前面两行加当前行
	7 =&gt; 5 + 6 + 7，因为 6 不存在，所以实际上只加了 5
	8 =&gt; 6 + 7 + 8，因为 6 不存在，所以实际上只加了 7
	11 =&gt; 9 + 10 + 11，因为 9 和 10 都不存在，所以实际上只有自己
	
	当 ORDER BY 缺少窗口范围子句时，窗口范围子句默认为： RANGE BETWEEN UNBOUNDED PRECEDING AND
CURRENT ROW 。
	当 ORDER BY 和窗口范围子句都缺失时，窗口范围子句默认为： ROWS BETWEEN UNBOUNDED PRECEDING AND
UNBOUNDED FOLLOWING 。
</code></pre><pre tabindex="0"><code>SELECT ename, deptno, sal,
-- 统计所有人薪资
SUM(sal) OVER() AS sumsal1,
-- 按部门统计所有人薪资（范围字句默认为：ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING）
SUM(sal) OVER(PARTITION BY deptno) AS sumsal2,
-- 起点到终点的窗口聚合，和 sumsal2 结果一样
SUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED
FOLLOWING) AS sumsal3,
-- 按部门统计所有人薪资，实现累计和的效果（范围字句默认为：RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT
ROW）
SUM(sal) OVER(PARTITION BY deptno ORDER BY sal) AS sumsal4,
-- 起点到当前行的窗口聚合，和 sumsal4 结果一样
SUM(sal) OVER(PARTITION BY deptno ORDER BY sal RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS
sumsal5,
-- 起点到当前行的窗口聚合，为了让大家区别 ROWS 和 RANGE（观察 sumsal5 和 sumsal6 的结果）
SUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS
sumsal6,
-- 前面一行和当前行的窗口聚合
SUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS sumsal7,
-- 前面一行和当前行和后面一行的窗口聚合
SUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS sumsal8,
-- 当前行到终点的窗口聚合
SUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS
sumsal9
FROM emp;
</code></pre>

<h4 class="relative group">分析性窗口函数 
    <div id="%E5%88%86%E6%9E%90%E6%80%A7%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%86%E6%9E%90%E6%80%A7%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>RANK() ：间断，相同值同序号，例如 1、2、2、2、5。
DENSE_RANK() ：不间断，相同值同序号，例如 1、2、2、2、3。
ROW_NUMBER() ：不间断，序号不重复，例如 1、2、3、4、5（2、3 可能是相同的值）。
SELECT ename, deptno, sal,
RANK() OVER(PARTITION BY deptno ORDER BY sal) AS salorder1,
DENSE_RANK() OVER(PARTITION BY deptno ORDER BY sal) AS salorder2,
ROW_NUMBER() OVER(PARTITION BY deptno ORDER BY sal) AS salorder3
FROM emp;

PERCENT_RANK() ：计算小于当前行的值在所有行中的占比，类似百分比排名。可以用来计算超过了百分之多少的
人。计算某个窗口或分区中某个值的累积分布。假定升序排序，则使用以下公式确定累积分布：小于 x 的行数 / 窗口
或 PARTITION 分区内的总行数。其中 x 等于 ORDER BY 子句中指定的列的当前行中的值。
CUME_DIST() ：计算小于等于当前行的值在所有行中的占比。
NTILE(N) ：如果把数据按行数分为 N 份，那么该行所属的份数是第几份。注意：N 必须为 INT 类型。
SELECT ename, deptno, sal,
PERCENT_RANK() OVER(PARTITION BY deptno ORDER BY sal) AS percent_rank_sal,
CUME_DIST() OVER(PARTITION BY deptno ORDER BY sal) AS cume_dist_sal,
NTILE(3) OVER(PARTITION BY deptno ORDER BY sal) AS ntile_sal
FROM emp;
</code></pre>

<h4 class="relative group">取值型窗口函数 
    <div id="%E5%8F%96%E5%80%BC%E5%9E%8B%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%8F%96%E5%80%BC%E5%9E%8B%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>LAG(COL, N, DEFAULT_VAL) ：往前第 N 行数据，没有数据的话用 DEFAULT_VAL 代替。
LEAD(COL, N, DEFAULT_VAL) ：往后第 N 行数据，没有数据的话用 DEFAULT_VAL 代替。
FIRST_VALUE(EXPR) ：分组内第一个值，但是不是真正意义上的第一个，而是截至到当前行的第一个。
LAST_VALUE(EXPR) ：分组内最后一个值，但是不是真正意义上的最后一个，而是截至到当前行的最后一个。
SELECT ename, deptno, sal,
FIRST_VALUE(sal) OVER(PARTITION BY deptno ORDER BY sal) AS firstsal,
LAST_VALUE(sal) OVER(PARTITION BY deptno ORDER BY sal) AS lastsal,
LAG(sal, 2, 1) OVER(PARTITION BY deptno ORDER BY sal) AS lagsal,
LEAD(sal, 2, -1) OVER(PARTITION BY deptno ORDER BY sal) AS leadsal
FROM emp;
</code></pre>

<h3 class="relative group">自定义函数 
    <div id="%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>​	Hive 自身已经提供了非常丰富的函数，比如：COUNT/MAX/MIN 等，但是数量与功能有限，如果无法满足需求，可以通过自定义函数来进行扩展。自定义函数分为三种：</p>
<p>UDF（User Defined Function）：普通函数，一进一出，比如 UPPER, LOWER
UDAF（User Defined Aggregation Function）：聚合函数，多进一出，比如 COUNT/MAX/MIN
UDTF（User Defined Table Generating Function）：表生成函数，一进多出，比如 LATERAL VIEW EXPLODE()
　　官方文档：https://cwiki.apache.org/confluence/display/Hive/HivePlugins（注意区分已过时的用法）</p>


<h4 class="relative group">创建项目 
    <div id="%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>完整 pom.xml 文件如下：
&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;project xmlns=&#34;http://maven.apache.org/POM/4.0.0&#34;
    xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34;
    xsi:schemaLocation=&#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-
4.0.0.xsd&#34;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;com.yjxxt&lt;/groupId&gt;
  &lt;artifactId&gt;hive-demo&lt;/artifactId&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;
    &lt;!-- Hadoop 版本控制 --&gt;
    &lt;hadoop.version&gt;3.3.4&lt;/hadoop.version&gt;
    &lt;!-- Hive 版本控制 --&gt;
    &lt;hive.version&gt;3.1.2&lt;/hive.version&gt;
  &lt;/properties&gt;
  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
      &lt;version&gt;${hadoop.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
      &lt;version&gt;${hadoop.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
      &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
      &lt;version&gt;${hive.version}&lt;/version&gt;
    &lt;/dependency&gt;
      &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre>

<h4 class="relative group">自定义 UDF 
    <div id="%E8%87%AA%E5%AE%9A%E4%B9%89-udf" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%87%AA%E5%AE%9A%E4%B9%89-udf" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	实现UDF的方式有两种，第一种是比较简单的形式，继承UDF类通过evaluate方法实现，目前已过时。第二种是继承GenericUDF重写initialize方法、evaluate方法、getDisplayString方法。这里是第二种</p>
<pre tabindex="0"><code>将自定义 UDF 程序打成 jar 包并上传至 HDFS，例如： /yjx/hive-demo-1.0-SNAPSHOT.jar 。

在 Hive 中定义自定义函数。

CREATE FUNCTION HELLO_UDF AS &#39;com.yjxxt.hive.udf.HelloUDF&#39;
USING JAR &#39;hdfs:///yjx/hive-demo-1.0-SNAPSHOT.jar&#39;;

重新加载函数。
-- 重新加载函数
RELOAD FUNCTIONS;
-- 查看函数详细信息
DESC FUNCTION EXTENDED HELLO_UDF;

测试函数
SELECT HELLO_UDF(ename) FROM emp LIMIT 5;

移除函数。
DROP [TEMPORARY] FUNCTION [IF EXISTS] [DBNAME.]FUNCTION_NAME;
-- 移除函数
DROP FUNCTION HELLO_UDF;
-- 重新加载函数
RELOAD FUNCTIONS;

注意：客户端会缓存同名的类，会导致自定义函数定义失败，可以通过重启客户端来解决。
</code></pre>

<h4 class="relative group">自定义 UDAF 
    <div id="%E8%87%AA%E5%AE%9A%E4%B9%89-udaf" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%87%AA%E5%AE%9A%E4%B9%89-udaf" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	实现 UDAF 的方式有两种，第一种是比较简单的形式，先继承 UDAF 类，然后使用静态内部类实现 UDAFEvaluator 接口，目前已过时。第二种是先继承 AbstractGenericUDAFResolver 类重写 getEvaluator 方法，然后使用静态内部类实现GenericUDAFEvaluator 接口。这里主要讲第二种。</p>
<pre tabindex="0"><code>init() ：初始化，一般负责初始化内部字段，通常初始化用来存放最终结果的变量。
iterate() ：处理每一条输入记录，每次对一个新的值进行聚合计算时都会调用该方法，一般会根据计算结果更新
用来存放最终结果的变量。
terminatePartial() ：终止部分，部分聚合结果的时候调用该方法，必须返回一个封装了聚合计算当前状态的对
象。
merge() ：接受来自 terminatePartial 的返回结果，进行合并。Hive 合并两部分聚合的时候回调用这个方法。
terminate() ：终止方法，返回最终聚合函数结果。

// 确定各个阶段输入输出参数的数据格式 ObjectInspectors
public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException
// 中间缓存的暂存结构，用于接收中间运行时需要暂存的变量数据
public static abstract class AbstractAggregationBuffer implements AggregationBuffer
// 保存数据聚集结果的类
public abstract AggregationBuffer getNewAggregationBuffer() throws HiveException;
// 重置聚集结果
public abstract void reset(AggregationBuffer agg) throws HiveException;
// Map 阶段，迭代处理 SQL 传过来的列数据
public abstract void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException;
// Map 与 Combiner 结束返回结果，得到部分数据聚集结果
public abstract Object terminatePartial(AggregationBuffer agg) throws HiveException;
// Combiner 合并 Map 返回的结果，还有 Reducer 合并 Mapper 或 Combiner 返回的结果
public abstract void merge(AggregationBuffer agg, Object partial) throws HiveException;
// Reduce 阶段，输出最终结果
public abstract Object terminate(AggregationBuffer agg) throws HiveException;

将自定义 UDAF 程序打成 jar 包并上传至 HDFS，例如： /yjx/hive-demo-1.0-SNAPSHOT.jar 。

-- 重新加载函数
RELOAD FUNCTIONS;
-- 查看函数详细信息
DESC FUNCTION EXTENDED FIELD_LEN;

测试函数。
SELECT deptno, COUNT(*) cnt, FIELD_LEN(ename) flen FROM emp GROUP BY deptno;

移除函数。
DROP [TEMPORARY] FUNCTION [IF EXISTS] [DBNAME.]FUNCTION_NAME;
-- 移除函数
DROP FUNCTION FIELD_LEN;
-- 重新加载函数
RELOAD FUNCTIONS;
注意：客户端会缓存同名的类，会导致自定义函数定义失败，可以通过重启客户端来解决。
</code></pre>

<h4 class="relative group">自定义 UDTF 
    <div id="%E8%87%AA%E5%AE%9A%E4%B9%89-udtf" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%87%AA%E5%AE%9A%E4%B9%89-udtf" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	UDTF（User Defined Table Generating Function）：表生成函数，一进多出，用于处理单行数据，并生成多个数据行。实现 UDF 需要继承的 GenericUDTF，然后重写父类的三个抽象方法，输出后有几列，在 initialize 中定义，主要处理逻辑在 process 中实现。</p>
<pre tabindex="0"><code>将自定义 UDTF 程序打成 jar 包并上传至 HDFS，例如： /yjx/hive-demo-1.0-SNAPSHOT.jar

在 Hive 中定义自定义函数。
CREATE FUNCTION HELLO_UDTF AS &#39;com.yjxxt.hive.udtf.HelloUDTF&#39;
USING JAR &#39;hdfs:///yjx/hive-demo-1.0-SNAPSHOT.jar&#39;;

-- 重新加载函数
RELOAD FUNCTIONS;
-- 查看函数详细信息
DESC FUNCTION EXTENDED HELLO_UDTF;

准备数据 /yjx/movie.json 
创建表并载入数据。
-- 创建表
CREATE TABLE IF NOT EXISTS t_movie3 (
movie_json string
);
-- 载入数据
LOAD DATA INPATH &#39;/yjx/movie.json&#39; INTO TABLE t_movie3;

测试函数。
SELECT HELLO_UDTF(movie_json) FROM t_movie3;
</code></pre>

<h2 class="relative group">案例练习 
    <div id="%E6%A1%88%E4%BE%8B%E7%BB%83%E4%B9%A0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%A1%88%E4%BE%8B%E7%BB%83%E4%B9%A0" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h2 class="relative group">Hive压缩/存储 
    <div id="hive%E5%8E%8B%E7%BC%A9%E5%AD%98%E5%82%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive%E5%8E%8B%E7%BC%A9%E5%AD%98%E5%82%A8" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h3 class="relative group">压缩 
    <div id="%E5%8E%8B%E7%BC%A9-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%8E%8B%E7%BC%A9-1" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>	在某些情况下，我们还是需要对数据做压缩处理。压缩技术能够有效减少存储系统的读写字节数，提高网络带宽和磁盘空间的效率。Hive 相当于 Hadoop 的客户端，所以 Hive 的压缩分两部分完成，一部分是 Hadoop 的压缩，一部分是 Hive 的。
	
	其他参考Hadoop压缩
</code></pre>

<h3 class="relative group">存储方式 
    <div id="%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>当今的数据处理大致可分为两大类：OLTP 和 OLAP。</p>


<h4 class="relative group">OLTP 事务处理 
    <div id="oltp-%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#oltp-%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>　OLTP 联机事务处理（On-Line Transaction Processing）：OLTP 是传统关系型数据库的主要应用，来执行一些基本的、日常的事务处理，比如数据库记录的增、删、查、改等。
　　数据计算和数据存储分开，所有用户发过来的请求都是一个事件 Event，事件处理从关系型数据库中查询并进行返
回。特点实时性很好，来一个事件处理一个事件，额外数据存储在关系型数据库中。最大问题是能够同时处理的数据有
限，数据库做连表查询的代价很高。
</code></pre>

<h4 class="relative group">OLTP 分析处理 
    <div id="oltp-%E5%88%86%E6%9E%90%E5%A4%84%E7%90%86" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#oltp-%E5%88%86%E6%9E%90%E5%A4%84%E7%90%86" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code></code></pre>

<h4 class="relative group">行式存储（Row-oriented） 
    <div id="%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8row-oriented" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8row-oriented" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>查询满足条件的一整行数据的时，只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更
快。

传统的关系型数据库，如 Oracle、DB2、MySQL、SQL SERVER 等都是采用行式存储，在基于行式存储的数据库中，数
据是按照行数据为基础逻辑存储单元进行存储的，一行中的数据在存储介质中以连续存储形式存在。
Text File 和 Sequence File 的存储格式都是基于行存储的。

这种存储格式比较方便进行 INSERT/UPDATE 操作，不足之处就是如果查询只涉及某几个列，它会把整行数据都读取出
来，不能跳过不必要的列读取。当然数据比较少，一般没啥问题，如果数据量比较大就比较影响性能，还有就是由于
每一行中，列的数据类型不一致，导致不容易获得一个极高的压缩比，也就是空间利用率不高。
</code></pre>

<h4 class="relative group">列式存储（Column-oriented） 
    <div id="%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8column-oriented" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8column-oriented" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>列存将每一列的数据连续存储。相比于行式存储，列式存储在分析场景下有着许多优良的特性：
在行存模式下，数据按行连续存储，所有列的数据都存储在一个 Block 中，不参与计算的列在 IO 时也要全部读出，读
取操作被严重放大。而列存模式下，只需要读取参与计算的列即可，极大的减低了 IO Cost，加速了查询。

同一列中的数据属于同一类型，压缩效果显著。列存往往有着高达十倍甚至更高的压缩比，节省了大量的存储空间，
降低了存储成本。

更高的压缩比意味着更小的 Data Size，从磁盘中读取相应数据耗时更短。

自由的压缩算法选择。不同列的数据具有不同的数据类型，适用的压缩算法也就不尽相同。可以针对不同列类型，选
择最合适的压缩算法。

高压缩比意味着同等大小的内存能够存放更多数据，系统 Cache 效果更好。官方数据显示，通过使用列存，在某些分
析场景下，能够获得 100 倍甚至更高的加速效应。

INSERT/UPDATE 很麻烦或者不方便，不适合扫描小量的数据。

列式存储相对于行式存储来说，是新兴的 HBase、HPVertica、EMCGreenplum、ClickHouse 等分布式数据库均采用的存储方式。在基于列式存储的数据库中， 数据是按照列为基础逻辑存储单元进行存储的，一列中的数据在存储介质中以
连续存储形式存在。
</code></pre>

<h3 class="relative group">存储格式 
    <div id="%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>	文件格式是定义数据文件系统中存储的一种方式，可以在文件中存储各种数据结构，特别是 ROW、COLUMN、MAP，
数组以及字符串，数字等。在 Hadoop 中，没有默认的文件格式，格式的选择取决于用途。而选择一种优秀、适合的数据
存储格式是非常重要的。选择合适的文件格式会带来以下好处：保证读取/写入的速度;对压缩支持友好;文件可被切分;
支持 Schema 的更改（Schema 指的是一组相关联的数据库对象，包含：表、字段、字段类型、索引、外键、等等）;
　　某些文件格式是为了其通用性而设计的（如 MapReduce、Spark、Flink），而其他文件则是针对特定的场景，特定的数据特征等。
</code></pre>

<h4 class="relative group">Text File 
    <div id="text-file" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#text-file" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	文本文件在非 Hadoop 领域很常见，在 Hadoop 领域更常见。数据一行一行到排列，每一行都是一条记录。以典型的UNIX 方式换行符 \n 终止。可以将每一行转为一个 JSON 串，让数据带有结构。文本文件是可以被切分的，但如果对文本文件进行压缩，则必须使用支持切分文件的压缩编解码器，例如 BZip2，否则只会被一个 MR 程序处理。
　　默认格式，存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大。
　
应用场景
仅在需要从 Hadoop 中直接提取数据，或直接从文件中加载大量数据的情况下，才建议使用纯文本格式或 CSV。

优点
简单易读、轻量级。
缺点
读写速度慢。
不支持块压缩，在 Hadoop 中对文本文件进行压缩/解压缩会有较高的读取成本，因为需要将整个文件全部压缩或
者解压缩。
无法切分压缩文件（会导致产生较大的 MapTask）。
</code></pre>

<h4 class="relative group">Sequence File 
    <div id="sequence-file" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#sequence-file" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Sequence File 是 Hadoop API 提供的一种支持二进制格式存储，存储方式为行存储，其具有使用方便、可分割、可压缩的特点。Sequence File 支持三种压缩选择：NONE，RECORD，BLOCK。Record 压缩率低，一般建议使用 BLOCK 压缩。
	Sequence 最初是为 MapReduce 设计的，因此和 MapReduce 集成很好。在 Sequence File 中，每个数据都是以一个 Key和一个 Value 进行序列化存储的，内部使用了 Hadoop 的标准的 Writable 接口实现序列化和反序列化。Sequence File 中的数据是以二进制格式存储的，这种格式所需的存储空间小于文本的格式。
	
应用场景
通常把 Sequence File 作为中间数据存储格式。例如：将大量小文件合并放入一个 SequenceFIle 中。
	Sequence File 的压缩方式有两种，“记录压缩”（Record Compression）和“块压缩”（Block Compression）。如果是记录压缩，则只压缩 Value 的值。如果是块压缩，则将多条记录一并压缩，包括 Key 和 Value。具体格式如下面两图所示

优点
与文本文件相比更紧凑，支持块级压缩。
压缩文件内容的同时，支持将文件切分。
序列文件在 Hadoop 和其他支持 HDFS 的项目兼容很好，例如：Spark。
让我们摆脱文本文件迈出的第一步。
可以作为大量小文件的容器。
缺点
对于具有 SQL 类型的 Hive 支持不好，需要读取和解压缩所有字段。
不存储元数据，并且对 Schema 扩展的唯一方式是在末尾添加新字段。
</code></pre>

<h4 class="relative group">Map File 
    <div id="map-file" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#map-file" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Map File 是排序后的 Sequence File，Map File 由两部分组成，分别是 data 和 index。
index 作为文件的数据索引，主要记录了每个 Record 的 Key 值，以及该 Record 在文件中的偏移位置。在 MapFile 被访问的时候，索引文件会被加载到内存，通过索引映射关系可迅速定位到指定的 Record 所在文件位置，因此，相对Sequence File 而言，Map File 的检索效率是高效的，缺点是会消耗一部分内存来存储 index 数据。

	需注意的是，MapFile 并不会把所有 Record 都记录到 index 中去，默认情况下每隔 128 条记录存储一个索引映射。当然，记录间隔可人为修改，通过 MapFIle.Writer 的 setIndexInterval() 方法，或修改 io.map.index.interval 属性；另外，与Sequence File 不同的是，Map File 的 KeyClass 一定要实现 WritableComparable 接口，即 Key 值是可比较的。
</code></pre>

<h4 class="relative group">Avro File 
    <div id="avro-file" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#avro-file" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Apache Avro 是与语言无关的序列化系统，由 Hadoop 创始人 Doug Cutting 开发。Avro 是基于行的存储格式，它会在每个文件中包含 JSON 格式的 Schema 定义，从而提高了互操作性并允许 Schema 的变化（删除列、添加列）。 除了支持可切分以外，还支持块压缩。Avro 是一种自描述格式，它将数据的 Schema 直接编码存储在文件中，可以用来存储复杂结构的数据。Avro 还可以进行快速序列化，生成的序列化数据也比较小。
	
应用场景
适合需要操作大量的列（数据比较宽），写入频繁的场景。

优点
Avro 是与语言无关的数据序列化系统，以 JSON 格式存储 Schema ，使其易于被任何程序读取和解释。
Avro 格式是 Hadoop 的一种基于行的存储格式，被广泛用作序列化平台。
数据本身以二进制格式存储，使其在 Avro 文件中紧凑且高效。
序列化和反序列化速度很快。
Avro 文件是可切分的、可压缩的，非常适合在 Hadoop 生态系统中进行数据存储。
缺点
行式存储效率较低。例如：读取 15 列中的 2 列数据，基于行式存储就需要读取数百万行的 15 列。而列式存储只
需要读取这 2 列即可。
列式存储因为是将同一列（类）的数据存储在一起，压缩率要比行式存储高。
</code></pre>

<h4 class="relative group">RC File 
    <div id="rc-file" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#rc-file" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>　　RC File 是为基于 MapReduce 的数据仓库系统设计的数据存储结构。它结合了行存储和列存储的优点（先水平划分数据，然后再垂直划分），可以满足快速数据加载和查询，有效利用存储空间以及适应高负载的需求。RC File 保证了同一行中的数据位于同一节点中，因此具有较低的元组重构成本。其次，RC File 可以利用按列的数据压缩并跳过不必要的列读取。RC File 是由二进制键/值对组成的 flat 文件，它与 Sequence File 有许多相似之处。在数仓中执行分析时，这种面向列的存储非常有用。
　　注意：无法直接将数据加载到 RC File 中。需要先将数据加载到另一个表中，然后通过查询另一张表的方式写入到新创建的 RC File 中。
　　
应用场景
常用在 Hive 中。RC File 可将数据分为几组行，将数据存储在行的列中。
RC File 首先将行水平划分为行拆分（Row Group），然后以列方式垂直划分每个行拆分（Columns）。
RC File 将行拆分的元数据存储为 Record 的 Key，并将行拆分的所有数据存储在 Value。
作为行存储，RC File 保证同一行中的数据位于同一节点中。
作为列存储，RC File 可以利用列数据压缩，并跳过不必要的列读取。

优点
基于列式的存储，更好的压缩比。
利用元数据存储来支持数据类型。
支持 Split。
缺点
RC File 不支持 Schema 扩展，如果要添加新的列，则必须重写文件，这会降低操作效率。
</code></pre>

<h4 class="relative group">ORC File 
    <div id="orc-file" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#orc-file" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Apache ORC（Optimized Row Columnar，优化行列）是 Apache Hadoop 生态系统面向列的开源数据存储格式，它与Hadoop 环境中的大多数计算框架兼容。ORC 代表“优化行列”，它以比 RC 更为优化的方式存储数据，提供了一种非常有效的方式来存储关系数据，然后存储 RC 文件。ORC 将原始数据的大小最多减少 75％，数据处理的速度大大提高。
	注意：无法直接将数据加载到 ORC File 中。需要先将数据加载到另一个表中，然后通过查询另一张表的方式写入到新创建的 ORC File 中。
	
应用场景
常用在 Hive 中。
	每个 ORC 文件由 1 个或多个 Stripe 组成，每个 Stripe 一般为 HDFS 的块大小，每个 Stripe 包含多
条记录，这些记录按照列进行独立存储，每个 Stripe 里有三部分组成，分别是 Index Data，Row Data，Stripe Footer。
	Postscript：提供了解释文件其余部分的必要信息，包括文件的页脚和元数据部分的长度，文件的版本以及使用的常规压缩类型（例如 none，zlib，LZO，LZ4，Zstd 或 Snappy）。
File Footer：包含文件正文的布局，类型架构信息，行数以及每个列的统计信息。
	Stripe 条带数据块：文件正文分为条带。每个条带都是自包含的，只能使用自己的字节与文件的 Footer 和 Postscript结合使用。每个条带仅包含整行，因此行不会跨越条带边界。Stripes 有三个部分：条带中行的一组索引，数据本身和条带页脚。索引和数据部分都按列分割，因此只需要读取所需列的数据。
	列统计：列统计信息的目标是，对于每个列，编写器记录计数并根据其他有用字段的类型进行记录。对于大多数原始
类型，它记录最小值和最大值；对于数字类型，它还存储总和。真实列数据块，其中又分为 Index Data（记录每列的
索引信息），Raw Data（记录原始数据），Stripe Footer（记录每列的统计信息，min/max/sum 等）。
	每个文件有一个 File Footer，这里面存的是每个 Stripe 的行数，每个 Column 的数据类型信息等；每个文件的尾部是一个 PostScript，这里面记录了整个文件的压缩类型以及 File Footer 的长度信息等。在读取文件时，会 Seek 到文件尾部读PostScript，从里面解析到 File Footer 长度，再读 File Footer，从里面解析到各个 Stripe 信息，再读各个 Stripe，即从后往前读。
	
优点
比 Text File，Sequence File 和 RC File 具备更好的的性能。
列数据单独存储。
带类型的数据存储格式，使用类型专用的编码器。
轻量级索引。
缺点
与 RC 文件一样，ORC 也是不支持列扩展的。如果要添加新的列，则必须重写文件，这会降低操作效率。
</code></pre>

<h4 class="relative group">Parquet File 
    <div id="parquet-file" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#parquet-file" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Parquet File 是另一种列式存储的结构，由 Twitter + Cloudera 开源，被 Hive、Spark、Drill、Impala、Pig 等支持，目前已成为 Apache 的顶级项目。和 ORC File 一样，Parquet 也是基于列的二进制存储格式，可以存储嵌套的数据结构。当指定要使用列进行操作时，磁盘输入/输出操效率很高。Parquet 与 Cloudera Impala 兼容很好，并做了大量优化。Parquet 还支持块压缩。与 RC 和 ORC 文件不同的是，Parquet Serdes 支持有限的 Schema 扩展。在 Parquet 中，可以在结构的末尾添加新列。
	关于 Hive 对 Parquet 文件的支持的一个注意事项：Parquet 列名必须小写，这一点非常重要。如果 Parquet 文件包含大小写混合的列名，则 Hive 将无法读取该列。
	行组（Row Group）：每一个行组包含一定的行数，在一个 HDFS 文件中至少存储一个行组，类似于 ORC 的 Stripe 的概念。列块（Column Chunk）：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可以使用不同的算法进行压缩。页（Page）：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可以使用不同的编码方
式。
	通常情况下，在存储 Parquet 数据的时候会按照 Block 大小设置行组的大小，由于一般情况下每一个 Mapper 任务处理数据的最小单位是一个 Block，这样可以把每一个行组由一个 Mapper 任务处理，增大任务执行并行度。
	一个 Parquet 文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的 Magic Code，用于校
验它是否是一个 Parquet 文件，Footer length 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的 Schema 信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在 Parquet 中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引。

优点：和 ORC 文件一样，它非常适合进行压缩，具有出色的查询性能，尤其是从特定列查询数据时，效率很高
缺点：与 RC 和 ORC 一样，Parquet 也具有压缩和查询性能方面的优点，与非列文件格式相比，写入速度通常较慢。
</code></pre>

<h5 class="relative group">Parquet vs ORC 
    <div id="parquet-vs-orc" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#parquet-vs-orc" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>ORC 文件格式压缩比 Parquet 要高，Parquet 文件的数据格式 Schema 要比 ORC 复杂，占用的空间也就越高。
ORC 文件格式的读取效率要比 Parquet 文件格式高。
如果数据中有嵌套结构的数据，则 Parquet 会更好。
Hive 对 ORC 的支持更好，对 Parquet 支持不好，ORC 与 Hive 关联紧密。
ORC 还可以支持 ACID、Update 操作等。
Spark 对 Parquet 支持较好，对 ORC 支持不好。
为了数据能够兼容更多的查询引擎，Parquet 也是一种较好的选择。
表的文件存储格式尽量采用 Parquet 或 ORC，不仅降低存储量，还优化了查询，压缩，表关联等性能。
</code></pre>

<h4 class="relative group">Hive 存储实践 
    <div id="hive-%E5%AD%98%E5%82%A8%E5%AE%9E%E8%B7%B5" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive-%E5%AD%98%E5%82%A8%E5%AE%9E%E8%B7%B5" aria-label="锚点">#</a>
    </span>        
    
</h4>


<h5 class="relative group">Sequence File 
    <div id="sequence-file-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#sequence-file-1" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>-- 创建表
CREATE TABLE IF NOT EXISTS t_weather_seq (
id int,
province string,
city string,
area_code int,
weather string,
temperature int,
wind_direction string,
wind_power string,
humidity int,
report_time string,
create_time string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;
-- 默认是 TEXTFILE
STORED AS SEQUENCEFILE;

-- 载入数据（只有 TEXTFILE 才可以使用 LOAD DATA 的方式）
INSERT INTO TABLE t_weather_seq SELECT * FROM t_weather;

文件格式虽然变了，但是 HQL 语句还是可以正常的执行。
</code></pre>

<h5 class="relative group">ORC File 
    <div id="orc-file-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#orc-file-1" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>-- 创建表
CREATE TABLE IF NOT EXISTS t_weather_orc_none (
id int,
province string,
city string,
area_code int,
weather string,
temperature int,
wind_direction string,
wind_power string,
humidity int,
report_time string,
create_time string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;
-- 默认是 TEXTFILE
STORED AS ORCFILE
-- 设置 ORCFILE 不使用压缩
TBLPROPERTIES(&#39;orc.compress&#39;=&#39;NONE&#39;);

-- 载入数据（只有 TEXTFILE 才可以使用 LOAD DATA 的方式）
INSERT INTO TABLE t_weather_orc_none SELECT * FROM t_weather;

-- 设置 ORCFILE 使用 SNAPPY 压缩
TBLPROPERTIES(&#39;orc.compress&#39;=&#39;SNAPPY&#39;);
ORC 文件默认采用 ZLIB 压缩。ZLIB 压缩率比 Snappy 高，但是 ZLIB 解压缩速率很低。
</code></pre>

<h5 class="relative group">Parquet File 
    <div id="parquet-file-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#parquet-file-1" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>-- 创建表
CREATE TABLE IF NOT EXISTS t_weather_parquet_none (
id int,
province string,
city string,
area_code int,
weather string,
temperature int,
wind_direction string,
wind_power string,
humidity int,
report_time string,
create_time string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;
-- 默认是 TEXTFILE
STORED AS PARQUETFILE
-- 设置 PARQUETFILE 不使用压缩（UNCOMPRESSED、GZIP、LZO、SNAPPY）
TBLPROPERTIES(&#39;parquet.compression&#39;=&#39;UNCOMPRESSED&#39;);

-- 载入数据（只有 TEXTFILE 才可以使用 LOAD DATA 的方式）
INSERT INTO TABLE t_weather_parquet_none SELECT * FROM t_weather;

-- 设置 PARQUETFILE 使用 SNAPPY 压缩（UNCOMPRESSED、GZIP、LZO、SNAPPY）
TBLPROPERTIES(&#39;parquet.compression&#39;=&#39;SNAPPY&#39;);

注意 ORC 是 xxx.compress ，Parquet 是 xxx.compression 。在实际的项目开发中，Hive 表的数据存储格式一般选择 ORC 或 Parquet。压缩方式一般选择 Snappy 或 LZO。
</code></pre>

<h2 class="relative group">Hive 优化 
    <div id="hive-%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hive-%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h2>


<h3 class="relative group">EXPLAIN 执行计划 
    <div id="explain-%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#explain-%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>	使用过 SQL 的同学应该都知道 EXPLAIN，它可以帮助开发人员分析 SQL 问题，使用 EXPLAIN 可以将 SQL 语句的执行过程打印出来，帮助我们选择更好的索引和写出更优化的查询语句。
　　Hive SQL 也提供了这个功能，Hive 和 SQL 的不同之处在于 Hive 会根据底层计算引擎将其转化为 MR/Tez/Spark 的 Job去运行，所以 Hive SQL 的 EXPLAIN 也可以看到 MR 相关的执行流程。
　　在 Hive 中，EXPLAIN 会将 HQL 语句的依赖关系、实现步骤、实现过程进行解析返回，有助于我们了解 HQL 语句在底层是如何实现数据的查询与处理的，辅助我们对 Hive 进行优化。
　　官方文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain
</code></pre>

<h4 class="relative group">语法 
    <div id="%E8%AF%AD%E6%B3%95-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%AF%AD%E6%B3%95-1" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>常用语法命令：
EXPLAIN [EXTENDED]|CBD|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE|query

EXTENDED：体统一些额外信息，通常是文件名这样的物理信息。
CBO：输出由Calcite优化器生成的计划。CBO（Cost Based Optimizer对查询进行动态评估，获取最佳物理计划）
AST：输出查询的抽象语法树。在2.1.0删除可能导致OOM错误，在4.0.0修复。
DEPENDENCY：以JSON格式返回查询所依赖的表和分区信息。
AUTHORIZATION：显示执行查询需要授权的条目。
LOCKS：这对于了解系统将获得哪些锁很有用。LOCKS 从 Hive 3.2.0 开始支持。
VECTORIZATION：显示是否启用了向量化查询，以及为什么没有启用的原因。
ANALYZE：用实际的行数注释计划。从 Hive 2.2.0 开始支持。
query：Hive SQL 语句。
</code></pre>

<h4 class="relative group">组成部分 
    <div id="%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>查询计划由以下几个部分组成：
The Abstract Syntax Tree for the query：抽象语法树
The dependencies between the different stages of the plan：计划不同阶段之间的依赖关系
The description of each of the stages：阶段描述
</code></pre>

<h4 class="relative group">剖析 
    <div id="%E5%89%96%E6%9E%90" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%89%96%E6%9E%90" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>我们将上述结果拆分看，先从最外层开始，包含两个大的部分：
STAGE DEPENDENCIES： 各个 Stage 之间的依赖性；
STAGE PLANS： 各个 Stage 的执行计划。

　　先看第一部分 STAGE DEPENDENCIES，包含两个 Stage，Stage-1 是根 Stage，说明这是开始的 Stage，Stage-0 依赖Stage-1，Stage-1 执行完成以后执行 Stage-0。
　　
　　再看第二部分 STAGE PLANS，里面有一个 Map Reduce，一个 MR 的执行计划分为两个部分：
Map Operator Tree：Map 端的执行计划树；Reduce Operator Tree：Reduce 端的执行计划树。

	这两个执行计划树里面包含这条 SQL 语句的 Operator：
　　Map 端第一个操作肯定是加载表，所以就是 TableScan 表扫描操作，常见的属性：
alias：表名称
Statistics：表统计信息，包含表中数据条数，数据大小等
Select Operator：查询操作，常见的属性：
	expressions：需要的字段名称及字段类型
	outputColumnNames：输出的列名称
	Statistics：表统计信息，包含表中数据条数，数据大小等
Group By Operator：分组聚合操作，常见的属性：
	aggregations：显示聚合函数信息
	mode：聚合模式，值有 hash：随机聚合，就是 hash partition；partial：局部聚合；final：最终聚合
	keys：分组的字段，如果没有分组，则没有此字段
	outputColumnNames：聚合之后输出列名
	Statistics：表统计信息，包含分组聚合之后的数据条数，数据大小等
Reduce Output Operator：输出到 Reduce 的操作，常见属性：
	sort order：值为空不排序；值为 + 正序排序，值为 - 倒序排序；值为 +- 排序的列为两列，第一列为正序，	   第二列为倒序
Filter Operator：过滤操作，常见的属性：
	predicate：过滤条件，如 SQL 语句中的 WHERE id &gt;= 1，则此处显示 (id &gt;= 1)
Map Join Operator：Join 操作，常见的属性：
	condition map：Join 方式，如 Inner Join 0 to 1 Left Outer Join 0 to 2
	keys：Join 的条件字段
	outputColumnNames：Join 完成之后输出的字段
	Statistics：Join 完成之后生成的数据条数，大小等
File Output Operator：文件输出操作，常见的属性
	compressed：是否压缩
	table：表的信息，包含输入输出文件格式化方式，序列化方式等
Fetch Operator：客户端获取数据操作，常见的属性：
	limit：值为 -1 表示不限制条数，其他值为限制的条数
</code></pre>

<h4 class="relative group">实践 
    <div id="%E5%AE%9E%E8%B7%B5-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AE%9E%E8%B7%B5-1" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>JOIN
	JOIN 语句会过滤 null 值吗？执行以下 SQL 查看执行计划：
	EXPLAIN SELECT * FROM emp e INNER JOIN dept d ON e.deptno = d.deptno;
	从上述结果可以看到 predicate: deptno is not null (type: boolean) 这样一行，说明 JOIN 时会自动过滤掉关联字段为 null 值的情况，但 LEFT JOIN、RIGHT JOIN 或 FULL JOIN 是不会自动过滤的，大家可以自行尝试下。
	
GROUP BY
	GROUP BY 分组语句会进行排序吗？执行以下 SQL 查看执行计划：
	EXPLAIN SELECT deptno, AVG(sal) FROM emp GROUP BY deptno;
	我们看 Group By Operator，里面有 keys: deptno (type: int) 说明按照 deptno 进行了分组，再往下看还有sort order: + ，说明是按照 deptno 字段进行正序排序的。
	
执行效率
	说明 Hive 底层会自动帮我们进行优化，所以这两条 SQL 语句执行效率是一样的。这种优化叫做谓词下推。
</code></pre>

<h3 class="relative group">SQL 优化 
    <div id="sql-%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#sql-%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">RBO 优化 
    <div id="rbo-%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#rbo-%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	Rule-Based Optimization，简称 RBO：基于规则优化的优化器，是一种经验式、启发式的优化思路，优化规则都已经预先定义好了，只需要将 SQL 往这些规则上套就可以。简单的说，RBO 就像是一个经验丰富的老司机，基本优化套路全都知道。例如谓词下推、列裁剪、常量替换等等。</p>


<h5 class="relative group">谓词下推 
    <div id="%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8" aria-label="锚点">#</a>
    </span>        
    
</h5>
<p>​	谓词下推（Predicate Pushdown）基本思想：将过滤表达式尽可能移动至靠近数据源的位置，以使真正执行时能直接跳过无关的数据。</p>
<pre tabindex="0"><code>	谓词，用来描述或判定客体性质、特征或者客体之间关系的词项。比如“x 大于 y”中的“大于”就是一个谓词。在计算机中，谓词下推（Predicate Pushdown）概念中的谓词指返回 Boolean 值即 true 和 false 的函数，或是隐式转换为 Boolean 的函数：
		如 SQL 中的谓词主要有 LKIE 、 BETWEEN 、 IS NULL 、 IS NOT NULL 、 IN 、 EXISTS 等。
		如 Spark 中的 input.filter(x =&gt; x &gt;= 5) 。
		
	在文件格式使用 Parquet 或 ORC 时，甚至可能整块跳过不相关的文件。而 Hive 中的谓词下推主要思想是把过滤条件下推到 Map 端，提前执行过滤，以减少 Map 到 Reduce 的传输数据，提升整体性能。简而言之，就是在不影响结果的情况下，尽量将过滤条件提前执行。在传统数据库的查询系统中谓词下推作为优化手段很早就出现了，谓词下推的目的就是通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能。
	
	对于 JOIN(INNER JOIN)、FULL OUTER JOIN，条件写在 ON 后面，还是 WHERE 后面，性能上面没有区别；
	对于 LEFT OUTER JOIN，右侧的表写在 ON 后面、左侧的表写在 WHERE 后面，性能上有提高；
	对于 RIGHT OUTER JOIN，左侧的表写在 ON 后面、右侧的表写在 WHERE 后面，性能上有提高；
	当条件分散在两个表时，谓词下推可按上述结论 2 和 3 自由组合。
	
	如果在表达式中含有不确定函数，整个表达式的谓词将不会被 Pushed，例如 RAND() 。因为不确定函数，在编译的时候无法得知，所以，整个表达式不会被 Pushed。
	
Hive 开启 PPD 命令如下：
-- 开启 PPD，默认为 true
SET hive.optimize.ppd=true;
</code></pre>

<h5 class="relative group">列裁剪&amp;常量替换 
    <div id="%E5%88%97%E8%A3%81%E5%89%AA%E5%B8%B8%E9%87%8F%E6%9B%BF%E6%8D%A2" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%88%97%E8%A3%81%E5%89%AA%E5%B8%B8%E9%87%8F%E6%9B%BF%E6%8D%A2" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>	列裁剪（Column Pruning）表示扫描数据源的时候，只读取那些与查询相关的字段。
　　常量替换（Constant Folding）表示将表达式提前计算出结果，然后使用结果对表达式进行替换。假设我们在部门编号上加的过滤条件是 deptno &gt; 5 + 5 ，Catalyst 会使用 ConstantFolding 规则，自动帮我们把条件变成 deptno &gt; 10 。再比如，我们在 SELECT 语句中，掺杂了一些常量表达式，Catalyst 也会自动地用表达式的结果进行替换。
</code></pre>

<h4 class="relative group">CBO 优化 
    <div id="cbo-%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#cbo-%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h4>
<p>​	CBO（Cost-Based Optimization）意为基于代价优化的策略，它需要计算所有可能执行计划的代价，并挑选出代价最小的执行计划。</p>
<pre tabindex="0"><code>	传统的数据库，成本优化器做出最优化的执行计划是依据统计信息来计算的。Hive 的成本优化器也一样，Hive 在提供最终执行前，优化每个查询的执行逻辑和物理执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等。
	
要使用基于成本的优化，需要在查询开始时设置以下参数：
# 开启 CBO 优化，默认为 true
SET hive.cbo.enable=true;
# 统计 SQL 的查询结果是否从统计信息中获取，默认为 true
SET hive.compute.query.using.stats=true;
# 是否统计列信息，默认为 false
SET hive.stats.fetch.column.stats=true;
# 是否统计分区信息，默认为 true。3.1.1 版本被废弃，不允许用户修改该属性，因为禁用分区状态的获取可能会导致分区表出现问题
SET hive.stats.fetch.partition.stats=true;

然后统计表的相关信息才能使用 CBO 优化：
# 新创建的表或者分区，插入数据时是否统计其信息，默认为 true
# 新创建的表或者分区，如果通过 INSERT OVERWRITE 的方式插入数据，那么 Hive 会自动将该表或分区的统计信息更新到元数据
SET hive.stats.autogather=true;

# 对于已经存在表或分区可以通过 ANALYZE 命令手动更新其 Statistics 信息
# 统计全表的所有分区的信息
ANALYZE TABLE 表名 COMPUTE STATISTICS;
# 只统计文件数和文件大小，不扫描文件行数，执行较快
ANALYZE TABLE 表名 COMPUTE STATISTICS NOSCAN;
# 统计指定字段的信息
ANALYZE TABLE 表名 COMPUTE STATISTICS FOR COLUMNS 列名1,列名2,列名n;
# 统计指定分区的信息
ANALYZE TABLE 表名 PARTITION(分区列1=值1, 分区列2=值2) COMPUTE STATISTICS;

对于非分区表列的 Statics 信息存在 Hive 元数据表 TABLE_COL_STATS 中；
对于分区表列的 Statics 信息存在 Hive 元数据表 PART_COL_STATS 中。
</code></pre>

<h4 class="relative group">JOIN 优化 
    <div id="join-%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#join-%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	而 Hive JOIN 的底层是通过 MapReduce 来实现的，Hive 实现 JOIN 时，为了提高 MapReduce 的性能，提供了多种 JOIN 方案：	
	小表 JOIN 大表的 Map Join
	大表 JOIN 大表的 Reduce Join，Reduce Join 又分为以下两种：
		Bucket Map Join（中型表和大表 JOIN）
		Sort Merge Bucket Join（大表和大表 JOIN）
</code></pre>

<h5 class="relative group">Map Join 
    <div id="map-join" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#map-join" aria-label="锚点">#</a>
    </span>        
    
</h5>
<p>​	Map Join 顾名思义，就是在 Map 阶段进行表之间的连接。而不需要进入到 Reduce 阶段才进行连接。这样就节省了在 Shuffle 阶段时要进行的大量数据传输。从而起到了优化作业的作用。Map Join 简单说就是在 Map 阶段将小表读入内存，顺序扫描大表完成 Join。</p>
<pre tabindex="0"><code>	通过 MapReduce Local Task 将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会对HashTableFiles 进行压缩。
	MapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务。
	
应用场景：小表 JOIN 大表或者小表 JOIN 小表。

实现原理：
	Map Join 会把小表全部读入内存中，在 Map 阶段直接拿另外一个表的数据和内存中的表数据做匹配，由
于在 Map 阶段进行了 JOIN 操作，底层不需要经过 Shuffle，这样就不会由于数据倾斜导致某个 Reduce 上落数据太多而失败，但是需要占用内存空间存放小表数据。

具体使用：尽量使用 Map Join 来实现 JOIN 过程，Hive 中默认开启了 Map Join：
-- 是否开启自动转为 Map Join 功能，默认为 true
SET hive.auto.convert.join=true;
-- 2.0 版本之前的控制参数
-- 如果打开上面这个参数，当参与 Join 的表小于下面参数指定的值，将转换为 Map Join，默认为 25000000B
SET hive.mapjoin.smalltable.filesize=25000000;
-- 2.0 版本新增的控制参数
-- 假设参与 Join 的表(或分区)有 N 个，是否启用基于输入文件的大小，将多个 Map Join 合并为一个，默认为 true 
SET hive.auto.convert.join.noconditionaltask=true;
-- 假设参与 Join 的表(或分区)有 N 个，如果打开上面这个参数，并且有 N-1 个小表(或分区)的大小总和小于下面参数指定的值，那么会直接将多个 Map Join 转换为 1 个，默认为 10000000B
SET hive.auto.convert.join.noconditionaltask.size=10000000;

	使用优化的 Map Join 过程中没有 Shuffle 是通过本地的一个 HashTable 较小的表（较小的表的识别可以通过元数据信息判断）生成 HashTable Files 文件，并保存到 HDFS 的临时缓存当中，然后通过与 Map 出来的另一个表进行直接匹配，得出结果，因此过程中没有 Shuffle，不需要网络，所以效率相对来说较快，即为优化。
</code></pre>

<h5 class="relative group">Reduce Join 
    <div id="reduce-join" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#reduce-join" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>　　Map 端的主要工作：为来自不同表或文件的 key/value 对，打标签以区别不同来源的记录。然后用连接字段作为
Key，其余部分和新加的标志作为 Value，最后进行输出。
　　Reduce 端的主要工作：在 Reduce 端以连接字段作为 Key 的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录进行合并即可。
　　
　　应用场景：大表 JOIN 大表。
　　实现原理：将两张表的数据在 Shuffle 阶段利用 Shuffle 的分组将数据按照关联字段进行合并。
　　具体使用：Hive 会自动判断是否满足 Map Join，如果不满足 Map Join，则会自动执行 Reduce Join。
</code></pre>

<h5 class="relative group">Bucket Map Join 
    <div id="bucket-map-join" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#bucket-map-join" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>	Bucket Map Join大表对小表应该使用 Map Join 来进行优化，但是如果是大表对大表，如果进行 Shuffle，那就非常可怕，第一个慢不用说，第二个容易出异常。此时就可以使用 Bucket Join 来进行优化，而 Bucket Join 又分为：
	Bucket Map Join
	Sort Merge Bucket Join（SMB Join）
	
具体使用流程如下：
	将两张大表的数据构建分桶
	数据按照分桶的规则拆分到不同的文件中
		分桶规则 =  MapReduce 分区的规则 =  Key 的 Hash 取余
		Key =  分桶的字段
	只需要实现桶与桶的 JOIN 即可，减少了比较次数
	分桶本质：底层 MapReduce 的分区，桶的个数 = Reduce 个数 = 文件个数。
</code></pre><pre tabindex="0"><code>	两张表 JOIN 的时候，小表不足以放到内存中，但是又想用 Map Join，这个时候就要用到 Bucket Map Join。其方法是两个 JOIN 表都在 Join Key上都做 Hash Bucket，并且把你打算复制的那个（相对）小表的 Bucket 数设置为大表的倍数。这样数据就会按照 Key Join 做 Hash Bucket。小表依然复制到所有节点，Map Join 的时候，小表的每一组 Bucket 加载成HashTable，与对应的一个大表 Bucket 做局部 JOIN，这样每次只需要加载部分 HashTable 就可以了。
	
Map Join 条件：
	SET hive.optimize.bucketmapjoin=true; ，默认为 false
	所有要 JOIN 的表必须分桶，如果表不是 Bucket 的，则只是做普通 JOIN
	大表的 Bucket 数是小表的 Bucket 数的整数倍（或相等）
	Bucket 列 == JOIN 列
	必须是应用在 Map Join 的场景中
</code></pre>

<h5 class="relative group">SMB Join 
    <div id="smb-join" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#smb-join" aria-label="锚点">#</a>
    </span>        
    
</h5>
<pre tabindex="0"><code>	SMB Join 是基于 Bucket Map Join 的有序 Bucket，可实现在 Map 端完成 JOIN 操作，只要桶内的下一条不是，就不用再比较了，有效地减少或避免 Shuffle 的数据量。
	
	SMB Join 的条件和 Map Join 类似但又不同：
		SET hive.optimize.bucketmapjoin=true;
		SET hive.optimize.bucketmapjoin.sortedmerge=true;
		SET hive.auto.convert.sortmerge.join=true;
		
	所有要 JOIN 的表必须分桶，如果表不是 Bucket 的，则只是做普通 JOIN
	大表的 Bucket 数 = 大表的 Bucket 数
	Bucket 列 == JOIN 列 == SORT 列
	必须是应用在 Bucket Map Join 的场景中	
</code></pre>

<h3 class="relative group">数据倾斜 
    <div id="%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">定义 
    <div id="%E5%AE%9A%E4%B9%89-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AE%9A%E4%B9%89-1" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	数据倾斜，即单个节点任务所处理的数据量远大于同类型任务所处理的数据量，导致该节点成为整个作业的瓶颈，这
是分布式系统不可能避免的问题。
	MapReduce 模型中，数据倾斜问题是很常见的，因为同一个 Key 的 Values，在进行 GroupByKey、CountByKey、ReduceByKey、Join 等操作时一定是分配到某一个节点上的一个 Task 中进行处理的，如果某个 Key 对应的数据量特别大的话，就会造成某个节点的堵塞甚至宕机的情况。在并行计算的作业中，整个作业的进度是由运行时间最长的那个 Task 决定的，在出现数据倾斜时，整个作业的运行将会非常缓慢，甚至会发生 OOM 异常。
</code></pre>

<h4 class="relative group">原因 
    <div id="%E5%8E%9F%E5%9B%A0-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%8E%9F%E5%9B%A0-1" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>从本质来说，导致数据倾斜有两种原因
	任务读取大文件，最常见的就是读取压缩的不可分割的大文件。
	任务需要处理大量相同键的数据。
		单表聚合操作，部分 Key 数据量较大，且大 Key 分布在很多不同的切片。
		两表进行 JOIN，都含有大量相同的倾斜数据键。
		数据含有大量无意义的数据，例如空值(NULL)、空字符串等。
		含有倾斜数据在进行聚合计算时无法聚合中间结果，大量数据都需要经过 Shuffle 阶段的处理，引起数据倾		斜。
		数据在计算时做多维数据集合，导致维度膨胀引起的数据倾斜。	
</code></pre>

<h4 class="relative group">解决 
    <div id="%E8%A7%A3%E5%86%B3" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%A7%A3%E5%86%B3" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>压缩引发的数据倾斜
	当对文件使用 GZIP压缩等不支持文件分割操作的压缩方式，在日后有作业涉及读取压缩后的文件时，该压缩文件只会被一个任务所读取。如果该压缩文件很大，则处理该文件的 Map 需要花费的时间会远多于读取普通文件的 Map 时间，该 Map 任务会成为作业运行的瓶颈。这种情况也就是 Map 读取文件的数据倾斜。为避免因不可拆分大文件而引发数据读取的倾斜，在数据压缩的时候可以采用 BZip2 和 Zip 或 LZO 支持文件分割的压缩算法。
	
	
单表数据倾斜优化
	为了减少 Shuffle 数据量以及 Reduce 端的压力，通常 SQL 会在 Map 端做一个 Partial Aggregate（被称为预聚合或偏聚合），即在 Shuffle 前将同一分区内相同 Key 的记录先进行一个预计算，再将结果进行 Shuffle，发送到 Reduce 端做汇总，比如 MapTask 的提前 Combiner。
	适用场景：聚合类的 Shuffle 操作，部分 Key 数据量较大，且大 Key 的数据分布在很多不同的切片。
　　 解决逻辑：两阶段聚合（加盐局部聚合+去盐全局聚合）+Map-Side聚合（开启 Map端聚合或自定义Combiner）。
　　 首先，通过自定义函数给每个数据的 Key 添加随机数前缀，对 Key 进行打散，将原先一样的 Key 变成不一样的 Key，然后进行第一次聚合，这样就可以让原本被一个 Task 处理的数据分散到多个 Task 上去做局部聚合；随后，去除掉每个Key 的前缀，再次进行聚合。
　　 
　　 
业务无关数据引发的数据倾斜
	　实际业务中有些大量的 NULL 值（空 Key）或者一些无意义的数据参与到计算作业中，这些数据可能来自业务为了上报或因数据规范将某类数据进行归一化变成空值或空字符等形式，这些与业务无关的数据导致在进行分组聚合或者在执行表连接时发生数据倾斜。对于这类问题引发的数据倾斜，在计算过程中排除含有这类“异常”数据即可。
	　空 Key 过滤使用场景：
		非 INNER JOIN 的查询
		不需要字段为 NULL 的查询
　	  　核心思想：先过滤再 JOIN。
	如果业务需求必须包含 NULL 值返回，此时我们可以为空 Key字段赋一个随机值，使得数据随机均匀地分散到不同的 Reduce 上。
	
	
无法消减中间结果的数据量引发的数据倾斜
	在一些操作中无法消减中间结果，例如使用 COLLECT_LIST 聚合函数（其实也是 GROUP BY优化）。在student 表中，假设 age 有数据倾斜，可以通过开启 hive.groupby.skewindata 将作业拆解成两个作业，第一个作业会尽可能将 Map 的数据平均分发到不同的 Reduce，每个 Reduce 实现预聚合，这样的处理结果是相同的 Key 会被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个作业在第一个作业处理的数据基础上，按 Key 分发到 Reduce 中，这个过程可以保证相同的 Key 会被分发到同一个 Reduce，完成最终聚合。

多维聚合计算数据膨胀引起的数据倾斜
</code></pre><pre tabindex="0"><code>JOIN 数据倾斜优化
Map JOIN
	在 Hive JOIN 策略中，当一张小表足够小并且可以先缓存到内存中时，可以使用 Map Join。Map Join 顾名思义，就是在 Map 阶段进行表之间的连接。而不需要进入到 Reduce 阶段才进行连接。这样就节省了在 Shuffle 阶段时要进行的大量数据传输。从而起到了优化作业的作用。
	Map Join 简单说就是在 Map 阶段将小表读入内存，顺序扫描大表完成 Join。
　　适用场景：适用于小表 JOIN 大表。小表足够小。
　　解决方案：
　　在小表 JOIN 大表时如果产生数据倾斜，那么 Map JOIN 可以直接规避掉 Shuffle 阶段（默认开启）。
　　
大小表 JOIN
	采样倾斜 Key 并分拆 JOIN，适用场景：适用于 JOIN 时出现数据倾斜。
	解决方案：
	将存在倾斜的表，根据抽样结果，拆分为倾斜 Key（Skew 表） 和没有倾斜 Key（Normal 表）的两个数据集。
	将 Skew 表的 Key 全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集做笛卡尔乘积（即将数据量扩大 N 倍，得到 New 表）。
	打散的 Skew 表 JOIN 扩容的 New 表，普通表 JOIN 未扩容的 Old 表，最后将这两部分结果 UNION ALL。
	
Skew Join
	两表进行普通的 JOIN 时，如果表的连接键存在倾斜，那么 Shuffle 阶段必然会引起数据倾斜。遇到这种情况，Hive 的通常做法是使用 Skew Join（启动两个作业），第一个作业处理没有倾斜的数据，第二个作业将倾斜的数据存到分布式缓存中，分发到各个 Map 任务的所在节点。在 Map 阶段完成 JOIN 操作，即 Map Join，这样避免了 Shuffle，从而避免了数据倾斜。
	Skew Join 是 Hive 自身对于数据倾斜的优化方案。Skew Join 是 Hive 中一种专门为了避免数据倾斜而设计的特殊的Join 过程，这种 Join 的原理是将 Map Join 和 Reduce Join 进行合并，如果某个值出现了数据倾斜，就会将产生数据倾斜的数据单独使用 Map Join 来实现，其他没有产生数据倾斜的数据由 Reduce Join 来实现，这样就避免了 Reduce Join 中产生数据倾斜的问题。最终将 Map Join 的结果和 Reduce Join 的结果进行合并。

Skew Join 的实现原理
	首先判断 HQL 是否会产生数据倾斜；
	如果会产数据倾斜并超过 hive.skewjoin.key 阈值使用 Map Join 来处理；
	如果不会产生数据倾斜使用 Reduce Join 来处理；
	将 Map Join 的结果和 Reduce Join 的结果进行合并。
	
关于 Skew Join 的相关设置如下：
-- 是否开启 Skew Join，默认为 false
SET hive.optimize.skewjoin=true;
-- 当 Key 超过阈值时作为一个 Skew Join 处理，默认为 100000
SET hive.skewjoin.key=100000;
-- 用于处理 Skew Join 的 MapTask 的最大数量，默认为 10000
SET hive.skewjoin.mapjoin.map.tasks=10000;
</code></pre>

<h3 class="relative group">资源优化 
    <div id="%E8%B5%84%E6%BA%90%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%B5%84%E6%BA%90%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h3>


<h4 class="relative group">向量化查询 
    <div id="%E5%90%91%E9%87%8F%E5%8C%96%E6%9F%A5%E8%AF%A2" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%90%91%E9%87%8F%E5%8C%96%E6%9F%A5%E8%AF%A2" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre><code>	向量化执行，可以简单地看作一项消除程序中循环的优化。为了实现向量化执行，需要利用 CPU 的 SIMD 指令。SIMD 的全称是 Single Instruction Multiple Data，即用单条指令操作多条数据。现代计算机系统概念中，它是通过数据并行以提高性能的一种实现方式（其他的还有指令级并行和线程级并行），它的原理是在 CPU 寄存器层面实现数据的并行操作。在计算机系统的体系结构中，存储系统是一种层次结构。存储媒介距离 CPU 越近，则访问数据的速度越快。
	SIMD：全称 Single Instruction Multiple Data，中文翻译叫单指令多数据流，可以使用一条指令同时完成多个数据的运算操作。相对的，最传统的指令架构是 SISD 就是单指令单数据流，每条指令只能对一个数据执行操作。
</code></pre>
<pre tabindex="0"><code>	Hive 不仅支持将数据按列存储，而且按列进行计算。传统 OLTP 数据库通常采用按行计算，原因是事务处理中以点查为主，SQL 计算量小，实现这些技术的收益不够明显。但是在分析场景下，单个 SQL 所涉及计算量可能极大，将每行作为一个基本单元进行处理会带来严重的性能损耗：
	对每一行数据都要调用相应的函数，函数调用开销占比高；
	存储层按列存储数据，在内存中也按列组织，但是计算层按行处理，无法充分利用 CPU Cache 的预读能力，造成 	CPUCache Miss 严重；
	按行处理，无法利用高效的 SIMD 指令；
	
	Hive 实现了向量执行引擎（Vectorized Execution Engine），对内存中的列式数据，一个 Batch 调用一次 SIMD 指令（而非每一行调用一次），不仅减少了函数调用次数、降低了 Cache Miss，而且可以充分发挥 SIMD 指的并行能力，大幅缩短了计算耗时。向量执行引擎，通常能够带来数倍的性能提升。
	
	数据加载出来之后，每批数据中的每一列都会转成一个向量，在后续的执行过程中，数据是一个批批从一个操作符流经另一个操作符，而不是一行行的。
	总结下来就是：让计算更多的停留在函数内，而不是频繁的交互切换，提高了 CPU 的流水线并行度。数据不仅按列式存储，而且按列（Batch Data）计算。
	
	开启向量化查询后，Hive 会将一个普通的查询转化为向量化查询执行。它大大减少了扫描、过滤器、聚合和连接等典型查询操作的 CPU 使用。标准查询执行系统一次处理一行。向量化查询执行可以一次性处理 1024 行的数据块，以减少底层操作系统处理数据时的指令和上下文切换。
	开启向量化查询命令如下：
-- 开启向量化查询，默认为 true
SET hive.vectorized.execution.enabled=true;
-- 开启 Reduce 任务的向量化执行模式，默认为 true（MR 计算引擎不支持，需要配置 Tez/Spark 计算引擎使用）
SET hive.vectorized.execution.reduce.enabled=true;
	Hive 中，向量化查询执行在 Hive 0.13.0 及以后版本可用，默认开启：
SET hive.vectorized.execution.enabled=true;

　　Hive 向量化执行支持数据类型有：tinyint、smallint、int、bigint、boolean、float、double、decimal、date、timestamp、string。如果使用了其他数据类型，查询将不会使用向量化执行，而是每次只会查询一行数据。可以通过Explain 来查看查询是否使用了向量化，如果输出信息中 Execution mode 的值为 vectorized ，则使用了向量化查询
</code></pre>

<h4 class="relative group">存储优化 
    <div id="%E5%AD%98%E5%82%A8%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AD%98%E5%82%A8%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Hive 的存储本质还是 HDFS，而 HDFS 是不利于小文件存储的，因为每个小文件都会产生一条元数据信息，并且不利于 MapReduce 的处理。Hive 中提供了一个特殊的机制，可以自动的判断是否是小文件，如果是小文件就将小文件进行合并。
	
-- 在 Map-Only 任务结束时合并小文件，默认为 true
SET hive.merge.mapfiles=true;
-- 在 Map-Reduce 任务结束时合并小文件，默认为 false
SET hive.merge.mapredfiles=true;
-- 合并文件的大小，默认约 244M
SET hive.merge.size.per.task=256000000;
-- 平均每个文件的大小，如果小于该值则进行合并，默认约 15M
SET hive.merge.smallfiles.avgsize=16000000;

Hive 中提供了一个 CombineHiveInputFormat 类专门用于小文件合并（默认启用）。相关配置命令如下：
SET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
</code></pre>

<h4 class="relative group">YARN 优化 
    <div id="yarn-%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#yarn-%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	配置每个容器请求被分配的最小内存。如果容器请求的资源小于该值，会以 1024MB 进行分配；如果NodeManager 可被分配的内存小于该值，则该 NodeManager 将会被 ResouceManager 给关闭。默认值 1024MB。
	SET yarn.scheduler.minimum-allocation-mb=1024;
	
	　配置每个容器请求被分配的最大内存。如果容器请求的资源超过该值，程序会抛出InvalidResourceRequestException异常。默认值 8192MB。
	　SET yarn.scheduler.maximum-allocation-mb=8192;
	　
	　配置每个容器请求被分配的最小虚拟 CPU 个数，低于此值的请求将被设置为此属性的值。此外，配置为虚拟内核少于此值的 NodeManager 将被 ResouceManager 关闭。默认值 1。
	　SET yarn.scheduler.minimum-allocation-vcores=1;
	　
	　配置每个容器请求被分配的最大虚拟CPU个数，高于此值的请求将抛出InvalidResourceRequestException 的异常。如果开发者所提交的作业需要处理的数据量较大，需要关注上面配置项的配置。
	　SET yarn.scheduler.maximum-allocation-vcores=4;
	　
	　配置一个节点内所有容器所能使用的物理 CPU 的占比，默认为 100%。即如果一台机器有 16 核，CPU 的使用率最大为 16 核，该比值为 100%，如果该比值为 50%，则所有容器能使用的 CPU 资源为 8 核。
	　SET yarn.nodemanager.resource.percentage-physical-cpu-limit=100;
	　
	　配置是否开启 CPU 的共享模式。共享模式告诉系统容器除了能够使用被分配的 CPU 资源外，还能使用空闲的 CPU 资源。默认值 false。
	　SET yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false;
</code></pre>

<h4 class="relative group">并行执行 
    <div id="%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Hive 在实现 HQL 计算运行时，会解析为多个 Stage，有时候 Stage 彼此之间有依赖关系，只能挨个执行。但有些时候，很多 Stage 之间是没有依赖关系的，例如 UNION 语句，JOIN 语句等等，这些 Stage 没有依赖关系，但是 Hive 依旧会挨个执行每个 Stage，这样会导致性能非常的差。我们可以通过修改参数，开启并行执行，当多个 Stage 之间没有依赖关系时，允许多个 Stage 并行执行，提高性能
	
-- 开启任务并行执行，默认为 false
SET hive.exec.parallel=true;
-- 同一条 SQL 的并行化线程数，默认为 8
SET hive.exec.parallel.thread.number=8;
	需要注意的是，在共享集群中，如果 Job 中并行阶段增多，那么集群利用率就会增加。系统资源比较空闲的时候才会有优势，如果系统资源比较吃紧，没资源意味着并行也无法启动。
</code></pre>

<h4 class="relative group">JVM 重用 
    <div id="jvm-%E9%87%8D%E7%94%A8" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#jvm-%E9%87%8D%E7%94%A8" aria-label="锚点">#</a>
    </span>        
    
</h4>
<pre tabindex="0"><code>	Hadoop 默认会为每个 Task 启动一个 JVM 来运行（JVM 启动时内存开销大）。当 Job 数据量大的时候，如果单个Task 数据量比较小，也会申请 JVM，这就导致了资源紧张及浪费的情况。JVM 重用可以使得 JVM 实例在同一个 Job 中重新使用 N 次，当一个 Task 运行结束以后，JVM 不会进行释放，而是继续供下一个 Task 运行，直到运行了 N 个Task 以后，就会释放。
	
	目前 Hadoop3 中已不再支持该配置，如果大家使用的是 Hadoop3 以前的版本可以通过修改 mapred-site.xml 修改该选项（默认值 1）：
	
&lt;!-- -1 表示一个 JVM 能够运行的任务数（来自同一个 Job）是没有限制的，通常在 10~20 之间 --&gt;
&lt;property&gt;
  &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt;
  &lt;value&gt;-1&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h3 class="relative group">聚合优化 
    <div id="%E8%81%9A%E5%90%88%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E8%81%9A%E5%90%88%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>GROUP BY 优化
	默认情况下，Map 阶段同一 Key 数据会分发给一个 Reduce，当一个 Key 数据过大时就会产生数据倾斜。但并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作也可以先在 Map 端进行部分聚合，最后再在 Reduce 端得出最终结果。
	
	开启 Map 端聚合相关选项如下：
-- 是否开启 Map 端聚合，默认为 true
SET hive.map.aggr=true;
-- Map 端进行聚合操作的条目数上限，默认为 100000
SET hive.groupby.mapaggr.checkinterval=100000;

	GROUP BY 数据倾斜的负载均衡处理：
-- 开启该参数后，当前程序会自动通过两个作业来运行任务，实现数据倾斜的负载均衡处理，默认为 false
SET hive.groupby.skewindata=true;
</code></pre><pre tabindex="0"><code>ORDER BY 优化
	ORDER BY 会将结果按某字段全局排序，将导致所有 Map 端数据都进入一个 Reducer 中，数据量大时可能会长时间计算不完。由于 Hive 中的 ORDER BY 对于大数据集存在性能问题，延伸出了部分排序。
	
	使用部分排序 SORT BY 会视情况启动多个 Reducer 进行排序，保证每个 Reducer 内局部有序，常与 DISTRIBUTE BY 搭配使用，DISTRIBUTE BY 可以保证相同 Key 的记录会被划分到一个 Reduce 中，不使用 DISTRIBUTE BY，Map 端数据就会随机分配到 Reducer。
	
	使用 SORT BY 可以配合指定执行的 Reduce 个数 （ SET mapred.reduce.tasks=&lt;number&gt; ）。SORT BY 结束后，对输出的数据再执行归并排序，即可以得到全部结果。能否利用 SORT BY 与 CLUSTER BY 达到 ORDER BY 一样的功能？ 答案是不能，因为 SORT BY 是局部有序，必须外面再嵌套一层。
</code></pre><pre tabindex="0"><code>COUNT(DISTINCT) 优化
	站在 MapReduce 的角度，由于 COUNT(DISTINCT) 操作需要用一个 ReduceTask 来完成，所以这个 Reduce 需要处理的数据量可能会非常大，会导致整个 Job 很难完成，所以一般 COUNT(DISTINCT) 使用先 GROUP BY 再 COUNT 的方式进行替换，但是需要注意 GROUP BY 造成的数据倾斜。
	
-- 改写前
SELECT COUNT(DISTINCT job) FROM emp;
-- 改写后
SELECT COUNT(*) FROM (SELECT job FROM emp GROUP BY job) e;
</code></pre>

<h3 class="relative group">Job 优化 
    <div id="job-%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#job-%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>Map 优化
	通常情况下，作业通过 Input 目录会产生一个或者多个 Map 任务。主要的决定因素有：Input 的文件总个数，Input 的文件大小，集群设置的文件块大小（默认 128M）。
	
	是不是 Map 数越多越好？
	如果一个任务有很多小文件（远远小于块大小 128M），则每个小文件也会被当做一个块，用一个Map 任务来完成，而一个 Map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 Map 数是受限的。解决：合并小文件。
	
	是不是保证每个 Map 处理接近 128M 的文件块，就可以高枕无忧了？
	答案也是不一定。比如有一个 127M 的文件，正常会用一个 Map 去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果 Map 处理的逻辑比较复杂，用一个 Map 任务去做，肯定也比较耗时的。解决：Split 分片。
	
	控制 Map 数量需要遵循两个原则：使大数据量利用合适的 Map 数；使单个 Map 任务处理合适的数据量。
	
关于切片设置 MapReduce 关键源码：
// getFormatMinSplitSize()：一个切片最少应该拥有 1 个字节
// getMinSplitSize(job)：读取程序员设置的切片的最小值，如果没有设置默认读取 1
long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
// 读取程序员设置的切片的最大值，如果没有设置默认读取 Long.MAX_VALUE
long maxSize = getMaxSplitSize(job);
// ...
// 获取 Block 的大小，默认为 128M
long blockSize = file.getBlockSize();
// 获取 Split 的大小，切片的默认大小为 128M
// return Math.max(minSize, Math.min(maxSize, blockSize));
// minSize 为 64M --&gt; 最终返回 128M，minSize 为 256M --&gt; 最终返回 256M
// maxSize 为 64M --&gt; 最终返回 64M，maxSize 为 256M --&gt; 最终返回 128M
// 如果需要调大切片，需要调节 minSize；如果需要调小切片，则调节 maxSize
long splitSize = computeSplitSize(blockSize, minSize, maxSize);
// computeSplitSize 方法的核心代码如下：
return Math.max(minSize, Math.min(maxSize, blockSize));

minSize 设置为 64M --&gt; 最终返回 128M
minSize 设置为 256M --&gt; 最终返回 256M
maxSize 设置为 64M --&gt; 最终返回 64M
maxSize 设置为 256M --&gt; 最终返回 128M

因此，如果需要调大切片，则调节 minSize；如果需要调小切片，则调节 maxSize。
-- 如果需要调小切片，则调节 maxSize，默认值 256000000
SET mapred.max.split.size=256000000;
-- 如果需要调大切片，需要调节 minSize，默认值 1
SET mapred.min.split.size=1;
-- 每个节点处理的最⼩ Split，默认值 1
SET mapred.min.split.size.per.node=1;
-- 每个机架处理的最⼩ Split，默认值 1
SET mapred.min.split.size.per.rack=1;
-- 存储优化合并小文件
SET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
</code></pre><pre tabindex="0"><code>Reduce 优化
	Reduce 个数的设定极大影响任务执行效率，不指定 Reduce 个数的情况下，Hive 会基于以下两个设定计算出 Reduce的个数：
-- 每个 Reduce 处理的数据量，默认值 256000000
SET hive.exec.reducers.bytes.per.reducer=256000000;
-- 每个任务最大的 Reduce 数，默认值 1009
SET hive.exec.reducers.max=1009;

	计算 Reducer 数的公式为：N=min(参数2，总输入数据量/参数1)。
	
	调整 Reduce 个数方法一：
	SET hive.exec.reducers.bytes.per.reducer=256000000;
	调整 Reduce 个数方法二：
	-- 设置 Reduce 的个数，默认值 -1（-1 时会通过计算得到 Reduce 个数）
	SET mapred.reduce.tasks=-1;
	关于单个 Reduce 写出文件的块的大小，可以在执行任务前通过 dfs.blocksize 来进行设置。
	-- HDFS Block 大小
	SET dfs.blocksize;
	SET dfs.block.size;
	
	Reduce 是不是越多越好？
	同 Map 一样，启动和初始化 Reduce 也会消耗时间和资源；另外，有多少个 Reduce 就会有多少个输出文件，如果生成了很多个小文件，这些小文件又作为下一个任务的输入，则也会出现小文件过多的问题。
	
	什么情况下只有一个 Reduce？
	很多时候你会发现任务中不管数据量多大，不管你有没有设置调整 Reduce 个数的参数，任务中一直都只有一个
Reduce 任务。其实只有一个 Reduce 任务的情况，除了数据量小于 hive.exec.reducers.bytes.per.reducer 参数值的情况外，还有以下原因：
	没有 GROUP BY 的汇总，比如把 SELECT a, COUNT(*) FROM t WHERE a = &#39;三年二班&#39; GROUP BY a; 写成 SELECTCOUNT(*) FROM t WHERE a = &#39;三年二班&#39;;用了 ORDER BY；有笛卡尔积
	
	控制 Reduce 数量需要遵循两个原则：使大数据量利用合适的 Reduce 数；使单个 Reduce 任务处理合适的数据量。
	
Shuffle 优化
	减少IO流和磁盘流。
</code></pre>

<h3 class="relative group">其他优化 
    <div id="%E5%85%B6%E4%BB%96%E4%BC%98%E5%8C%96" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%85%B6%E4%BB%96%E4%BC%98%E5%8C%96" aria-label="锚点">#</a>
    </span>        
    
</h3>
<pre tabindex="0"><code>Fetch 模式
	当我们执行一个简单的 HQL 语句时，例如 SELECT * FROM emp; ，发现数据可以很快的返回，这其实涉及到 Fetch抓取的概念。相关配置为：	SET hive.fetch.task.conversion;
	该配置默认值为 more ，如果设置为 none ，那么每次执行 HQL 都会执行 MapReduce 程序。
	
多重模式
	如果碰到一堆 SQL 都是从同一个表进行扫描，然后做不同的逻辑，可以采用多重模式。例如以下 SQL：
	INSERT INTO t_user1 SELECT id, username FROM t_user;
	INSERT INTO t_user2 SELECT id, username FROM t_user;
	INSERT OVERWRITE TABLE t_user3 SELECT id, username FROM t_user;
	INSERT OVERWRITE TABLE t_user4 SELECT id, password FROM t_user;
	如果有 n 条 SQL，每个 SQL 执行都会扫描一次这张表。可以修改 SQL 为以下模式（多重模式）：
	FROM t_user
	INSERT INTO t_user1 SELECT id, username
	INSERT INTO t_user2 SELECT id, password
	INSERT OVERWRITE TABLE t_user3 SELECT id, username
	INSERT OVERWRITE TABLE t_user4 SELECT id, password;
	一次读取，多次插入，对于需要从一张表读取数据后并多次使用的场景非常管用。
	
关联优化
	当一个程序中有一些操作彼此之间有关联性的时候，是可以在一个 MapReduce 中完成的，但是 Hive 会不智能的选择使用两个 MapReduce 来完成这两个操作。
	方案一：
	第一个 MapReduce 做 GROUP BY，经过 Shuffle 阶段对 id 做分组
	第二个 MapReduce 对第一个 MapReduce 的结果做 ORDER BY，经过 Shuffle 阶段对 id 进行排序
	
	方案二：
	因为都是对 id 处理，可以使用一个 MapReduce 的 Shuffle，既可以做分组也可以做排序
　　Hive 默认采用方案一来执行，这样会导致性能相对较差，通过开启相关选项可以让 Hive 按方案二来执行：
　　
　　-- 开启关联关系优化，默认为 false
	SET hive.optimize.correlation=true;
	
本地模式
	使用 Hive 的过程中，有一些数据量不大的表也会转换为 MapReduce 执行。而只要提交到集群，就需要申请资源、等待资源分配、启动 JVM 进程，再运行 Task，一系列的过程比较繁琐。Hive 为了解决这个问题，延用了 MapReduce 中的设计，提供了本地计算模式，允许程序不提交给 YARN，直接在本地运行，以便于提高小数据量程序的性能。对数据量比较小的操作，就可以在本地执行，这样要比提交任务到集群执行快很多。
	
	开启本地模式后，Hive 会判断当前作业执行的前置环境，如果满足环境才会在本地执行。判定条件如下：
	ob 的输入数据大小必须小于参数： hive.exec.mode.local.auto.inputbytes.max （默认 128M）
	Job 的 Map 数必须小于参数： hive.exec.mode.local.auto.tasks.max （默认 4）
	Job 的 Reduce 数必须为 1 或 0： mapred.reduce.tasks （默认 -1）。
	
　　开启本地模式命令如下：SET hive.exec.mode.local.auto=true;

严格模式
	Hive 通过参数 hive.mapred.mode 来设置是否开启严格模式。目前参数值有两个：strict（严格模式）和 nostrict（非严格模式，默认）。
　　开启严格模式，主要是为了禁止某些查询（这些查询可能会造成意想不到的坏结果），目前主要禁止三种类型的查
询：
	分区表查询时，必须在 WHERE 语句后指定分区字段，否则不允许执行。因为在查询分区表时，如果不指定分区查
询，会进行全表扫描。而分区表通常有非常大的数据量，全表扫描非常消耗资源。
	ORDER BY 查询必须带有 LIMIT 语句，否则不允许执行。因为 ORDER BY 会进行全局排序，这个过程会将处理的结果分配到一个 Reduce 中进行处理，处理时间长且影响性能。
	笛卡尔积查询（多使用 JOIN 和 ON 语句查询）。数据量非常大时，笛卡尔积查询会出现不可控的情况，因此严格模式下也不允许执行。
	在开启严格模式下，进行上述三种不符合要求的查询时，通常会报类似 FAILED: Error in semantic analysis:In strict mode, XXX is not allowed. If you really want to perform the operation,+set hive.mapred.mode=nonstrict+ 的错误。
	开启严格模式命令如下：
	-- 是否开启严格模式 strict（严格模式）和 nostrict（非严格模式，默认）
	SET hive.mapred.mode=strict;	

推测执行
	推测执行(Speculative Execution)是指在集群环境下运行 MapReduce，可能是程序 Bug，负载不均或者其他的一些问题，导致在一个 Job 下的多个 Task 速度不一致，比如有的任务已经完成，但是有些任务可能只跑了 10%，根据木桶原理，这些任务将成为整个 Job 的短板，如果集群启动了推测执行，这时为了最大限度的提高短板，Hadoop 会为该 Task 启动备份任务，让 Speculative Task 与原始 Task 同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果，并且在运行完成后 Kill 掉另外一个任务。
	推测执行(Speculative Execution)是通过利用更多的资源来换取时间的一种优化策略，但是在资源很紧张的情况下，推测执行也不一定能带来时间上的优化。所以是否启用推测执行，需要根据资源情况来决定，如果在资源本身就不够的情况下，还要跑推测执行的任务，这样会导致后续启动的任务无法获取到资源，以导致无法执行。
	
	关于推测执行相关配置如下：
-- 是否启用 MapTask 推测执行，默认为 true
SET mapreduce.map.speculative=true;
-- 是否启用 ReduceTask 推测执行，默认为 true
SET mapreduce.reduce.speculative=true;
-- 推测任务占当前正在运行的任务数的比例，默认为 0.1
SET mapreduce.job.speculative.speculative-cap-running-tasks=0.1;
-- 推测任务占全部要处理任务数的比例，默认为 0.01
SET mapreduce.job.speculative.speculative-cap-total-tasks=0.01
-- 最少允许同时运行的推测任务数量，默认为 10
SET mapreduce.job.speculative.minimum-allowed-tasks=10;
-- 本次推测没有任务下发，执行下一次推测任务的等待时间，默认为 1000（ms）
SET mapreduce.job.speculative.retry-after-no-speculate=1000;
-- 本次推测有任务下发，执行下一次推测任务的等待时间，默认为 15000（ms）
SET mapreduce.job.speculative.retry-after-speculate=15000;
-- 标准差，任务的平均进展率必须低于所有正在运行任务的平均值才会被认为是太慢的任务，默认为 1.0
SET mapreduce.job.speculative.slowtaskthreshold=1.0;
-- Hive 是否启用 MapReduce 推测执行，默认为 true
SET hive.mapred.reduce.tasks.speculative.execution=true;
</code></pre>
        </div>
        
        

        
        

          
      </div>
     
      
      
        
        
          
          
        
      <script>
        var oid = "views_docs\\Hive\\Hive.md"
        var oid_likes = "likes_docs\\Hive\\Hive.md"
      </script>
      
      
      
      <script type="text/javascript" src="/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js" integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q&#43;oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script>
  
    </section>
  <footer class="pt-8 max-w-prose print:hidden">

    
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="flex group mr-3" href="/docs/linux/linux/">
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  ></span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="0001-01-01 00:00:00 &#43;0000 UTC">0001-01-01</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="flex text-right group ml-3" href="/docs/hbase/hbase/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  ></span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="0001-01-01 00:00:00 &#43;0000 UTC">0001-01-01</time>
                  
                </span>
              </span>
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


    
  </footer>
</article>

      <div id="top-scroller" class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0">
  <a href="#the-top"
    class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="返回顶部" title="返回顶部">
    &uarr;
  </a>
</div>
    </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
  
  <div class="flex items-center justify-between">

    
    
    <p class="text-sm text-neutral-500 dark:text-neutral-400">
      &copy;
      2024
      KV先生
    </p>
    

    
    
    <p class="text-xs text-neutral-500 dark:text-neutral-400">
      
      
      由 <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
        href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
        href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a> 强力驱动
    </p>
    

  </div>
  <script>
    
    mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
      margin: 24,
      background: 'rgba(0,0,0,0.5)',
      scrollOffset: 0,
    })
    
  </script>
  
  
  <script type="text/javascript" src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js" integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="https://your_domain.com/"
  style="z-index:500"
>
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="搜索"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="关闭 (Esc)"
      >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

  </div>
</body>

</html>
