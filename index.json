
[{"content":" Hadoop MapReduce # https://www.processon.com/view/link/64f680ccf5c8296d88838473\nMapReduce设计思想 # 分而治之：\n对付大数据并行处理，将大的数据切分成多个小数据，交给更多的节点参与运算。注意：不可拆分的计算\r任务或相互间有依赖关系的数据无法进行并行计算。 抽象模型：Input、Split、Map、Shuffle、Reduce、Output。\nInput：读取数据。\rSplit：对数据进行粗粒度切分。\rMap：对数据进行细粒度切分。\rShuffle：洗牌。将各个 MapTask 结果合并输出到 Reduce。\rReduce：对 Shuffle 进行汇总并输出到指定存储。\rOutput：HDFS、Hive、Spark、Flume…… 统一架构：\n程序员需要考虑数据存储、划分、分发、结果手机、错误恢复等诸多细节。因此MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的细节处理。程序员只需要集中于应用问题和算法本身，而不需要关注其他系统层的细节处理，大大减轻了程序员开发程序的负担。 离线框架：可以实现上千台服务器几圈并发工作，适合PB级以上海量数据的离线处理\n不擅长实时计算：MapReduce 无法像 MySQL 一样，在毫秒或者秒级内返回结果。如果数据量小，使用 MR 反而不\r合适。\r不擅长流式计算：流式计算的输入数据是动态的，而 MapReduce 的输入数据集是静态的，不能动态变化。这是因\r为 MapReduce 自身的设计特点决定了数据源必须是静态的。\r不擅长 DAG（有向图）计算：多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况\r下，MapReduce 并不是不能做，而是使用后，每个 MapReduce 作业的输出结果都会写入到磁盘，会造成大量的\r磁盘 IO，导致性能非常的低下。 计算机项数据靠拢：将计算放在数据节点上进行工作\n顺序处理数据，避免随机访问数据\n大规模数据处理的特点决定了大量的数据记录不可能存放在内存、而只可能放在\r外存中进行处理。磁盘的顺序访问和随即访问在性能上有巨大的差异。例：100 亿个数据记录，每个记录 100B，共计\r1TB 的数据库。更新 1% 的记录（随机访问）需要 1 个月时间；而顺序访问并重写所有数据记录仅需 1 天时间。 节点失效是常态\nMapReduce 集群中使用大量的低端服务器（Google 目前在全球共使用百万台以上的服务器节点），因此，节点硬件失效和软件出错是常态。因而：一个良好设计、具有容错性的并行计算系统不能因为节点失效而影响计算服务的质量，任何节点失效都不应当导致结果的不一致或不确定性；任何一个节点失效时，其它节点要能够无缝接管失效节点的计算任务；当失效节点恢复后应能自动无缝加入集群，而不需要管理员人工进行系统配置。MapReduce 并行计算软件框架使用了多种有效的机制，如节点自动重启技术，使集群和计算框架具有对付节点失效的健壮性，能有效处理失效节点的检测和恢复。 常用排序算法 # 分类 # 普遍型 # 指的是学过计算机的基本都会的算法，大家都会了变的普遍了。冒泡排序，选择排序，插入排序\n进阶型 # 希尔排序（高级插入），堆排序（高级选择\n常用型 # 快速排序，归并排序\n偏方型 # 所谓偏方治大病，在某些场景下会有奇效。计数排序，桶排序，基数排序\n快速排序 # ​\t快速排序（Quick sort）从冒泡排序演变而来，实际上是在冒泡排序基础上的递归分治法。快速排序在每一轮挑选一个基准元素，让比他大的元素移动到数列的一边，比他小的元素移动到另一边，从而把数列拆解成两个部分。快排又分为：Hoare法（左右指针）、前后指针法、挖坑法。\n左右指针法 # 定义一个Begin指向第一个元素，定义一个END指向最后一个元素。令第一个元素为Key,Begin向后找大于Key的数，End向前找小于key的数，找到后Begin和End交换位置，直到Begin的索引大于等于End的索引时结束，然后将Key和End指针交换位置。再将Key左右两边重复上述操作，最终有序。 挖坑法 # 从数列中挑出一个元素称为基准。重新排序数列，所有比元素基准小的摆放在基准左边，比基准大的摆在基准右边。\r从右向左依次和基准进行比较，小于基准就和基准进行交换，大于基准就保持不变。\r交换后基准到了右边，所以从左向右依次和基准进行比较，如果大于基准就和基准进行交换，小于基准就保持不变\r交换后基准到了左边，从右向左依次和基准进行比较，小于基准就和基准进行交换，大于基准就保持不变。\r整体排序后基准的位置是正确的，左右两边无序。\r将左右两边各当做一个新的序列继续进行挖坑法，递归的把剩余数列排序。 前后指针法 # 选择一端下标设为基准值\r然后初始设置 Cur、Prev 两个标志指针，Prev 标志序列第一个元素，Cur 标志 Prev 后一个位置；\rCur 位置的元素若大于基准值，Cur 向前前进。若小于基准值，对 Prev 进行加一，然后判断是否与 Cur 的位置相等：\r若相等，Cur 继续向前前进；\r若不相等，则交换 Cur 和 Prev 的值。这样就能保证 Cur 与 Prev 之间的元素都大于基准值，Prev 之前的元\t素都小于基准值。\r重复上述过程，直到 Cur 超过序列最右侧位置，最后进行一次判断，若 Prev 标记位置不是序列最后一个位置，则将基\r准值交换到 Prev 位置，即完成左右子序列划分，再对左右子序列重复上述过程，直到整个序列完成排序 优化选 Key # 快速排序在选 Key 的时候，最理想，效率最高的情况就是每次都能选到中间值。但是，快速排序在没有优化前，对数\r据有序的情况进行排序，那么它每次选 Key 的值都在最左边或最右边，效率就会大大降低。所以：当待排序数组有序时，\r快速排序的时间复杂的最差。\r为解决这一情况引入一个较为巧妙的方法，三数取中。“三数取中”即取三个数中不是最大也不是最小的数，将这一概\r念引入快速排序中，取首中尾三个元素的中间值（排序三个数，取中间数）作为待排序区间的 Key，再把这个元素和队头\r元素互换，即可解决这一问题。一开始基准在最左边，所以从右向左依次和基准进行比较，如果小于基准就和基准进行交换，如果大于基准保持不变 归并排序 # ​\t并归排序（Merge Sort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的典型应用。\n​\t归并排序的基本思想是：将原序列不断拆分，一直拆到每个子序列只有一个元素时，再用插入的方式归并。即先使每个子序列有序，再使子序列段间有序。将已有序的子序列合并，得到完全有序的序列。两个有序表合并成一个有序表的过程称为二路归并。和选择排序一样，归并排序的性能不收输入数据的影响，表现比选择排序好，代价是需要额外的空间内存。\n归并排序算法步骤：\r将原序列二等分，二等分之后的子序列继续二等分；\r直到每个子序列只有一个元素时，停止拆分；\r再按照分割的顺序进行归并。 MapReduce计算流程 # 原始数据File # ​\t数据文件上传（Input），被切分成块（Block）存放在HDFS上，每一块有128M。\n数据块Block # ​\tHDFS上数据存储的一个单元，用一个文件中块的大小都是相同的。因为数据存储到HDFS上不可变，所以有可能块的数量和集群的计算能力不匹配。需要一个动态调整本次参与计算节点数量的一个单位。可以动态改变这个单位即参与的节点。\n切片Split # ​\t切片是一个逻辑概念。Split可以在不改变先有数据存储的情况喜爱动态调整参与计算的节点数量，使用比文件块更小的切片设置-得到更多节点，使用比文件块更大的切片设置-更少节点参与计算。默认情况下，Split大小等于Block大小128M，一个切片对应一个MapTask去计算程序员编写的计算代码。\nMapTask # ​\tMapTask工作流程：\n读取数据切片 -\u0026gt; 读入环形数据缓冲区 -\u0026gt; 分区\u0026amp;排序 -\u0026gt; 组合器（可选） -\u0026gt; 合并 -\u0026gt; 写出ReduceTask ​\tMap默认从所属切片读取数据，每次读取一行到内存；可以根据自己书写的分词逻辑计算每个单词出现的次数；会产生Map临时数据，存放在内存中，内存大小有限多个任务同时执行有可能OOM，把数据直接存放硬盘效率太低需要一个有效方案——内存写入一份，然后写出到硬盘。\n环形数据缓冲区 # ​\t每个Map可以独享一个内存区域，在内存中构建一个环形数据缓冲区（kvBuffer），默认大小为100M，设置缓冲取得阈值为80%，达到阈值后开始外溢写到硬盘，溢写的时候还有20M空间可以被使用。实现了循环利用这块内存区域，减少了数据溢写时map停止的时间，循环写数据到硬盘也不用担心OOM问题。\n1、LineRecordReader读取切片数据，读取的数据格式是KV，K偏移量，V数据；向环形写入KV数据，\r2、每次环形数据缓冲区达到溢写阈值时，只会溢出一个spill次.out，还会生成spill次.out.index的索引文件用于描述数据文件中各分区的起始及长度\r3、每次合并最多合并10个文件，合并使用归并排序\r4、最终输出至ReducdTask时会最终合并一次，合并为file.out数据文件和file.out.index索引文件 ​\t底层是一个数组，逻辑上数组首尾相连实现环形缓冲区，溢写时剩余空间的中间位置重新设置新的迟到，如此反复，实现无卡顿读写，默认使用Hash分区和快速排序。向左写入数据向右写入对应的元数据。\n分区Partation # ​\t有多少个ReduceTask就有多少个分区，一个分区可以有若干组，默认用MapOutputKey的比较规则作为分组条件。根据key直接计算出对应的Reduce，分区数量和Reduce的数量是相等的。hash（key）%partation=num；默认分区的算法是Hash然后取余，Object的hashCode()\u0026mdash;equals()，如果两个对象equals，那么两个对象的hashcode一定相等，hashcode相等，对象不一定equals。根据元数据的信息（分区排序）对数据一些默认Hash分区。\n排序Sort # ​\t对一些的数据进行排序，默认快速排序，按照先Partation后Key的顺序\u0026ndash;\u0026gt;相同分区在一起，相同key在一起，将来一些出的小文件也是有序的。\n溢写Spill # ​\t将内存中数据循环写到硬盘，不用担心OOM的问题，每次写出一个80M文件。环形缓冲区达到溢写阈值时，写出文件spillTIME.out和spillTIME.out.index-数据长度数据位置。索引文件用于描述数据文件中各分区的起始及长度。溢写时会调取用户自定义设置Combiner预聚合。\n合并Merge # ​\t溢写会产生很多有序（分区key）的小文件，而小文件的数目不确定，后面向reduce传递数据带来很多问题，所以将小文件合成一个大文件，将来拉取数据可以直接拉取大文件。合并小文件时进行归并排序最终产生一个有序的大文件 file.out.index和file.out，Merge减少了传输的文件数量，减少了网络IO次数，没有改变数据传输量。合并数据超过3个时会调取用户自定义设置的预聚合Combiner。\n组合器Combiner # 预聚合-组合器-小RM，Combiner 的意义是对每一个 MapTask 的输出进行局部汇总，以减小网络传输量。例如\n原先传给reduce的数据是 a1 a1 a1 a1 a1\r第一次combiner组合之后变为a{1,1,1,1,..}\r第二次combiner后传给reduce的数据变为a{4,2,3,5...} 集群的带宽限制了mapreduce作业的数量，因此应该尽量避免map和reduce任务之间的数据传输。hadoop允许用户对map的输出数据进行处理，用户可自定义combiner函数（如同map函数和reduce函数一般），其逻辑一般和reduce函\r数一样，combiner的输入是map的输出，combiner的输出作为reduce的输入，很多情况下可以直接将reduce函数作为\rconbiner函数来使用（job.setCombinerClass(FlowCountReducer.class);）。\rcombiner属于优化方案，所以无法确定combiner函数会调用多少次，可以在环形缓存区溢出文件时调用combiner函数，也可以在溢出的小文件合并成大文件时调用combiner。但要保证不管调用几次combiner函数都不会影响最终的结\r果，所以不是所有处理逻辑都可以使用combiner组件，有些逻辑如果在使用了combiner函数后会改变最后rerduce的输\r出结果（如求几个数的平均值，就不能先用combiner求一次各个map输出结果的平均值，再求这些平均值的平均值，\r这将导致结果错误）。 拉取Fetch # ​\t当已完成的MapTask任务达到总MapTask任务的5%时，ReduceTask初始化并拉取分区数据。将Map的临时结果拉取到Reduce节点。未排序前拉取数据时必须对Map产生的最终的合并文件做全序遍历，二期每个reduce都要做全序遍历。如果map产生的大文件是有序的， 每一个reduce只需要从文件中读取自己所需的即可。\n拉取规则\n1、相同的Key必须拉取到同一个Reduce节点，同一个Reduce节点可以有多个Key\r2、根据file.out.index索引文件拉取对应的分区数据至ReduceTask内存\r3、ReduceTask内存缓冲区大小默认是当前节点可用内存的70%，超过内存66%开始溢写。\r4、ReduceTask分组规则\ra、查询程序员是否定义分组器，如果有直接用\rb、如果a不满足，查找程序员是否定义了比较器，如果有直接使用\rc、如果b不满足，按MapTask写出Key的比较器进行分组 拉取后\n当已完成的MapTask任务达到总MapTask任务的5%时，ReduceTask初始化并根据file.out.index拉取分区数据，拉取回来的数据在MapTask阶段是有序地，但是从不同的MapTask拉取回来相同分区的数据后，又无序了。对拉取回来的数据进行合并，全局归并排序，然后再分组进入Reduce阶段（用户重写reduce方法）。有多少个ReduceTask就有多少个分区，一个分区可以有若干组，默认情况下使用MapOutputKey的比较器规则为分组条件。 合并Merge # ​\t因为reduce拉取的时候，会从多个map拉取数据，每个map都会产生一个小文件，之间没有序列，为了方便计算，需要合并文件，归并算法将同key的都放在一起。拉取回来的数据在MapTask阶段是有序地，但是从不同的MapTask拉取回来相同分区的数据后，又无序了。对拉取回来的数据进行合并。\n归并Reduce # ​\t将文件中的数据读取到内存中，一次性将相同的key全部读取到内存中，直接将相同的Key得到结果-\u0026gt;最终结果。全局归并排序，然后再分组进入Reduce阶段（用户重写reduce方法）。\n​\tReduceTask分组规则：自定义分组器 -\u0026gt; 自定义比较器 -\u0026gt; MapTask写出的Key的比较器\n写出Output # ​\t每个reduce将自己计算的最终结果都会存放到HDFS上。\nHadoop-YARN架构 # 基本概念 # ​\tYarn（另一种资源协调者），统一管理资源。之后其他的计算机框架可以直接访问yarn获取当前集群的空闲节点。\n​\tclient，客户端发送mr任务到集群，客户端的种类有很多种\n​\tResourceManager，资源协调框架的管理者。分为主节点和备用节点；时刻和NodeManager保持心跳，接收NM的汇报，NM会帮当前节点的资源情况；外部框架要使用资源的时候直接访问RM即可；如果有MR任务，先去ResourceManager申请资源，ResourceManage根据汇报相对灵活分配资源，资源在NodeManager1负责开辟。\n​\tNodeManager,资源协调框架的执行者，每一个DataNode上面默认有一个NodeManager，NM汇报自己的信息到ResourceManager\n​\tContainer，资源的代名词，Container动态分配的。\n​\tApplicationMaster，本次Job任务的主导者，负责调度本次被分配的资源Container，当所有的节点任务全部完成，application告诉ResourceNanager请求杀死当前ApplicationMaster线程，本次任务所有资源都会被释放。\n​\tTask（MapTask\u0026ndash;ReduceTask）开始按照MR的流程执行业务，当任务完成时，ApplicationMaster接收到当前节点的回馈。\n工作流程 # ​\t确认执行MapReduce作业的运行时框架，根据mapreduce.framework.name变量进行配置：\n如果等于yarn：创建YARNRunner对象。等于local：创建LocalJobRunner对象\n1、Client对ResourceMananger（ANN）发起提交作业申请\r2、ResourceManager返回JOBID（即Application ID）和保存数据资源（作业的Jar文件，配置文件，计算所得输入分片，资源信息等）的临时目录（使用JobID命名的目录）\r3、Client计算分片，拷贝资源（作业Jar文件，配置文件，计算所得分配，资源信息等）到HDFS，最后用submitApplicaotin函数提交job给ResourceManager\r4、RM中的ApplicatoinManager（负责调度本次被分配的资源Container）接收提交的job，并将其交给ResourceScheduler（调度者）处理\r5、RS选择一台NodeManager（DN）分配一个Container，在Container中开启ApplicationMaster进程\r6、AM向RM进行注册，这样用户可以直接通过RM查看应用程序的运行状态；然后AM收集计算后的输入分片情况来向RM申请对应的资源运行Task；最后AM初始化一定数量的记录对象（bookkeeping）来跟踪Job的运行进度，并收集每个Task的进度和完成情况，直到运行结束\r1、AM采用轮询的方式，通过RPC协议向RM申请和领取资源\r2、AM申请到资源后，会和对应的NM进行通讯，请求启动Container\r3、NM为任务设置好运行环境（包括环境变量、jar包、二进制程序等），将Task启动命令写到一个脚本中，并通过该脚本启动对应的任务\r4、各个任务通过RPC协议向AM汇报自己的状态和进度，方便AM随时掌握各个任务的运行状态，从而可以在任务失败的时候重启任务\r5、此期间，客户端会每秒轮询检测AM，这样就会随时收到更新信息，这些信息可以通过Web UI来进行查看。除此之外客户端还会每5秒轮询检查Job是否完成，需要调用Job类下的waitForCompletion()方法，Job结束后该方法返回，轮询时间间隔可以通过mapreduce.client.completion.pollinterval进行设置\r7、应用程序运行完成后，AM向RM注销并关闭自己。 Hadoop-YARN环境搭建 # 目标环境 # 节点 NN01 NN02 DN Zookeeper ZKFC JouralNode ResourceManager NodeManager node1 √ √ √ √ √ √ √ node2 √ √ √ √ √ √ node √ √ √ √ √ 修改配置文件 # 修改环境配置文件 hadoop-env.sh ：\n[root@node01 ~]# cd /opt/yjx/hadoop-3.3.4/etc/hadoop/\r[root@node01 hadoop]# vim hadoop-env.sh\r文件末尾加上\rexport JAVA_HOME=/usr/java/jdk1.8.0_351-amd64\rexport HDFS_NAMENODE_USER=root\rexport HDFS_DATANODE_USER=root\rexport HDFS_ZKFC_USER=root\rexport HDFS_JOURNALNODE_USER=root\rexport YARN_RESOURCEMANAGER_USER=root\rexport YARN_NODEMANAGER_USER=root 修改 Map 配置文件 mapred-site.xml ：\n在 configuration 节点中添加以下内容：\r\u0026lt;!-- 设置执行 MapReduce 任务的运行时框架为 YARN --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 MapReduce JobHistory 服务器的地址 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.jobhistory.address\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01:10020\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 MapReduce JobHistory 服务器的 Web 地址 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.jobhistory.webapp.address\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01:19888\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置已经运行完的 Hadoop 作业记录的存放路径（HDFS 文件系统中的目录），默认是\r${yarn.app.mapreduce.am.staging-dir}/history/done --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.jobhistory.done-dir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/history/done\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置正在运行中的 Hadoop 作业记录的存放路径（HDFS 文件系统中的目录），默认是\r${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.jobhistory.intermediate-done-dir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/history/done_intermediate\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置需要加载的 jar 包和环境配置 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.application.classpath\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;\r/opt/yjx/hadoop-3.3.4/etc/hadoop,\r/opt/yjx/hadoop-3.3.4/share/hadoop/common/*,\r/opt/yjx/hadoop-3.3.4/share/hadoop/common/lib/*,\r/opt/yjx/hadoop-3.3.4/share/hadoop/hdfs/*,\r/opt/yjx/hadoop-3.3.4/share/hadoop/hdfs/lib/*,\r/opt/yjx/hadoop-3.3.4/share/hadoop/mapreduce/*,\r/opt/yjx/hadoop-3.3.4/share/hadoop/yarn/*,\r/opt/yjx/hadoop-3.3.4/share/hadoop/yarn/lib/*\r\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt; 修改 YARN 配置文件 yarn-site.xml ：\n[root@node01 hadoop]# vim yarn-site.xml\r在 configuration 节点中添加以下内容：\r\u0026lt;!-- 提交 MapReduce 作业的 staging 目录（HDFS 文件系统中的目录），默认是 /tmp/hadoop-yarn/staging --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.app.mapreduce.am.staging-dir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/tmp/hadoop-yarn/staging\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置开启 ResourceManager 高可用 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.ha.enabled\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 ResourceManager 的集群 ID --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.cluster-id\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;yarn-yjx\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 ResourceManager 节点的名字 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.ha.rm-ids\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;rm1,rm2\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 ResourceManager 服务器的地址 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.hostname.rm1\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 ResourceManager 服务器的地址 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.hostname.rm2\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node03\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 ResourceManager 服务器的 Web 地址 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.webapp.address.rm1\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01:8088\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 ResourceManager 服务器的 Web 地址 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.webapp.address.rm2\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node03:8088\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 YARN 的 ZK 集群地址，以逗号分隔 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.zk-address\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01:2181,node02:2181,node03:2181\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 定义用户自定义服务或者系统服务，以逗号分隔，服务名称只能包含 A-za-z0-9，不能以数字开头，例如：\rmapreduce_shuffle,spark_shuffle --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- MapReduce 是在各个机器上运行的，在运行过程中产生的日志存在于不同的机器上，为了能够统一查看各个机器的运行\r日志，将日志集中存放在 HDFS 上，这个过程就是日志聚合 --\u0026gt;\r\u0026lt;!-- 设置开启日志聚合，日志聚合会收集每个容器的日志，并在应用程序完成后将这些日志移动到文件系统，例如 HDFS --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.log-aggregation-enable\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置聚合日志的保留时间 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.log-aggregation.retain-seconds\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;640800\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置是否启用自动恢复，如果为 true 则必须指定 yarn.resourcemanager.store.class --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.recovery.enabled\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 ResourceManager 的状态信息存储在 ZooKeeper 集群 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.resourcemanager.store.class\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置是否对容器强制执行物理内存限制 --\u0026gt;\r\u0026lt;!-- 是否启动一个线程检查每个任务正在使用的物理内存量，如果任务超出分配值，则将其直接杀掉，默认为 true --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.nodemanager.pmem-check-enabled\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置是否对容器强制执行虚拟内存限制 --\u0026gt;\r\u0026lt;!-- 是否启动一个线程检查每个任务正在使用的虚拟内存量，如果任务超出分配值，则将其直接杀掉，默认为 true --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.nodemanager.vmem-check-enabled\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置容器的虚拟内存限制，虚拟内存与物理内存之间的比率。作用：在物理内存不够用的情况下，如果占用了大量虚拟内\r存并且超过了一定阈值，那么就认为当前集群的性能比较差 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.nodemanager.vmem-pmem-ratio\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;2.1\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 配置 JobHistory --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;yarn.log.server.url\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;http://node01:19888/jobhistory/logs\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r提示：如果 yarn.nodemanager.aux-services 选项配置为 spark_shuffle ，需要拷贝\r$SPARK_HOME/yarn/spark-x.y.z-yarn-shuffle.jar 到 $HADOOP_HOME/share/hadoop/yarn/lib 目录。 拷贝至其他节点 # 将 node01 已配置好的 YARN 拷贝至 node02 和 node03。\n[root@node01 hadoop]# pwd\r/opt/yjx/hadoop-3.3.4/etc/hadoop\r[root@node01 hadoop]# scp mapred-site.xml yarn-site.xml root@node02:`pwd`\r[root@node01 hadoop]# scp mapred-site.xml yarn-site.xml root@node03:`pwd`\r# 或者使用分发脚本\r[root@node01 hadoop]# yjxrsync mapred-site.xml yarn-site.xml 启动 # 首先启动 ZooKeeper（三台机器都需要执行）。\nzkServer.sh start\rzkServer.sh status\r启动 HDFS。\r[root@node01 hadoop]# start-dfs.sh\r启动 YARN。\r[root@node01 hadoop]# start-yarn.sh\r启动 JobHistory。\r[root@node01 hadoop]# mapred --daemon start historyserver\r后期只需要先启动 ZooKeeper 然后启动 Hadoop（start-all.sh）再启动 JobHistory 即可 访问 # 访问：http://node01:9870/ 和 http://node02:9870/ 结果如下。\n访问：http://node01:8088 或者 http://node03:8088，会被自动转发到 ResourceManager 的主节点。\n访问：http://node01:19888/jobhistory 结果如下。\n关闭 # 先关闭 Hadoop 和 JobHistory。\n[root@node01 hadoop]# mapred --daemon stop historyserver\r[root@node01 hadoop]# stop-all.sh 再关闭 ZooKeeper（三台机器都需要执行）。\nzkServer.sh stop 环境搭建成功后 shutdown -h now 关机拍摄快照。\nMapReduce案例[WordCount] # Java代码实现 # 继续在学习 HDFS 时创建的 hadoop-demo 项目基础上进行编写。将新搭建的 YARN 环境的配置文件重新拷贝一份至项目。主要拷贝以下配置文件： core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml log4j.properties\n然后修改 hadoop-demo 项目的 pom.xml 文件，在 节点中添加以下内容（打包插件）：\n\u0026lt;build\u0026gt;\r\u0026lt;plugins\u0026gt;\r\u0026lt;plugin\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;3.4.2\u0026lt;/version\u0026gt;\r\u0026lt;configuration\u0026gt;\r\u0026lt;!-- 生成的 jar 包名称中是否追加 appendAssemblyId --\u0026gt;\r\u0026lt;appendAssemblyId\u0026gt;false\u0026lt;/appendAssemblyId\u0026gt;\r\u0026lt;descriptorRefs\u0026gt;\r\u0026lt;!-- 将项目依赖的 jar 包打包到当前 jar 包 --\u0026gt;\r\u0026lt;descriptorRef\u0026gt;jar-with-dependencies\u0026lt;/descriptorRef\u0026gt;\r\u0026lt;/descriptorRefs\u0026gt;\r\u0026lt;archive\u0026gt;\r\u0026lt;manifest\u0026gt;\r\u0026lt;!-- 打成可执行的 jar 包的主方法入口类 --\u0026gt;\r\u0026lt;mainClass\u0026gt;xxx.xxx.xxx.xxx.XxxXxx\u0026lt;/mainClass\u0026gt;\r\u0026lt;/manifest\u0026gt;\r\u0026lt;/archive\u0026gt;\r\u0026lt;/configuration\u0026gt;\r\u0026lt;!-- 插件目标列表 --\u0026gt;\r\u0026lt;executions\u0026gt;\r\u0026lt;!-- 将插件目标与生命周期阶段绑定 --\u0026gt;\r\u0026lt;execution\u0026gt;\r\u0026lt;!-- 插件目标 --\u0026gt;\r\u0026lt;goals\u0026gt;\r\u0026lt;!-- 只运行一次 --\u0026gt;\r\u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt;\r\u0026lt;/goals\u0026gt;\r\u0026lt;!-- 生命周期阶段 --\u0026gt;\r\u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt;\r\u0026lt;/execution\u0026gt;\r\u0026lt;/executions\u0026gt;\r\u0026lt;/plugin\u0026gt;\r\u0026lt;/plugins\u0026gt;\r\u0026lt;/build\u0026gt; Job代码\nimport org.apache.hadoop.conf.Configuration;\rimport org.apache.hadoop.fs.Path;\rimport org.apache.hadoop.io.IntWritable;\rimport org.apache.hadoop.io.Text;\rimport org.apache.hadoop.mapreduce.Job;\rimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\rimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\rimport java.io.IOException;\rpublic class WordCountJob {\rpublic static void main(String[] args) throws IOException,\rInterruptedException, ClassNotFoundException {\r// 加载配置文件\rConfiguration configuration = new Configuration(true);\r// 本地模式运行\rconfiguration.set(\u0026#34;mapreduce.framework.name\u0026#34;, \u0026#34;local\u0026#34;);\r// 创建作业\rJob job = Job.getInstance(configuration);\r// 设置作业主类\rjob.setJarByClass(WordCountJob.class);\r// 设置作业名称\rjob.setJobName(\u0026#34;yjx-wordcount-\u0026#34; + System.currentTimeMillis());\r// 设置 Reduce 的数量\rjob.setNumReduceTasks(2);\r// 设置数据的输入路径（需要计算的数据从哪里读）\rFileInputFormat.setInputPaths(job, new Path(\u0026#34;/yjx/harry potter.txt\u0026#34;));\r// 设置数据的输出路径（计算后的数据输出到哪里）\rFileOutputFormat.setOutputPath(job, new Path(\u0026#34;/yjx/result/\u0026#34; + job.getJobName()));\r// 设置 Map 的输出的 Key 和 Value 的类型\rjob.setMapOutputKeyClass(Text.class);\rjob.setMapOutputValueClass(IntWritable.class);\r// 设置 Map 和 Reduce 的处理类\rjob.setMapperClass(WordCountMapper.class);\rjob.setReducerClass(WordCountReducer.class);\r// 将作业提交到集群并等待完成\rjob.waitForCompletion(true);\r}\r} Job的Mapper代码\nimport org.apache.hadoop.io.IntWritable;\rimport org.apache.hadoop.io.Text;\rimport org.apache.hadoop.mapreduce.Reducer;\rimport java.io.IOException;\rpublic class WordCountReducer extends Reducer\u0026lt;Text, IntWritable, Text, IntWritable\u0026gt; {\r/**\r* @param key Map 的输出的 Key\r* @param values Map 的输出的 Value\r* @param context\r* @throws IOException\r* @throws InterruptedException\r*/\r@Override\rprotected void reduce(Text key, Iterable\u0026lt;IntWritable\u0026gt; values, Context context)\rthrows IOException, InterruptedException {\r// 声明计数器\rint count = 0;\r// 循环处理\rfor (IntWritable value : values) {\rcount += value.get();\r}\r// 写出数据\rcontext.write(key, new IntWritable(count));\r}\r} Job提交方式 # ​\tLinux 端执行方式 ​\thadoop jar wordcount.jar com.yjxxt.mapred.wordcount.WordCountJob ​\tWindows 端本地化执行 ​\t拷贝 Hadoop 配置文件 ​\tconfiguration.set(\u0026ldquo;mapreduce.framework.name\u0026rdquo;, \u0026ldquo;local\u0026rdquo;);\n​\t直接从本地 IDEA 将程序提交到 YARN 平台\n// 加载配置文件\rConfiguration configuration = new Configuration(true);\r// HDFS 文件系统主机地址\rconf.set(\u0026#34;fs.defaultFS\u0026#34;, \u0026#34;hdfs://node01:8020\u0026#34;);\r// MapReduce 作业的运行时框架，Local 本地模式（学习环境） YARN 模式（正式环境）\rconf.set(\u0026#34;mapreduce.framework.name\u0026#34;, \u0026#34;yarn\u0026#34;);\r// 设置 ResourceManager 的主机地址，默认为 0.0.0.0\rconf.set(\u0026#34;yarn.resourcemanager.hostname\u0026#34;, \u0026#34;node01\u0026#34;);\r// 允许跨平台提交（因为 Windows 和 Linux 系统的结构不一样，默认使用 Linux 系统的提交方式）\r// 不开启跨平台提交，在 Windows 提交应用会报 /bin/bash: line 0: fg: no job control 错误\rconf.set(\u0026#34;mapreduce.app-submission.cross-platform\u0026#34;, \u0026#34;true\u0026#34;);\r// 创建作业\rJob job = Job.getInstance(conf);\r// 要提交的应用程序的 Jar 包位置\rjob.setJar(\u0026#34;D:\\\\Projects\\\\IdeaProjects\\\\yjxxt\\\\hadoop-demo\\\\target\\\\hadoop-demo-1.0-SNAPSHOT.jar\u0026#34;);\r注意：如果在 resources 目录下添加了 Hadoop 集群的 core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml，那么\r可以注释以上除 Jar 包之外的配置代码。这里的配置文件要和 Hadoop 集群中的配置文件一致，否则会出错，最好的\r做法是从集群中直接 Copy 出来。 YARN 常用命令 # yarn node -list -all ：列出所有节点；\ryarn application -list ：列出所有 Application；\ryarn application -list -appStates ：根据 Application 的状态过滤（所有状态：ALL、NEW、NEW_SAVING、\rSUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）；\ryarn logs -applicationId application_1667293209556_0001 ：查看 Application 的日志；\ryarn logs -applicationId application_1667293209556_0001 -containerId\rcontainer_e01_1667293209556_0001_01_000002 ：查询 Container 的日志；\ryarn application -kill application_1667293209556_0001 ：根据 ApplicationID 杀死应用；\ryarn container -list appattempt_1667293209556_0001_000001 ：列出 Application 的所有 Container；\ryarn container -status container_e01_1667293209556_0001_01_000002 ：打印 Container 的状态；只有在任务运行的过程中才能看到 Container 的状态。 MapReduce案例[充值记录] # 在Mapper通过加盐操作，让数据的Key达到相对平衡，解决数据倾斜的问题。\nMapReduce案例[天气信息] # 解决方案一（用户自定义算法解决）： Mapper 写出 Key：省市年月 Value：温度 Reducer 根据 省市年月 分组拿到所有 Value，计算出前三\r解决方案二（合理利用 Hadoop 框架解决）： 自定义 Weather 对象实现 WritableComparable 接口，重写比较规则，底层快排就会按照自定义的规则去进行比较，完成排序 Mapper 写出 Key：Weather 对象 Value：温度 Reducer 根据 省市年月 分组拿到所有 Value，这个 Value 是已经排好序的，直接取走前三即可（需要处理重复数据） MapReduce案例[好友推荐] # MapReducer压缩 # 概述 # ​\t压缩技术能够有效减少存储系统的读写字节数，提高网络带宽和磁盘空间的效率。注意：压缩特性运用得当提高性能，运用不当也可能降低性能。\n在Hadoop中，当数据规模很大，工作负载非常密集时，I/O 操作和网络数据传输需要花费大量的时间，Shuffle 与Merge过程同样也面临着巨大的 I/O 压力。在这种情况下，数据压缩的重要性不言而喻。而在 Hive 中则体现在数据文件最终存储的格式是否启用压缩。\r鉴于磁盘 I/O 和网络带宽是 Hadoop 的宝贵资源，数据压缩对于节省资源、最小化磁盘 I/O 和网络传输非常有帮助，但其性能的提升和资源的节省并非没有代价（增加了 CPU 的运算负担）。如果磁盘 I/O 和网络带宽影响了 MapReduce作业性能，在任意 MapReduce 阶段启用压缩都可以改善端到端处理时间并减少 I/O 和网络流量。 条件与优缺点 # ​\t优点：减少存储系统读写字节数、提高网络带宽和磁盘空间的效率\n​\t缺点：使用数据时需要先解压，加重CPU负载，压缩算法越复杂，解压时间越长\n​\t条件：空间和CPU要充裕，CPU紧张慎用压缩\n​\t技术：有损压缩（LOSSY COMPRESSION）压缩和解压过程中有数据丢失，一般用于视频\n​\t无损压缩（LOSSLESS COMPRESSION）压缩和解压过程没有数据地市，日志数据\n​\t对称和非对称：对称压缩和解压时间一致、非对称时间不一致\n基本原则 # 计算密集型（CPU-Intensive）作业少用压缩\r特点：要进行大量的计算，消耗 CPU 资源。比如计算圆周率、对视频进行高清解码等等，全靠 CPU 的运算能力。\r计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU 执行任务的效率就越\r低，所以，要最高效地利用 CPU，计算密集型任务同时进行的数量应当等于 CPU 的核心数。计算密集型任务由于主要消耗 CPU 资源，因此，代码运行效率至关重要。Python 这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用 C 语言编写。\rIO密集型（IO-Intensive）作业，多用压缩\r特点：CPU 消耗很少，任务的大部分时间都在等待 IO 操作完成（因为 IO 的速度远远低于 CPU 和内存的速度）。\r涉及到网络、磁盘 IO 的任务都是 IO 密集型任务。对于 IO 密集型任务，任务越多，CPU 效率越高，但也有一个限度。常见的大部分任务都是 IO 密集型任务，比如 Web 应用。IO 密集型任务执行期间，99% 的时间都花在 IO 上，花在 CPU 上的时间很少，因此，用运行速度极快的 C 语言替换Python 这样运行速度极低的脚本语言，完全无法提升运行效率。对于 IO 密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C 语言最差。 压缩实践 # 压缩支持 # ​\t使用 hadoop checknative 命令，可以查看是否有相应压缩算法的库，如果显示为 false，则需要额外安装。\n注意：Hadoop 2.X 版本已经集成了 Snappy、LZ4、BZip2 等压缩算法的编/解码器，会自动调用对应的本地库，而CentOS7 中又自带了 Snappy 的依赖库，所以无需安装 Snappy 依赖。\n[root@node01 ~]# hadoop checknative\r2023-02-20 19:55:59,761 INFO bzip2.Bzip2Factory: Successfully loaded \u0026amp; initialized native-bzip2\rlibrary system-native\r2023-02-20 19:55:59,764 INFO zlib.ZlibFactory: Successfully loaded \u0026amp; initialized native-zlib library\r2023-02-20 19:55:59,771 WARN zstd.ZStandardCompressor: Error loading zstandard native libraries:\rjava.lang.InternalError: Cannot load libzstd.so.1 (libzstd.so.1: cannot open shared object file: No\rsuch file or directory)!\r2023-02-20 19:55:59,773 WARN erasurecode.ErasureCodeNative: Loading ISA-L failed: Failed to load\rlibisal.so.2 (libisal.so.2: cannot open shared object file: No such file or directory)\r2023-02-20 19:55:59,773 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your\rplatform... using builtin-java codec where applicable\r2023-02-20 19:55:59,829 INFO nativeio.NativeIO: The native code was built without PMDK support.\rNative library checking:\rhadoop: true /opt/yjx/hadoop-3.3.4/lib/native/libhadoop.so.1.0.0\rzlib: true /lib64/libz.so.1\rzstd : false\rbzip2: true /lib64/libbz2.so.1\ropenssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or\rdirectory)!\rISA-L: false Loading ISA-L failed: Failed to load libisal.so.2 (libisal.so.2: cannot open shared\robject file: No such file or directory)\rPMDK: false The native code was built without PMDK support. 压缩比较 # 压缩格式 算法 文件扩展名 是否可切分 压缩比 压缩速度 解压速度 DEFLATE DEFLATE .deflate 否 高 低 低 Gzip DEFLATE .gz 否 高 低 低 BZip2 BZip2 .bz2 是 高 低 低 LZO LZO .lzo 是（索引） 低 高 高 LZ4 LZ4 .lz4 否 低 高 高 Snappy Snappy .snappy 否 低 高 高 Zstd Zstd .zst 否 高 高 高 1、文件扩展名：压缩后问数据文件的后缀名\r2、是否可切分：表示压缩后的数据文件在被 MapReduce 读取的时候，是否会产生多个 InputSplit。 如果这个压缩格式产生的文件不可切分，那也就意味着，无论这个压缩文件有多大，在 MapReduce 中都只会产生 1 个 Map 任务。如果压缩后的文件不大，也就 100M 左右，这样对性能没有多大影响。但是如果压缩后的文件比较大，达到了 1 个 G，由于不可切分，这样只能使用 1 个 Map 任务去计算，性能就比较差了，这个时候就没有办法达到并行计算的效果了。所以是否可切分这个特性是非常重要的，特别是当我们无法控制单个压缩文件大小的时候。\r3、压缩比：表示压缩格式的压缩效果，压缩比越高，说明压缩效果越好，对应产生的压缩文件就越小。 如果集群的存储\r空间有限，则需要重点关注压缩比，这个时候需要选择尽可能高的压缩比。\r4、压缩速度：表示将原始文件压缩为指定压缩格式消耗的时间。 压缩功能消耗的时间会体现在任务最终消耗的时间里\r面，所以这个指标也需要重点考虑。\r5、解压速度：表示将指定压缩格式的数据文件解压为原始文件消耗的时间。 因为 MapReduce 在使用压缩文件的时候需要先进行解压才能使用，解压消耗的时间也会体现在任务最终消耗的时间里面，所以这个指标也需要重点考虑。\r存放数据到HDFS时，可以通过配置指定数据的压缩方式。当MapReduce程序读取数据时，会根据拓展名自动解压。\rLZO，LZ4，Snappy 等压缩算法专注于压缩和解压缩性能，Zstd 在性能不错的同时号称压缩率跟Deflate（Zip/Gzip 的算法）相当。Linux 内核、HTTP 协议、以及一系列的大数据工具（包括 Hadoop 3.0.0，HBase 2.0.0，Spark 2.3.0，Kafka 2.1.0）等都已经加入了对 Zstd 的支持。 压缩选择 # 1、能压缩数据的地方只有两个：MapTask 写出和ReduceTask的写出\n2、MapTask写出的数据不需要考虑是否可切分，只需要考虑性能，推荐Snappy\n3、ReduceTask写出的数据需要考虑是否是下一个作业的数据入口\n​\ta.如果是，不推荐使用不可切分的压缩算法\n​\tb.如果计算后直接归档落盘，且后续计算可能性小，使用高比例压缩算法，节省存储空间。\nMR 主要在三个地方会用到数据压缩：Input ：数据来源；Transformation ：中间计算；Output ：最后的输出\rUse Compressd Map Input ：第一次传入压缩文件，应选用可以切片的压缩方式，否则整个文件将只有一个 Map执行。建议：从 HDFS 中读取文件进行 MapReuce 作业，如果数据很大，可以使用压缩并且选择支持分片的压缩方式，例如 BZip2、LZO，这样可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式例如 Sequence Files、RC、ORC 等。\rCompress Intermediate Data ：第二次压缩应选择压缩解压速度快的压缩方式。建议：Map 的输出作为 Reducer的输入，需要经过 Shuffle 这一过程，需要把数据读取到环形数据缓冲区，然后再读取到本地磁盘，所以选择压缩可以减少了存储文件所占空间，提升了数据传输速率，建议使用压缩/解压速度快的压缩方式，例如 Snappy、LZO、LZ4、Zstd。\rCompress ReducerOutput ：第三次压缩有两种场景分别是：当输出的文件为下一个 Job 的输入时，建议：选择可切分的压缩方式例如：BZip2。当输出的文件直接存到 HDFS 作为归档时，建议：选择压缩比高的压缩方式。Reduce 阶段数据落盘通常使用 Gzip 或BZip2 进行压缩（减少磁盘使用）。\r总结：\rGzip：Hadoop 内置支持，压缩比高，不支持 Split。\r用途：通常用来放不常访问的冷数据，较高的压缩比可以极大的节省磁盘空间。\r对应的编码/解码器： org.apache.hadoop.io.compress.GzipCodec 。\rBZip2：Hadoop 内置支持，压缩比高，支持 Split，支持多文件，缺点就是慢。\r用途：适用于对处理速度要求不高的场景。一般不常用。\r对应的编码/解码器： org.apache.hadoop.io.compress.BZip2Codec 。\rLZO： 压缩比一般，支持 Split（需要建索引，文件修改后需要重新建索引），压缩/解压速度快，支持 Hadoop Native库，需要自己安装。\r用途：适合于经常访问的热数据。\r对应的编码/解码器： com.hadoop.compression.lzo.LzopCodec 。\rLZ4：压缩比一般，不支持 Split，压缩/解压速度快，支持 Hadoop Native 库，需要自己安装。\r用途：和 LZO 性能类似，但不支持 Split，可以用于 Map 中间结果的压缩。\r对应的编码/解码器： org.apache.hadoop.io.compress.Lz4Codec 。\rSnappy：压缩比一般，不支持 Split，压缩/解压速度快，支持 Hadoop Native 库，需要自己安装。\r用途：和 LZO 性能类似，但不支持 Split，可以用于 Map 中间结果的压缩。\r对应的编码/解码器： org.apache.hadoop.io.compress.SnappyCodec 。\rZstd：压缩比高跟 Deflate（Gzip 算法）相当，不支持 Split，压缩/解压速度快，支持 Hadoop Native 库，需要自己安装。\r用途：和 LZO 性能类似，但不支持 Split，可以用于 Map 中间结果的压缩。\r对应的编码/解码器： org.apache.hadoop.io.compress.ZStandardCodec 。 压缩配置 # 要在 Hadoop 中启用压缩，需要配置以下参数。配置文件配置完所有文件都压缩；windons可以压缩，但是需要自己配置环境 core-site.xml\n\u0026lt;!-- 可用于压缩/解压缩的编解码器，用逗号分隔列表 --\u0026gt;\r\u0026lt;propery\u0026gt;\r\u0026lt;name\u0026gt;io.compression.codecs\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;\rorg.apache.hadoop.io.compress.DefaultCodec,\rorg.apache.hadoop.io.compress.GzipCodec,\rorg.apache.hadoop.io.compress.BZip2Codec,\rcom.hadoop.compression.lzo.LzopCodec,\rorg.apache.hadoop.io.compress.Lz4Codec,\rorg.apache.hadoop.io.compress.SnappyCodec,\rorg.apache.hadoop.io.compress.ZStandardCodec\r\u0026lt;/value\u0026gt;\r\u0026lt;/propery\u0026gt; mapred-site.xml\n\u0026lt;!-- 开启 Mapper 输出压缩 --\u0026gt;\r\u0026lt;propery\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.map.output.compress\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/propery\u0026gt;\r\u0026lt;!-- 设置 Mapper 输出压缩的压缩方式 --\u0026gt;\r\u0026lt;propery\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.map.output.compress.codec\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;org.apache.hadoop.io.compress.SnappyCodec\u0026lt;/value\u0026gt;\r\u0026lt;/propery\u0026gt;\r\u0026lt;!-- 开启 Reducer 输出压缩 --\u0026gt;\r\u0026lt;propery\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.output.fileoutputformat.compress\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/propery\u0026gt;\r\u0026lt;!-- 设置 Reducer 输出压缩的压缩方式 --\u0026gt;\r\u0026lt;propery\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.output.fileoutputformat.compress.codec\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;org.apache.hadoop.io.compress.BZip2Codec\u0026lt;/value\u0026gt;\r\u0026lt;/propery\u0026gt;\r\u0026lt;!-- SequenceFiles 输出可以使用的压缩类型：NONE、RECORD 或者 BLOCK --\u0026gt;\r\u0026lt;!-- 如果作业输出被压缩为 SequenceFiles，该属性用来控制使用的压缩格式。默认为 RECORD，即针对每条记录进行压\r缩，如果将其改为 BLOCK，将针对一组记录进行压缩，这是推荐的压缩策略，因为它的压缩效率更高。 --\u0026gt;\r\u0026lt;propery\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.output.fileoutputformat.compress.type\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;BLOCK\u0026lt;/value\u0026gt;\r\u0026lt;/propery\u0026gt; 除了使用配置文件的方式指定压缩器（优先考虑）外，还可以使用编码的方式进行配置。\n// 加载配置文件\rConfiguration configuration = new Configuration(true);\r// 开启 Mapper 输出压缩\rconfiguration.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);\rconfiguration.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, SnappyCodec.class, CompressionCodec.class);\r// 创建作业\rJob job = Job.getInstance(configuration);\r// ... Job 的其他设置\r// 开启 Reducer 输出压缩\rFileOutputFormat.setCompressOutput(job, true);\rFileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);\r// 将作业提交到集群并等待完成\rjob.waitForCompletion(true); 压缩实践 # 将 Job 打包并上传至 Hadoop 服务器。 运行命令： hadoop jar xxxxx.jar com.yjxxt.mapred.weather.compress.WeatherCompressJob\nMap 到 Reduce 阶段的压缩可以通过日志查看：\n修改之前 MR 的天气信息 Job 代码如下（Mapper 和 Reducer 的代码不动）：\nimport com.yjxxt.wordcount.WordCountJob;\rimport org.apache.hadoop.conf.Configuration;\rimport org.apache.hadoop.fs.Path;\rimport org.apache.hadoop.io.IntWritable;\rimport org.apache.hadoop.io.Text;\rimport org.apache.hadoop.io.compress.BZip2Codec;\rimport org.apache.hadoop.io.compress.CompressionCodec;\rimport org.apache.hadoop.io.compress.SnappyCodec;\rimport org.apache.hadoop.mapreduce.Job;\rimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\rimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\rimport java.io.IOException;\rpublic class WeatherCompressJob {\rpublic static void main(String[] args) {\rtry {\r// 加载配置文件\rConfiguration configuration = new Configuration(true);\r// 本地模式运行\r// configuration.set(\u0026#34;mapreduce.framework.name\u0026#34;, \u0026#34;local\u0026#34;);\r// 开启 Mapper 输出压缩\rconfiguration.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);\rconfiguration.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, SnappyCodec.class,\rCompressionCodec.class);\r// 创建作业\rJob job = Job.getInstance(configuration);\r// 设置作业主类\rjob.setJarByClass(WeatherCompressJob.class);\r// 设置作业名称\rjob.setJobName(\u0026#34;yjx-weather-compress-\u0026#34; + System.currentTimeMillis());\r// 设置 Reduce 的数量\rjob.setNumReduceTasks(2);\r// 设置数据的输入路径（需要计算的数据从哪里读）\rFileInputFormat.setInputPaths(job, new Path(\u0026#34;/yjx/weather.csv\u0026#34;));\r// 设置数据的输出路径（计算后的数据输出到哪里）\rFileOutputFormat.setOutputPath(job, new Path(\u0026#34;/yjx/result/\u0026#34; + job.getJobName()));\r// 设置 Map 的输出的 Key 和 Value 的类型\rjob.setMapOutputKeyClass(Text.class);\rjob.setMapOutputValueClass(IntWritable.class);\r// 设置 Map 和 Reduce 的处理类\rjob.setMapperClass(WeatherMapper.class);\rjob.setReducerClass(WeatherReducer.class);\r// 开启 Reducer 输出压缩\rFileOutputFormat.setCompressOutput(job, true);\rFileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);\r// 将作业提交到集群并等待完成\rjob.waitForCompletion(true);\r} catch (IOException | InterruptedException | ClassNotFoundException e) {\re.printStackTrace();\r}\r}\r} MapReduce优化 # 概述 # 优化前我们需要知道 Hadoop 适合干什么活，适合什么场景，在工作中，我们要知道业务是怎样的，才能结合平台资\r源达到最优优化。除了这些我们还要知道 MapReduce 的执行流程，比如从文件的读取，Map 处理，Shuffle 过程，Reduce\r处理，文件的输出或者存储压缩等等。\r在工作中，往往平台的参数都是固定的，不可能为了某一个作业去修改整个平台的参数，所以在作业的执行过程中，\r需要对作业进行单独的设定，这样既不会对其他作业产生影响，也能很好的提高作业的性能，提高优化的灵活性。\r接下来，回顾一下 Hadoop 的优势（适用场景）：\r可构建在廉价机器上，设备成本相对较低\r高容错性，HDFS将数据自动保存为多个副本，副本丢失后，自动恢复，防止数据丢失或损坏\r适合批处理，HDFS适合一次写入多次查询的情况，适合在已有数据的情况下进行多次分析，稳定性好\r适合存储大文件，其中的大可以表示存储单个大文件，因为是分块存储的。也可以表示存储大量的数据，但是不适合小文件 小文件优化 # 从概述中我们知道，很明显 Hadoop 适合大文件的处理和存储，那为什么不适合小文件呢？\r从存储方面来说：Hadoop 存储的每个文件都会在 NameNode 上记录元数据，如果同样大小的文件，文件很小的话，\r就会产生很多元数据文件，造成 NameNode 的压力；\r从读取方面来说：同样大小的文件分为很多小文件的话，会增加磁盘寻址次数，降低性能；\r从计算方面来说：我们知道一个 MapTask 默认处理一个分片或者一个文件，如果 MapTask 的启动时间比数据处理的时\r间还要长，那么就会造成低性能。而且在 Map 端溢写磁盘的时候每一个 MapTask 最终会产生 Reduce 数量个数的中间结果，如果 MapTask 数量特别多，就会造成临时文件很多，造成 Reduce 拉取数据的时候增加磁盘的 IO。\r明白小文件造成的弊端之后，那我们应该怎么处理这些小文件呢？\r从源头解决问题，也就是在 HDFS 上不要存储小文件，在数据上传至 HDFS 的时候提前合并小文件；\r如果小文件合并后的文件过大，可以更换文件存储格式或压缩存储，当然压缩存储需要考虑是否能切片的问题；\r如果小文件已经存储至 HDFS 了，那么在 FileInputFormat 读取数据的时候使用CombineFileInputFormat类读取数据，在读取数据的时候进行合并。 数据倾斜 # 数据倾斜，每个 Reduce 处理的数据量大小不一致，导致有些已经跑完了，有些还在执行；\r还有可能就是某些作业所在的 NodeManager 有问题或者 Container 有问题或者 JVM GC 等，导致作业执行缓慢。\r那么为什么会产生数据倾斜呢？比如数据本身就不平衡，所以在默认的 HashPartition 时造成分区数据不一致问题，还有就是代码设计不合理等。\r那如何解决数据倾斜的问题呢？\r不使用默认的 Hash 分区算法，采用自定义分区，结合业务特点，使得每个分区数据基本平衡；\r或者既然有默认的分区算法，那么我们可以修改分区的键，让其符合 Hash 分区，并且使得最后的分区平衡，比如在\rKey 前加随机数或盐 n-key；\r既然 Reduce 处理慢，那么可以增加 Reduce 的 memory 和 vcore，提高性能解决问题，虽然没从根本上解决问题，但是还有效果的；\r如果是因为只有一个 Reduce 导致作业很慢，可以增加 Reduce 的数量来分摊压力，然后再来一个作业实现最终聚合。 推测执行 # 如果不是数据倾斜带来的问题，而是节点服务有问题造成某些 Map 和 Reduce 执行缓慢呢？\r可以使用推测执行，你跑的慢，我们可以找个其他节点重启一样的任务进行竞争，谁快以谁为准。推测执行是空间换时间的一种优化思想，会带来集群资源的浪费，给集群增加压力，所以一般情况下集群的推测执行都是关闭的，可以根据实际情况选择是否开启\r推测执行相关参数如下：\r# 是否启用 MapTask 推测执行，默认为 true\rmapreduce.map.speculative=true\r# 是否启用 ReduceTask 推测执行，默认为 true\rmapreduce.reduce.speculative=true\r# 推测任务占当前正在运行的任务数的比例，默认为 0.1\rmapreduce.job.speculative.speculative-cap-running-tasks=0.1;\r# 推测任务占全部要处理任务数的比例，默认为 0.01\rmapreduce.job.speculative.speculative-cap-total-tasks=0.01\r# 最少允许同时运行的推测任务数量，默认为 10\rmapreduce.job.speculative.minimum-allowed-tasks=10;\r# 本次推测没有任务下发，执行下一次推测任务的等待时间，默认为 1000（ms）\rmapreduce.job.speculative.retry-after-no-speculate=1000;\r# 本次推测有任务下发，执行下一次推测任务的等待时间，默认为 15000（ms）\rmapreduce.job.speculative.retry-after-speculate=15000;\r# 标准差，任务的平均进展率必须低于所有正在运行任务的平均值才会被认为是太慢的任务，默认为 1.0\rmapreduce.job.speculative.slowtaskthreshold=1.0; MapReduce执行流程优化 # Map # 临时文件 # 上面我们从 Hadoop 的某些特定场景下聊了 MapReduce 的优化，接下来我们从 MapReduce 的执行流程进行优化。\r前面我们已经聊过小文件在数据读取这里也可以做优化，所以选择一个合适的数据文件的读取类（FIleInputFormat 的实现类）也很重要。我们在作业提交的过程中，会把作业 Jar 文件，配置文件，计算所得输入分片，资源信息等提交到HDFS 的临时目录(Job ID 命名的目录下)，默认 10 个副本，可以通过mapreduce.client.submit.file.replication参数修改副本数量。后期作业执行时会下载这些文件到本地，中间会产生磁盘 IO。如果集群很大的时候，可以增加该参数的值，这样集群很多副本都可以供 NM 访问，从而提高下载的效率。 分片 # 源码中分片的计算公式：\n// getFormatMinSplitSize()：一个切片最少应该拥有 1 个字节\r// getMinSplitSize(job)：读取程序员设置的切片的最小值，如果没有设置默认读取 1\rlong minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));\r// 读取程序员设置的切片的最大值，如果没有设置默认读取 Long.MAX_VALUE\rlong maxSize = getMaxSplitSize(job);\r// 获取 Block 的大小（默认为 128M）\rlong blockSize = file.getBlockSize();\r// 获取 Split 的大小，切片的默认大小为 Block 的大小\r// return Math.max(minSize, Math.min(maxSize, blockSize));\r// minSize 为 64M --\u0026gt; 最终返回 128M，minSize 为 256M --\u0026gt; 最终返回 256M\r// maxSize 为 64M --\u0026gt; 最终返回 64M，maxSize 为 256M --\u0026gt; 最终返回 128M\r// 如果需要调大切片，则调节 minSize；如果需要调小切片，则调节 maxSize\rlong splitSize = computeSplitSize(blockSize, minSize, maxSize);\r因为 Map 数没有具体的参数指定（默认情况下一个切片一个 MapTask），所以可以通过如上的公式调整切片的大小，\r这样就可以实现动态设置 Map 数了，那么问题来了，Map 数该如何设置呢？ 资源 # ​\t这些东西一定要结合业务，Map 数太多，会产生很多中间结果，导致 Reduce 拉取数据变慢；Map 数太少，每个 Map处理的时间又很长。那如果数据量就是很大，并且还需要控制 Map 的数量，这个时候每个 Map 的执行时间就比较长了，这时候可以调整每个 Map 的资源来提升 Map 的处理能力，相关参数如下。\n# MapTask 的执行内存，默认为 1024MB\rmapreduce.map.memory.mb=2048\r# MapTask 的虚拟 CPU 核数，默认为 1\rmapreduce.map.cpu.vcores=1\r这里需要注意的是，单个 Map/Reduce 申请的资源大小，其值应该在每个容器申请的最大/最小分配之间，具体如下。\r# NodeManager 节点最大可用虚拟核，默认值为 -1。如果设置为 -1 且yarn.nodemanager.resource.detect-\rhardware-capabilities 为 true（默认为 false），则会自动计算(在Windows和Linux环境下)。在其他情况下，默认为8。\r# 推荐将该值设置为与物理 CPU 核数相同。如果你的节点 CPU 核数不够 8 个，则需要调减小这个值，因为 YARN 不会智能的探测节点的物理 CPU 总数。\ryarn.nodemanager.resource.cpu-vcores=-1\r# 单个容器可申请的最小虚拟 CPU 核数，默认是 1，如果一个容器申请的 CPU 个数少于该数，则修改对应的值为这个数。\ryarn.scheduler.minimum-allocation-vcores=1\r# 单个容器可申请的最大虚拟 CPU 核数，默认是 4。\ryarn.scheduler.maximum-allocation-vcores=4\r# NodeManager 节点最大可用物理内存，默认值为 -1。如果设置为-1 且yarn.nodemanager.resource.detect-\rhardware-capabilities 为 true（默认为 false），则会自动计算(在Windows和Linux环境下)。在其他情况下，默认为8192MB。\ryarn.nodemanager.resource.memory-mb=-1\r# ResourceManager 上每个容器可以申请内存资源的最小值，默认值为 1024MB\ryarn.scheduler.minimum-allocation-mb=1024\r# ResourceManager 上每个容器可以申请内存资源的最大值，默认值为 8192MB\ryarn.scheduler.maximum-allocation-mb=8192 环形缓冲区 \u0026amp; 溢写 # 从源头上确定好 Map 之后，接下来看看 Map 的具体执行过程。首先写环形数据缓冲区，为啥要写环形数据缓冲区\r呢，为什么不直接写磁盘？这样的目的主要是为了减少磁盘 IO。\r每个 Map 任务不断地将键值对输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内\r存空间，在内存中放置尽可能多的数据。该缓冲默认为 100M（ mapreduce.task.io.sort.mb 参数控制），当达到 80%（ mapreduce.map.sort.spill.percent 参数控制）时就会溢写至磁盘，每达到 80% 都会重写溢写到一个新的文件。\r可以根据机器的配置和数据量来设置这两个参数，当内存足够时，增大 mapreduce.task.io.sort.mb=500 会提高溢写的过程，而且会减少中间结果的文件数量。\rmapreduce.task.io.sort.mb=500\rmapreduce.map.sort.spill.percent=0.8 合并 # mapreduce.task.io.sort.factor=50当文件溢写完后，Map 会对这些文件进行 Merge 合并，默认每次最多合并 10 个溢写的文件，由参数\rmapreduce.task.io.sort.factor 进行设置。调大可以减少合并的次数，提高合并的并行度，降低对磁盘操作的次\r数。\rmapreduce.task.io.sort.factor=50 输出 # 组合器 # 在 Reduce 拉取数据之前，我们可以使用 Combiner 实现 Map-Side 的预聚合（不影响最终结果的情况下），如果自定义了 Combiner，此时会根据 Combiner 定义的函数对 map 方法的结果进行合并，这样可以减少数据的传输，降低磁盘和网络 IO，提升性能。 压缩 # 到了 Map 到 Reduce 的数据传输过程了，这中间主要的影响无非就是磁盘 IO，网络 IO，数据量的大小了（是否压缩），其实减少数据量的大小，就可以做到优化了，所以我们可以选择性压缩数据，压缩后数据量会进一步减少，降低\r磁盘和网络 IO，提升性能。\r开启压缩后，数据会被压缩写入磁盘，Reduce 读的是压缩数据所以需要解压，在实际经验中 Hive 在 Hadoop 的运行的瓶颈一般都是 IO 而不是 CPU，压缩一般可以 10 倍的减少 IO 操作。具体可以通过以下参数进行配置。\r# Map 的输出在通过网络发送之前是否被压缩，默认为 false 不压缩\rmapreduce.map.output.compress=false\r# 如果 Map 的输出被压缩，那么应该如何压缩它们，默认为 org.apache.hadoop.io.compress.DefaultCodec\rmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r影响线程 # Map 流程完成之后，会通过运行一个 HTTP Server 暴露自身，供 Reduce 端获取数据。这里用来响应 Reduce 数据请求的线程数量是可以配置的，通过 mapreduce.shuffle.max.threads 属性进行配置，默认为 0，表示当前机器内核数量的两倍。注意该配置是针对 NodeManager 配置的，而不是每个作业配置。具体如下。\rmapreduce.shuffle.max.threads=0 容错 # Reduce 的每一个下载线程在下载某个 Map 数据的时候，有可能因为那个 Map 中间结果所在的机器发生错误，或者中间结果的文件丢失，或者网络中断等等情况，这样 Reduce 的下载就有可能失败，所以 Reduce 的下载线程并不会无休止的等待下去，当一定时间后下载仍然失败，那么下载线程就会放弃这次下载，并在随后尝试从其他的地方下载（因为这段时间 Map 可能会重跑）。\r为什么会从其他地方下载呢？因为 Map/Reduce Task 有容错机制，当任务执行失败后会尝试重启任务，相关参数如下。\r# MapTask 最大重试次数，一旦重试次数超过该值，则认为 MapTask 运行失败，其对应的输入数据将不会产生任何结果，默认为 4\rmapreduce.map.maxattempts=4\r# ReduceTask最大重试次数，一旦重试次数超过该值，则认为ReduceTask运行失败，其对应的输入数据将不会产生任何结果，默认为4\rmapreduce.reduce.maxattempts=4\r# 当一个NodeManager 上有超过 3 个任务失败时，ApplicationMaster会将该节点上的任务调度到其他节点上执行\r# 该值必须小于 Map/Reduce Task 最大重试次数，否则失败的任务将永远不会在不同的节点上尝试\rmapreduce.job.maxtaskfailures.per.tracker=3\r# 当 NodeManager 发生故障，停止向 ResourceManager 节点发送心跳信息时，ResourceManager 节点并不会立即移除NodeManager，而是要等待一段时间，该参数如下，默认为 600000ms\ryarn.nm.liveness-monitor.expiry-interval-ms=600000\r# 如果一个 Task 在一定时间内没有任务进度的更新（ApplicationMaster 一段时间没有收到任务进度的更新），即不会读取新的数据，也没有输出数据，则认为该 Task 处于 Block 状态，可能是临时卡住，也可能会永远卡住。为了防止 Task 永远 Block 不退出，则设置了一个超时时间（单位毫秒），默认为 600000ms，为 0 表示禁用超时\rmapreduce.task.timeout=600000\r# YARN 中的应用程序失败之后，最多尝试的次数，默认为 2，即当 ApplicationMaster 失败 2 次以后，运行的任务将会失败\rmapreduce.am.max-attempts=2\r# YARN 对 ApplicationMaster 的最大尝试次数做了限制，每个在 YARN 中运行的应用程序不能超过这个数量限制\ryarn.resourcemanager.am.max-attempts=2\r# Hadoop 对 ResourceManager 节点提供了检查点机制，当所有的 ResourceManager 节点失败后，重启 ResouceManager节点，可以从上一个失败的 ResourceManager 节点保存的检查点进行状态恢复\r# 检查点的存储由 yarn-site.xml 配置文件中的 yarn-resourcemanager.store.class 属性进行设置，默认是保存到文件中\ryarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore Reduce # 资源 # 接下来就是 Reduce 了，首先可以通过参数设置合理的 Reduce 数量（ mapreduce.job.reduces 参数控制），以及通过参数设置每个 Reduce 的资源。具体如下。\r# 默认为 1\rmapreduce.job.reduces=1\r# ReduceTask 的执行内存，默认为 1024MB\rmapreduce.reduce.memory.mb=4096\r# ReduceTask 的虚拟 CPU 核数，默认为 1\rmapreduce.reduce.cpu.vcores=1\r# Map 和 Reduce 共享，当 MapTask 完成的比例达到该值后会为 ReduceTask 申请资源，默认是 0.05\r# 只要有溢写合并完成的 MapTask，申请到资源的 ReduceTask 就可以开始拉取\rmapreduce.job.reduce.slowstart.completedmaps=0.05\r这里需要注意的是，单个 Map/Reduce 申请的资源大小，其值应该在每个容器申请的最大/最小分配之间，具体如下。\r# NodeManager 节点最大可用虚拟核，默认值为 -1。如果设置为 -1 且yarn.nodemanager.resource.detect-\rhardware-capabilities 为 true（默认为 false），则会自动计算(在Windows和Linux环境下)。在其他情况下，默认为8。\r# 推荐将该值设置为与物理 CPU 核数相同。如果你的节点 CPU 核数不够 8 个，则需要调减小这个值，因为 YARN 不会智能的探测节点的物理 CPU 总数。\ryarn.nodemanager.resource.cpu-vcores=-1\r# 单个容器可申请的最小虚拟 CPU 核数，默认是 1，如果一个容器申请的 CPU 个数少于该数，则修改对应的值为这个数。\ryarn.scheduler.minimum-allocation-vcores=1\r# 单个容器可申请的最大虚拟 CPU 核数，默认是 4。\ryarn.scheduler.maximum-allocation-vcores=4\r# NodeManager 节点最大可用物理内存，默认值为 -1。如果设置为 -1 且 yarn.nodemanager.resource.detecthardware-capabilities 为 true（默认为 false），则会自动计算(在Windows和Linux环境下)。在其他情况下，默认为、8192MB。\ryarn.nodemanager.resource.memory-mb=-1\r# ResourceManager 上每个容器可以申请内存资源的最小值，默认值为 1024MB\ryarn.scheduler.minimum-allocation-mb=1024\r# ResourceManager 上每个容器可以申请内存资源的最大值，默认值为 8192MB\ryarn.scheduler.maximum-allocation-mb=8192 拉取 # Reduce 在 Copy 的过程中默认使用 5 个（ mapreduce.reduce.shuffle.parallelcopies 参数控制）并行度进行数据复制，可以将其调大例如 100。Reduce 的每一个下载线程在下载某个 Map 数据的时候，有可能因为那个 Map 中间结果所在的机器发生错误，或者中间结果的文件丢失，或者网络中断等等情况，这样 Reduce 的下载就有可能失败，所以 Reduce 的下载线程并不会无休止的\r等待下去，当一定时间后下载仍然失败，那么下载线程就会放弃这次下载，并在随后尝试从其他的地方下载（因为这段时间 Map 可能会重跑）。Reduce下载线程的最大下载时间段通过mapreduce.reduce.shuffle.read.timeout （默认为 180000 秒）进行调整。 缓冲区\u0026amp;溢写 # Copy 过来的数据会先放入内存缓冲区中，然后当使用内存达到一定量的时候才 Spill 磁盘。这里的缓冲区大小要比Map 端的更为灵活，它基于 JVM 的 Heap Size 进行设置。该内存大小不像 Map一样可以通过 mapreduce.task.io.sort.mb来设置，而是通过参数mapreduce.reduce.shuffle.input.buffer.percent （默认为 0.7）进行设置。意思是说，Shuffile 在 Reduce 内存中的数据最多使用内存量为：0.7 * maxHeap of reduce task，内存到磁盘 Merge 的启动门限可以通过 mapreduce.reduce.shuffle.merge.percent （默认为 0.66）进行设置。\r假设 mapreduce.reduce.shuffle.input.buffer.percent 为 0.7，ReduceTask 的 max heapsize 为 1G，那么用来做拉取数据缓存的内存大概为 700MB 左右。这 700MB 的内存跟 Map 端一样，也不是要等到全部写满才会往磁盘溢写，而是达到指定的阈值就会开始往磁盘溢写（溢写前会先做 sortMerge）。这个限度阈值可以通过参数mapreduce.reduce.shuffle.merge.percent 来设定（默认为 0.66）。整个过程同 Map 类似，如果用户设置了 Combiner，也会被启用，然后磁盘中会生成众多的溢写文件。这种 Merge 方式一直在运行，直到没有 Map 端的数据时才会结束，然后启动磁盘到磁盘的 Merge 方式生成最终的文件。 合并 # 同 Map 一样，当文件溢写完后，Reduce 会对这些文件进行 Merge 合并。最大合并因子默认为 10，由参数\rmapreduce.task.io.sort.factor 进行设置。如果 Map 输出很多，则需要合并很多趟，所以可以减少合并的次数，提高合并的并行度，降低对磁盘操作的次数。 读缓存 # 默认情况下，数据达到一个阈值的时候，缓冲区中的数据就会写入磁盘，然后 Reduce 会从磁盘中获得所有的数据。也就是说，缓冲区和 Reduce 是没有直接关联的，中间会有多次写磁盘 -\u0026gt; 读磁盘的过程，既然有这个弊端，那么可以通过修改参数，使得缓冲区中的一部分数据可以直接输送到 Reduce（缓冲区 -\u0026gt; 读缓存 -\u0026gt; Reduce），从而减少 IO 开销。\r修改参数 mapreduce.reduce.input.buffer.percent ，默认为 0.0，表示不开启缓存，直接从磁盘读。当该值大于 0 的时候，会保留指定比例的内存用于缓存（缓冲区 -\u0026gt; 读缓存 -\u0026gt; Reduce），从而提升计算的速度。这样一来，设置缓冲区需要内存，读取数据需要内存，Reduce 计算也需要内存，所以要根据作业的用运行情况进行调整。\r当 Reduce 计算逻辑消耗内存很小时，可以分一部分内存用来缓存数据，可以提升计算的速度。默认情况下都是从磁盘读取数据，如果内存足够大的话，务必设置该参数让 Reduce 直接从缓存读数据 ","date":"2024-07-01","externalUrl":null,"permalink":"/docs/hadoop-mapreduce/hadoop-mapreduce/","section":"Docs","summary":"Hadoop MapReduce # https://www.","title":"Hadoop MapReduce","type":"docs"},{"content":"\rShell 编程概述 # Shell 本身并不是内核的一部分，它只是站在内核的基础上编写的一个应用程序，它和 QQ、迅雷、Firefox 等其它软件没有什么区别。然而 Shell 也有着它的特殊性，就是开机立马启动，并呈现在用户面前；用户通过 Shell 来使用 Linux，不启动 Shell 的话，用户就没办法使用 Linux。\n在计算机科学中，Shell 俗称壳（用来区别于核），是指“为使用者提供操作界面”的软件（command interpreter，命令解析器）。它类似于 DOS 下的 COMMAND.COM 和后来的 cmd.exe。它接收用户命令，然后调用相应的应用程序。\nShell 并不是简单的堆砌命令，我们还可以在 Shell 中编程，这和使用 C++、Java、Python 等常见的编程语言并没有什么两样。\nShell 虽然没有 C++、Java、Python 等强大，但也支持了基本的编程元素，例如：\n变量、数组、字符串、注释、加减乘除、逻辑运算等概念； if\u0026hellip;else 选择结构，case\u0026hellip;in 开关语句，for、while、until 循环； 函数，包括用户自定义的函数和内置函数（例如 printf、export、eval 等）。 站在这个角度讲，Shell 也是一种编程语言，它的编译器（解释器）是 Shell 这个程序。我们平时所说的 Shell，有时候是指连接用户和内核的这个程序，有时候也指 Shell 编程。\nShell 名词解释 # Shell # Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 也是一个命令行解释器，是用户和内核之间的接口。用户可以在 Shell 中输入命令，然后，它解释命令来执行所需的任务。此外，它还可以执行程序和 Shell 脚本。Shell 脚本是一组命令，用户应该遵循标准语法向 Shell 写入命令。\n总的来说，Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。Shell 既是一种命令语言，又是一种程序设计语言，如果要与内核打交道就必须学习 Shell 语言。\n分类 # Shell 是提供与内核沟通接口的命令解释器程序，但实际上 Shell 是这种解释器的统称，Linux 系统的 Shell 种类很多，包括 Bourne Shell（简称 sh）、Bourne Again Shell（简称 bash）、C Shell（简称 csh）、K shell（简称 ksh）、Shell for Root 等等。如下图：\n也就是说 sh 和 bash 都是 Linux 系统 Shell 的一种，其中 bash 命令是 sh 命令的超集，大多数 sh 脚本都可以在 bash 下运行。Linux 系统中预设默认使用的就是 bash。\n要想知道操作系统支持哪种Shell类型，可在终端中输入以下命令：\n[root@node01 ~]# cat /etc/shells /bin/sh /bin/bash /usr/bin/sh /usr/bin/bash 要想知道 bash 在操作系统中具体的位置，可通过以下命令查看：\n[root@node01 ~]# which bash sh /usr/bin/bash /usr/bin/sh She Bang # She Bang 是 Shell 脚本开头字符#!也可以叫 Sha Bang，当 Shell 文件被 Linux 系统读取时，内核会通过#!表示的值（0x23, 0x21）识别出随后的解释器路径并调用，最后再将整个脚本内容传递给解释器。由于 Shell 当中#字符同时表示注释，因此 Shell 解释脚本文件时会自动忽略该行。\n总结：#!就是告诉系统解释此脚本文件的 Shell 程序在哪（其后路径所指定的程序）。例如：\n#!/bin/bash echo \u0026#34;Hello World!\u0026#34; She Bang 的格式很重要，格式不正确会导致命令工作不正常。因此，在创建脚本时，要始终记住 She Bang 格式的这两点：\n它应该始终在脚本的第一行。 在#!和解释器的路径之间，#之前不应有任何空格。 echo是 bash 中的内置命令，用于通过传递参数来显示标准输出。它是用于将文本/字符串行打印到屏幕上的最广泛使用的命令。\n脚本 # 在计算机编程中，脚本是用于适当的运行时环境的一组命令，这些命令用于自动执行任务。\n我们经常说的 Shell 脚本，其实就是利用 Shell 的功能，编写能够直接运行的脚本文件。\n第一个 Shell 脚本 # 几乎所有的编程语言教程都是从著名的“Hello World”开始，出于对这种传统的尊重，我们的第一个 Shell 脚本也输出“Hello World”。\n打开文本编辑器，新建一个文本文件，并命名为 hello.sh。\n扩展名sh代表 shell，扩展名并不影响脚本执行，见名知意就好。\n在 hello.sh 中输入代码：\n#!/bin/bash echo \u0026#34;Hello World!\u0026#34; #!是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell；后面的/bin/bash就是指明了解释器的具体位置。\necho命令用于向标准输出文件（Standard Output，stdout，一般就是指显示器）输出文本。在.sh文件中使用命令与在终端直接输入命令的效果是一样的。\n接下来使用bash或sh运行脚本：\n[root@node01 ~]# bash hello.sh Hello World! Shell 脚本执行 # 脚本的执行并非只有bash和sh，还有source和.且它们之间还存在一些细微的差异，接下来我们详细的给大家讲解一下。\n使用路径 # 格式：相对路径/脚本.sh或绝对路径/脚本.sh。\n注意：脚本文件必须为可执行文件（拥有 x 权限）。\n范例：\n[root@node01 ~]# ls -l total 4 -rw-r--r-- 1 root root 33 May 29 11:57 hello.sh [root@node01 ~]# /root/hello.sh bash: /root/hello.sh: Permission denied [root@node01 ~]# chmod ug+x hello.sh [root@node01 ~]# ./hello.sh Hello World! [root@node01 ~]# /root/hello.sh Hello World! bash 或 sh # 格式：bash 脚本.sh或sh 脚本.sh。\n范例：\n[root@node01 ~]# ls -l total 4 -rw-r--r-- 1 root root 33 May 29 11:57 hello.sh [root@node01 ~]# bash hello.sh Hello World! [root@node01 ~]# sh hello.sh Hello World! source 或 . # 格式：source 脚本.sh或. 脚本.sh\n范例：\n[root@node01 ~]# ls -l total 4 -rw-r--r-- 1 root root 33 May 29 11:57 hello.sh [root@node01 ~]# . hello.sh Hello World! [root@node01 ~]# source hello.sh Hello World! 区别 # bash 或 sh 执行脚本时会新开一个 bash，不同 bash 中的变量无法共享。而 source 或 . 是在同一个 bash 里面执行的，所以变量可以共享。\n范例：\n[root@node01 ~]# cat hello.sh #!/bin/bash echo \u0026#34;Hello World!\u0026#34; echo ${name} [root@node01 ~]# name=mrhelloworld [root@node01 ~]# bash hello.sh Hello World! [root@node01 ~]# sh hello.sh Hello World! [root@node01 ~]# . hello.sh Hello World! mrhelloworld [root@node01 ~]# source hello.sh Hello World! mrhelloworld 在脚本中添加ping baidu.com的指令，用不同的方式重新执行脚本并查看进程。\n[root@node01 ~]# echo \u0026#34;ping baidu.com\u0026#34; \u0026gt;\u0026gt; hello.sh [root@node01 ~]# cat hello.sh #!/bin/bash echo \u0026#34;Hello World!\u0026#34; echo ${name} ping baidu.com bash：\n[root@node01 ~]# bash hello.sh [root@node01 ~]# ps -ef root 4447 4445 0 12:15 pts/0 00:00:00 -bash root 4494 4447 0 12:17 pts/0 00:00:00 bash hello.sh root 4495 4494 0 12:17 pts/0 00:00:00 ping baidu.com sh：\n[root@node01 ~]# sh hello.sh [root@node01 ~]# ps -ef root 4447 4445 0 12:15 pts/0 00:00:00 -bash root 4497 4447 0 12:18 pts/0 00:00:00 sh hello.sh root 4498 4497 0 12:18 pts/0 00:00:00 ping baidu.com source：\n[root@node01 ~]# source hello.sh [root@node01 ~]# ps -ef root 4447 4445 0 12:15 pts/0 00:00:00 -bash root 4502 4447 0 12:19 pts/0 00:00:00 ping baidu.com .：\n[root@node01 ~]# . hello.sh [root@node01 ~]# ps -ef root 4447 4445 0 12:15 pts/0 00:00:00 -bash root 4508 4447 0 12:25 pts/0 00:00:00 ping baidu.com 怎么解决这个问题呢？可以使用export命令，它可以将当前进程的变量传递给子进程去使用，如下：\n[root@node01 ~]# export name=mrhelloworld [root@node01 ~]# bash hello.sh Hello World! mrhelloworld 所以，将来在配置环境变量（profile 文件）的时候，所有的变量前必须加export。\nShell 基础 # 注释 # 单行注释 # 要在 bash 中编写单行注释，必须在注释的开头使用井号#。\n#!/bin/bash # 我是注释 多行注释 # 有两种方法可以在 bash 脚本中插入多行注释：\n通过在\u0026lt;\u0026lt; COMMENT和COMMENT之间加上注释，可以在 bash 脚本中编写多行注释。 也可以通过将注释括在: '和单引号'之间来编写多行注释。 #!/bin/bash \u0026lt;\u0026lt; EOF 我是注释 我是注释 我是注释 EOF echo \u0026#34;Hello World!\u0026#34; 提示：EOF 表示 End Of File，表示文件结尾，这里代指从哪开始到哪结束。EOF 只是一个名称而已，可以使用任意非关键字名称进行替换，例如 COMMENT，通常都使用 EOF。\n或者：\n#!/bin/bash : \u0026#39; 我是注释 我是注释 我是注释 \u0026#39; echo \u0026#34;Hello World\u0026#34; 变量 # 语法 # 变量是将数据或有用的信息作为值存储的容器。变量的值可以更改，并且可以多次使用。变量是任何类型的数据（例如整数，浮点数，字符等）的临时存储。\n定义变量时，变量名不加$符号，而引用变量时则需要使用$。同其他编程语言一样，Shell 的变量声明也需要遵循一定的规则：\n可以包含字母，数字和下划线。 只能以字母和下划线开头，不能定义以任何数字开头的变量名称。 严格区分大小写。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 变量名称与值之间的等号=的两侧不能有空格。 不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。 Shell 声明变量的语法格式如下：\nvariable=value variable=\u0026#39;value\u0026#39; variable=\u0026#34;value\u0026#34; variable 是变量名，value 是赋给变量的值。如果 value 不包含任何空白符（例如空格、Tab 缩进等），那么可以不使用引号；如果 value 包含了空白符，那么就必须使用引号包围起来。使用单引号和使用双引号也是有区别的，稍后我们会详细说明。\n变量定义举例：\n# 变量的声明与赋值 name=\u0026#34;zhangsan\u0026#34; # 变量的调用 echo $name echo ${name} # 修改变量的值，已定义的变量，可以被重新赋值 name=\u0026#34;lisi\u0026#34; # 只读变量 url=\u0026#34;https://www.baidu.com\u0026#34; readonly url # 测试只读变量是否可以被修改 url=\u0026#34;https://www.google.com\u0026#34; # 删除变量 unset name # 将命令结果复制给变量 info=`ls /usr/` info=$(ls /usr/) 调用变量时，变量名外面的花括号{}是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况：\nskill=\u0026#34;Shell\u0026#34; echo \u0026#34;I am good at ${skill}Script\u0026#34; 如果不给 skill 变量加花括号，写成echo \u0026quot;I am good at $skillScript\u0026quot;，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。\n调用变量时，推荐给所有变量加上花括号{}，这是个良好的编程习惯。\nShell 也支持将命令的执行结果赋值给变量，常见的有以下两种方式：\nvariable=`command` variable=$(command) 第一种方式把命令用反引号（位于 Esc 键的下方）包围起来，反引号和单引号非常相似，容易产生混淆，所以不推荐使用这种方式；第二种方式把命令用$()包围起来，区分更加明显，所以推荐使用这种方式。\n类型 # 局部变量：局部变量在脚本或命令中定义，仅在当前 Shell 实例中有效，其他 Shell 启动的程序不能访 问局部变量。例如，不同会话创建的变量无法互相访问。 环境变量：所有的程序，包括 Shell 启动的程序，都能访问环境变量，有些程序需要环境变量来保证 其正常运行。 Shell 变量：Shell 变量是由 Shell 程序设置的特殊变量。Shell 变量中有一部分是环境变量，有一部分是 局部变量。例如： # bash 在操作系统中具体的位置 echo ${BASH} # bash 版本信息 echo ${BASH_VERSION} # 操作系统的类型 echo $OSTYPE # 当前登录用户 echo ${USERNAME} # 当前用户家目录 echo ${HOME} # 当前的工作目录 echo ${PWD} 引号 # 当希望变量存储更复杂的值时，就需要使用引号。引号用于处理带有空格字符的文本和文件名。这是因为 Bash 使用空格来确定单独的项目。在 Shell 中，变量的值可以由单引号' '包围，也可以由双引号\u0026quot; \u0026quot;包围，它们到底有什么区别呢？不妨以下面的代码为例来说明：\n#!/bin/bash name=\u0026#34;zhangsan\u0026#34; echo \u0026#39;My name is ${name}\u0026#39; echo \u0026#34;My name is ${name}\u0026#34; 运行结果如下：\nMy name is ${name} My name is zhangsan 单引号' '包围变量的值时，单引号里面是什么就输出什么，即使内容中有变量和命令（命令需要反引起来）也会把它们原样输出。这种方式比较适合定义显示纯字符串的情况，即不希望解析变量、命令等的场景。\n双引号\u0026quot; \u0026quot;包围变量的值时，输出时会先解析里面的变量和命令，而不是把双引号中的变量名和命令原样输出。这种方式比较适合字符串中附带有变量和命令并且想将其解析后再输出的变量定义。\n位置参数 # 运行 Shell 脚本文件时我们还可以给它传递一些参数，这些参数在脚本文件内部可以使用$n的形式来接收。例如，$1表示第一个参数，$2表示第二个参数，依次类推。\n这种通过$n的形式来接收的参数，在 Shell 中称为位置参数。在讲解变量的命名时，我们提到变量的名字必须以字母或者下划线开头，不能以数字开头；但是位置参数却偏偏是数字，这和变量的命名规则是相悖的，所以我们将它们视为“特殊变量”。\n注意：如果参数个数太多，达到或者超过了 10 个，那么就得用${n}的形式来接收了，例如 ${10}、${23}。{}的作用是为了帮助解释器识别参数的边界，这跟使用变量时加{}是一样的效果。\n除了$n，Shell 中还有$#、$*、$@、$?、$$几个特殊参数，我们将在下节讲解。\n范例 test.sh：\n#!/bin/bash echo \u0026#34;name: $1\u0026#34; echo \u0026#34;age: $2\u0026#34; 运行 test.sh，并附带参数：\n[root@node01 ~]# bash test.sh zhangsan 18 name: zhangsan age: 18 特殊变量 # 上节我们讲到了 $n，它是特殊变量的一种，用来接收位置参数。本节我们继续讲解剩下的几个特殊变量，它们分别是：$#、$*、$@、$?、$$。\n变量 含义 $$ 当前 Shell 进程 ID。对于 Shell 脚本，就是这些脚本所在的进程 ID。 $0 当前脚本的文件名。 $n 传递给脚本或函数的参数。n 为数字，表示第几个参数。例如，第一个参数是 $1，第二个参数是 $2。 $@ 传递给脚本或函数的所有参数。 $* 传递给脚本或函数的所有参数。当被双引号\u0026quot; \u0026quot;包含时，$@ 与 $* 稍有不同。 $# 传递给脚本或函数的参数个数。 $? 上个命令的退出状态，或函数的返回值。 $*和$@作用都是获取传递给脚本或函数的所有参数。在没有被双引号包围时，两者没有区别，接收到的每个参数都是独立的，用空格分隔。\n当被双引号包围时，$@与没有被双引号包围时没有变化，每个参数依然是独立的。但是$*被双引号包围时，会将所有参数看作一个整体。\n$?是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。所谓退出状态，就是上一个命令执行后的返回结果。退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1，这和 C 语言的 main() 函数是类似的。\n范例 test.sh：\n#!/bin/bash echo \u0026#34;Process ID: $$\u0026#34; echo \u0026#34;File Name: $0\u0026#34; echo \u0026#34;First Parameter: $1\u0026#34; echo \u0026#34;Second Parameter: $2\u0026#34; echo \u0026#34;All parameters 1: $@\u0026#34; echo \u0026#34;All parameters 2: $*\u0026#34; echo \u0026#34;Total: $#\u0026#34; echo \u0026#34;--------------------\u0026#34; for var in \u0026#34;$@\u0026#34; do echo ${var} done echo \u0026#34;--------------------\u0026#34; for var in \u0026#34;$*\u0026#34; do echo ${var} done 运行 test.sh，并附带参数：\n[root@node01 ~]# bash test.sh Linux Shell Process ID: 4779 File Name: test.sh First Parameter: Linux Second Parameter: Shell All parameters 1: Linux Shell All parameters 2: Linux Shell Total: 2 -------------------- Linux Shell -------------------- Linux Shell $?是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。所谓退出状态，就是上一个命令执行后的返回结果。退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1，这和C语言的 main() 函数是类似的。不过，也有一些命令返回其他值，表示不同类型的错误。\n范例1，$? 获取上一个命令的退出状态。test.sh：\n#!/bin/bash if [ \u0026#34;$1\u0026#34; == 100 ] then exit 0 # 参数正确，退出状态为 0 else exit 1 # 参数错误，退出状态 1 fi 例如，运行 test.sh 时传递参数 100：\n[root@node01 ~]# bash test.sh 100 [root@node01 ~]# echo $? 0 再如，运行 test.sh 时传递参数 50：\n[root@node01 ~]# bash test.sh 50 [root@node01 ~]# echo $? 1 范例2，$? 获取函数的返回值。test.sh：\n#!/bin/bash # 得到两个数相加的和 function add() { return `expr $1 + $2` } # 调用函数 add 20 30 echo $? # 获取函数返回值 运行结果如下：\n[root@node01 ~]# bash test.sh 50 有 C++、C#、Java 等编程经验的读者请注意：严格来说，Shell 函数中的 return 关键字用来表示函数的退出状态，而不是函数的返回值；Shell 不像其它编程语言，没有专门处理返回值的关键字。\n范例 2 在其它编程语言中没有任何问题，但是在 Shell 中是非常错误的，Shell 函数的返回值和其它编程语言大有不同，我们将在后面的 Shell 函数返回值中展开讨论。\n字符串 # 定义 # 字符串（String）就是一系列字符的组合。字符串是 Shell 编程中最常用的数据类型之一（除了数字和字符串，也没有其他类型了）。\n字符串可以由单引号' '包围，也可以由双引号\u0026quot; \u0026quot;包围，也可以不用引号。\n由单引号' '包围的字符串：\n任何字符都会原样输出，在其中使用变量是无效的。\n字符串中不能出现单引号，即使对单引号进行转义（\\'）也不行。\n由双引号\u0026quot; \u0026quot;包围的字符串：\n如果其中包含了某个变量，那么该变量会被解析（得到该变量的值），而不是原样输出。\n字符串中可以出现双引号，只要它被转义（\\\u0026quot;）就行。\n不被引号包围的字符串：\n不被引号包围的字符串中出现变量时也会被解析，这一点和双引号\u0026quot; \u0026quot;包围的字符串一样。\n字符串中不能出现空格，否则空格后边的字符串会作为其他变量或者命令解析。\n长度 # 在 Shell 中获取字符串长度很简单，具体方法如下：\n[root@node01 ~]# name=mrhelloworld [root@node01 ~]# echo ${#name} 12 拼接 # 在脚本语言中，字符串的拼接（也称字符串连接或者字符串合并）往往都非常简单，例如：\n在 PHP 中，使用 . 即可连接两个字符串； 在 JavaScript 中，使用 + 即可将两个字符串合并为一个。 然而，在 Shell 中你不需要使用任何运算符，只需要将两个字符串并排放在一起就能实现拼接，非常简单粗暴。范例 test.sh：\n#!/bin/bash name=\u0026#34;zhangsan\u0026#34; age=18 str1=$name$age # 中间不能有空格 str2=\u0026#34;$name $age\u0026#34; # 如果被双引号包围，那么中间可以有空格 str3=$name\u0026#34;: \u0026#34;$age # 中间可以出现别的字符串 str4=\u0026#34;$name: $age\u0026#34; # 这样写也可以 str5=\u0026#34;${name}同学: ${age}岁\u0026#34; # 这个时候需要给变量名加上大括号 echo $str1 echo $str2 echo $str3 echo $str4 echo $str5 运行 test.sh：\n[root@node01 ~]# bash test.sh zhangsan18 zhangsan 18 zhangsan: 18 zhangsan: 18 zhangsan同学: 18岁 截取 # Shell 截取字符串通常有两种方式：\n从指定位置开始截取 从指定字符（子字符串）开始截取 从指定位置开始截取 # 这种方式需要两个参数：除了指定起始位置，还需要截取长度，才能最终确定要截取的字符串。\n既然需要指定起始位置，那么就涉及到计数方向的问题，到底是从字符串左边开始计数，还是从字符串右边开始计数。答案是 Shell 同时支持两种计数方式。\n如果想从字符串的左边开始计数，那么截取字符串的具体格式如下：\n${string:start:length} 其中，string 是要截取的字符串，start 是起始位置（从左边开始，从 0 开始计数），length 是要截取的长度（省略的话表示直到字符串的末尾）。\n范例：\n[root@node01 ~]# name=mrhelloworld [root@node01 ~]# echo ${name:2} helloworld [root@node01 ~]# echo ${name:2:5} hello 如果想从字符串的右边开始计数，那么截取字符串的具体格式如下：\n${string:0-start:length} 相比从左边开始计数仅仅多了0-，这是固定的写法，专门用来表示从字符串右边开始计数。\n范例：\n[root@node01 ~]# name=mrhelloworld [root@node01 ~]# echo ${name:0-5} world [root@node01 ~]# echo ${name:0-10} helloworld [root@node01 ~]# echo ${name:0-10:5} hello 从指定字符（子字符串）开始截取 # 这种截取方式无法指定字符串长度，只能从指定字符（子字符串）截取到字符串末尾。Shell 可以截取指定字符（子字符串）右边的所有字符，也可以截取左边的所有字符。\n使用#号可以截取指定字符（或者子字符串）右边的所有字符，具体格式如下：\n${string#*chars} 其中，string 表示要截取的字符串，chars 是指定的字符（或者子字符串），*是通配符的一种，表示任意长度的字符串。*chars连起来使用的意思是：忽略左边的所有字符，直到遇见 chars（chars 不会被截取）。\n范例：\n[root@node01 ~]# url=https://www.yjxxt.com [root@node01 ~]# echo ${url#*https://} www.yjxxt.com [root@node01 ~]# echo ${url#*://} www.yjxxt.com 注意，以上写法遇到第一个匹配的字符（子字符串）就结束了，如果希望直到最后一个指定字符（子字符串）再匹配结束，那么可以使用##。例如：\n[root@node01 ~]# echo ${url#*.} yjxxt.com [root@node01 ~]# echo ${url##*.} com 使用%号可以截取指定字符（或者子字符串）左边的所有字符，具体格式如下：\n${string%chars*} 请注意*的位置，因为要截取 chars 左边的字符，而忽略 chars 右边的字符，所以*应该位于 chars 的右侧。其他方面%和#的用法相同，这里不再赘述，仅举例说明：\n[root@node01 ~]# url=https://www.yjxxt.com [root@node01 ~]# echo ${url%www*} https:// [root@node01 ~]# echo ${url%.*} https://www.yjxxt [root@node01 ~]# echo ${url%%.*} https://www 总结 # 格式 说明 ${string:start:length} 从 string 字符串的左边第 start 个字符开始，向右截取 length 个字符。 ${string:start} 从 string 字符串的左边第 start 个字符开始截取，向右直到最后。 ${string:0-start:length} 从 string 字符串的右边第 start 个字符开始，向左截取 length 个字符。 ${string:0-start} 从 string 字符串的右边第 start 个字符开始截取，向左直到最后。 ${string#*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string##*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string%*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 ${string%%*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 数组 # 和其他编程语言一样，Shell 也支持数组。数组（Array）是若干数据的集合，其中的每一份数据都称为元素（Element）。\nShell 没有限制数组的大小，理论上可以存放无限量的数据。和 C++、Java 等类似，Shell 数组元素的下标也是从 0 开始计数。\n获取数组中的元素要使用下标[index]，下标可以是一个整数，也可以是一个结果为整数的表达式；当然，下标必须大于等于 0。遗憾的是，常用的 Bash Shell 只支持一维数组，不支持多维数组。\n定义 # 在 Shell 中，用括号()来表示数组，数组元素之间用空格来分隔。由此，定义数组的一般形式为：\narray_name=(ele1 ele2 ele3 ... elen) Shell 是弱类型的，它并不要求所有数组元素的类型必须相同，例如：\narr=(20 56 \u0026#34;mrhelloworld\u0026#34;) Shell 数组的长度不是固定的，定义之后还可以增加元素。例如，对于上面的 arr 数组，它的长度是 3，使用下面的代码会在最后增加一个元素，使其长度扩展到 4：\nnums[3]=88 此外，Shell 还支持只给特定元素赋值：\nages=([3]=24 [5]=19 [10]=12) 以上代码就只给第 3、5、10 个元素赋值，所以数组长度是 3。\n获取 # 获取数组元素的值，一般使用下面的格式：\n${array_name[index]} 其中，array_name 是数组名，index 是下标。例如：\n[root@node01 ~]# echo ${arr[2]} mrhelloworld 使用@或*可以获取数组中的所有元素，例如：\n[root@node01 ~]# echo ${arr[*]} 20 56 mrhelloworld [root@node01 ~]# echo ${ages[@]} 24 19 12 长度 # 所谓数组长度，就是数组元素的个数。使用@或*可以获取数组中的所有元素，然后使用#来获取数组元素的个数，所以获取数组长度的语法如下：\n[root@node01 ~]# echo ${#arr[@]} 3 [root@node01 ~]# echo ${#ages[*]} 3 如果某个元素是字符串，还可以通过指定下标的方式获得该元素的长度，如下所示：\n[root@node01 ~]# echo ${#arr[2]} 12 拼接 # 所谓 Shell 数组拼接（数组合并），就是将两个数组连接成一个数组。\n拼接数组的思路是：先利用@或*，将数组展开成列表，然后再合并到一起。具体格式如下：\narray_new=(${array1[@]} ${array2[@]}) array_new=(${array1[*]} ${array2[*]}) 两种方式是等价的，选择其一即可。其中，array1 和 array2 是需要拼接的数组，array_new 是拼接后形成的新数组。\n范例 test.sh：\n#!/bin/bash array1=(23 56) array2=(99 \u0026#34;mrhelloworld\u0026#34;) array_new=(${array1[@]} ${array2[*]}) echo ${array_new[@]} # 也可以写作 ${array_new[*]} 运行结果如下：\n[root@node01 ~]# bash test.sh 23 56 99 mrhelloworld 删除 # 在 Shell 中，使用 unset 关键字来删除数组元素，具体格式如下：\nunset array_name[index] 其中，array_name 表示数组名，index 表示数组下标。\n如果不写下标只写数组名，则表示删除整个数组，所有元素都会消失。\n范例 test.sh：\n#!/bin/bash arr=(23 56 99 \u0026#34;mrhelloworld\u0026#34;) unset arr[1] echo ${arr[@]} unset arr echo ${arr[*]} echo \u0026#39;--------------------\u0026#39; 运行结果如下：\n[root@node01 ~]# bash test.sh 23 99 mrhelloworld -------------------- Shell 高级 # Shell 运算符 # Shell 和其他编程语言一样，支持多种运算符，包括：\n算数运算符 关系运算符 逻辑运算符 字符串运算符 文件测试运算符 算数运算符 # 但是，Shell 和其它编程语言不同，Shell 不能直接进行算数运算，必须使用数学计算命令，这让初学者感觉很困惑，也让有经验的程序员感觉很奇葩。expr 是一款表达式计算工具，使用它能完成表达式的求值操作。\n#!/bin/bash a=10 b=20 val=$(expr $a + $b) echo \u0026#34;a + b : $val\u0026#34; val=`expr $a - $b` echo \u0026#34;a - b : $val\u0026#34; val=`expr $a \\* $b` echo \u0026#34;a * b : $val\u0026#34; val=`expr $b / $a` echo \u0026#34;b / a : $val\u0026#34; val=`expr $b % $a` echo \u0026#34;b % a : $val\u0026#34; if [ $a == $b ] then echo \u0026#34;a 等于 b\u0026#34; fi if [ $a != $b ] then echo \u0026#34;a 不等于 b\u0026#34; fi 关系运算符 # 关系运算符只支持数字，不支持字符串，除非字符串的值是数字。\n#!/bin/bash a=10 b=20 if [ $a -eq $b ] then echo \u0026#34;$a -eq $b : a 等于 b\u0026#34; else echo \u0026#34;$a -eq $b: a 不等于 b\u0026#34; fi if [ $a -ne $b ] then echo \u0026#34;$a -ne $b: a 不等于 b\u0026#34; else echo \u0026#34;$a -ne $b : a 等于 b\u0026#34; fi if [ $a -gt $b ] then echo \u0026#34;$a -gt $b: a 大于 b\u0026#34; else echo \u0026#34;$a -gt $b: a 不大于 b\u0026#34; fi if [ $a -lt $b ] then echo \u0026#34;$a -lt $b: a 小于 b\u0026#34; else echo \u0026#34;$a -lt $b: a 不小于 b\u0026#34; fi if [ $a -ge $b ] then echo \u0026#34;$a -ge $b: a 大于或等于 b\u0026#34; else echo \u0026#34;$a -ge $b: a 小于 b\u0026#34; fi if [ $a -le $b ] then echo \u0026#34;$a -le $b: a 小于或等于 b\u0026#34; else echo \u0026#34;$a -le $b: a 大于 b\u0026#34; fi 逻辑运算符 # #!/bin/bash a=10 b=20 if [ $a != $b ] then echo \u0026#34;$a != $b : a 不等于 b\u0026#34; else echo \u0026#34;$a == $b: a 等于 b\u0026#34; fi if [ $a -lt 100 -a $b -gt 15 ] then echo \u0026#34;$a 小于 100 且 $b 大于 15 : 返回 true\u0026#34; else echo \u0026#34;$a 小于 100 且 $b 大于 15 : 返回 false\u0026#34; fi if [ $a -lt 100 -o $b -gt 100 ] then echo \u0026#34;$a 小于 100 或 $b 大于 100 : 返回 true\u0026#34; else echo \u0026#34;$a 小于 100 或 $b 大于 100 : 返回 false\u0026#34; fi if [ $a -lt 5 -o $b -gt 100 ] then echo \u0026#34;$a 小于 5 或 $b 大于 100 : 返回 true\u0026#34; else echo \u0026#34;$a 小于 5 或 $b 大于 100 : 返回 false\u0026#34; fi #!/bin/bash a=10 b=20 if [[ $a -lt 100 \u0026amp;\u0026amp; $b -gt 100 ]] then echo \u0026#34;返回 true\u0026#34; else echo \u0026#34;返回 false\u0026#34; fi if [[ $a -lt 100 || $b -gt 100 ]] then echo \u0026#34;返回 true\u0026#34; else echo \u0026#34;返回 false\u0026#34; fi 字符串运算符 # #!/bin/bash a=\u0026#34;abc\u0026#34; b=\u0026#34;efg\u0026#34; if [ $a = $b ] then echo \u0026#34;$a = $b : a 等于 b\u0026#34; else echo \u0026#34;$a = $b: a 不等于 b\u0026#34; fi if [ $a != $b ] then echo \u0026#34;$a != $b : a 不等于 b\u0026#34; else echo \u0026#34;$a != $b: a 等于 b\u0026#34; fi if [ -z $a ] then echo \u0026#34;-z $a : 字符串长度为 0\u0026#34; else echo \u0026#34;-z $a : 字符串长度不为 0\u0026#34; fi if [ -n \u0026#34;$a\u0026#34; ] then echo \u0026#34;-n $a : 字符串长度不为 0\u0026#34; else echo \u0026#34;-n $a : 字符串长度为 0\u0026#34; fi if [ $a ] then echo \u0026#34;$a : 字符串不为空\u0026#34; else echo \u0026#34;$a : 字符串为空\u0026#34; fi 文件测试运算符 # 操作符 说明 举例 -b file 检测文件是否是块设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -c $file ] 返回 false。 -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 [ -f $file ] 返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 [ -k $file ] 返回 false。 -p file 检测文件是否是有名管道，如果是，则返回 true。 [ -p $file ] 返回 false。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。 -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。 -s file 检测文件是否为空（文件大小是否大于0），不为空返回 true。 [ -s $file ] 返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 #!/bin/bash file=\u0026#34;/root/test.sh\u0026#34; if [ -r $file ] then echo \u0026#34;文件可读\u0026#34; else echo \u0026#34;文件不可读\u0026#34; fi if [ -w $file ] then echo \u0026#34;文件可写\u0026#34; else echo \u0026#34;文件不可写\u0026#34; fi if [ -x $file ] then echo \u0026#34;文件可执行\u0026#34; else echo \u0026#34;文件不可执行\u0026#34; fi if [ -f $file ] then echo \u0026#34;文件为普通文件\u0026#34; else echo \u0026#34;文件为特殊文件\u0026#34; fi if [ -d $file ] then echo \u0026#34;文件是个目录\u0026#34; else echo \u0026#34;文件不是个目录\u0026#34; fi if [ -s $file ] then echo \u0026#34;文件不为空\u0026#34; else echo \u0026#34;文件为空\u0026#34; fi if [ -e $file ] then echo \u0026#34;文件存在\u0026#34; else echo \u0026#34;文件不存在\u0026#34; fi echo 打印数据 # ## 显示普通字符串 echo \u0026#34;Hello World\u0026#34; ## 显示转义字符 echo \u0026#34;\\\u0026#34;Hello World\\\u0026#34;\u0026#34; ## 显示变量 name=\u0026#34;zhangsan\u0026#34; echo \u0026#34;$name Hello World\u0026#34; ## 显示换行 echo -e \u0026#34;OK! \\n\u0026#34; echo \u0026#34;Hello World\u0026#34; ## 显示不换行 echo -e \u0026#34;OK! \\c\u0026#34; echo \u0026#34;Hello World\u0026#34; ## 显示结果定向至文件 echo \u0026#34;Hello World\u0026#34; \u0026gt; myfile ## 原样输出字符串 echo \u0026#39;$name\\\u0026#34;\u0026#39; ## 显示命令执行结果，推荐方式 echo $(date) ## 显示命令执行结果 echo `date` test 命令 # Shell 中的 test 命令用于检查某个条件是否成立，它可以进行数值、字符和文件三个方面的测试。\n数字\n字符串\n文件测试\n# 比较 num1=100 num2=100 if test $[num1] -eq $[num2] then echo \u0026#39;两个数相等！\u0026#39; else echo \u0026#39;两个数不相等！\u0026#39; fi Shell 流程控制 # if # if condition1 then command1 elif condition2 then command2 else commandN fi a=10 b=20 if [ $a == $b ] then echo \u0026#34;a 等于 b\u0026#34; elif [ $a -gt $b ] then echo \u0026#34;a 大于 b\u0026#34; elif [ $a -lt $b ] then echo \u0026#34;a 小于 b\u0026#34; else echo \u0026#34;没有符合的条件\u0026#34; fi case # case 语句为多选择语句。可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。 case 值 in 模式1) command1 command2 ... commandN ;; 模式2) command1 command2 ... commandN ;; esac echo \u0026#39;输入 1 到 4 之间的数字:\u0026#39; echo \u0026#39;你输入的数字为:\u0026#39; read num case $num in 1) echo \u0026#39;你选择了 1\u0026#39; ;; 2) echo \u0026#39;你选择了 2\u0026#39; ;; 3) echo \u0026#39;你选择了 3\u0026#39; ;; 4) echo \u0026#39;你选择了 4\u0026#39; ;; *) echo \u0026#39;你没有输入 1 到 4 之间的数字\u0026#39; ;; esac for # 当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。\n命令可为任何有效的shell命令和语句。in列表可以包含替换、字符串和文件名。\nin列表是可选的，如果不用它，for循环使用命令行的位置参数。\nfor var in item1 item2 ... itemN do command1 command2 ... commandN done for loop in 1 2 3 4 5 do echo \u0026#34;The value is: $loop\u0026#34; done for str in \u0026#39;This is a string\u0026#39;,\u0026#39;hello moto\u0026#39; do echo $str done while循环 # while 循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。 while condition do command done # Bash let 命令，它用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量 #!/bin/bash i=1 while(( $i\u0026lt;=5 )) do echo $i let \u0026#34;i++\u0026#34; done # 无限循环 while true do command done break # break 命令允许跳出所有循环（终止执行后面的所有循环）。 #!/bin/bash while : do echo -n \u0026#34;输入 1 到 5 之间的数字:\u0026#34; read aNum case $aNum in 1|2|3|4|5) echo \u0026#34;你输入的数字为 $aNum!\u0026#34; ;; *) echo \u0026#34;你输入的数字不是 1 到 5 之间的! 游戏结束\u0026#34; break ;; esac done continue # continue 命令不会跳出所有循环，仅仅跳出当前循环。 #!/bin/bash while : do echo -n \u0026#34;输入 1 到 5 之间的数字: \u0026#34; read aNum case $aNum in 1|2|3|4|5) echo \u0026#34;你输入的数字为 $aNum!\u0026#34; ;; *) echo \u0026#34;你输入的数字不是 1 到 5 之间的!\u0026#34; continue echo \u0026#34;游戏结束\u0026#34; ;; esac done Shell 函数 # linux shell 可以用户定义函数，然后在shell脚本中可以随便调用。 可以带function fun() 定义，也可以直接fun() 定义,不带任何参数。 参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 #!/bin/bash ## 第一个函数------------------------------ demoFun(){ echo \u0026#34;这是我的第一个 shell 函数!\u0026#34; } echo \u0026#34;-----函数开始执行-----\u0026#34; demoFun echo \u0026#34;-----函数执行完毕-----\u0026#34; ## 函数返回值------------------------------ funWithReturn(){ echo \u0026#34;这个函数会对输入的两个数字进行相加运算...\u0026#34; echo \u0026#34;输入第一个数字: \u0026#34; read aNum echo \u0026#34;输入第二个数字: \u0026#34; read anotherNum echo \u0026#34;两个数字分别为 $aNum 和 $anotherNum !\u0026#34; return $(($aNum+$anotherNum)) } funWithReturn # 函数返回值在调用该函数后通过 $? 来获得。 echo \u0026#34;输入的两个数字之和为 $? !\u0026#34; ## 函数参数------------------------------ funWithParam(){ echo \u0026#34;第一个参数为 $1 !\u0026#34; echo \u0026#34;第二个参数为 $2 !\u0026#34; echo \u0026#34;第十个参数为 $10 !\u0026#34; # 如果参数个数太多，达到或者超过了 10 个，那么就得用 ${n} 的形式来接收了 echo \u0026#34;第十个参数为 ${10} !\u0026#34; echo \u0026#34;第十一个参数为 ${11} !\u0026#34; echo \u0026#34;参数总数有 $# 个!\u0026#34; echo \u0026#34;作为一个字符串输出所有参数 $* !\u0026#34; } funWithParam 1 2 3 4 5 6 7 8 9 Shell 实战 # 添加开机启动项 # 需求：服务器开机后自动与 cn.ntp.org.cn 同步时间。\ntouch /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;#!/bin/bash\u0026#39; \u0026gt;\u0026gt; /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;yum info ntp \u0026amp;\u0026amp; yum -y install ntp \u0026amp;\u0026amp; ntpdate cn.ntp.org.cn\u0026#39; \u0026gt;\u0026gt; /usr/local/scripts/auto_ntpdate.sh chmod +x /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;/usr/local/scripts/auto_ntpdate.sh\u0026#39; \u0026gt;\u0026gt; /etc/rc.d/rc.local chmod +x /etc/rc.d/rc.local 虚拟机初始化脚本 # 目标服务器环境如下：\n主机名 IP node01 192.168.88.101 node02 192.168.88.102 node03 192.168.88.103 首先，使用最初始的example虚拟机克隆出一台完整虚拟机。然后，启动虚拟机并修改网络配置与主机名：\n修改网络配置中的IPADDR并重启网络； echo node01 \u0026gt; /etc/hostname修改主机名。 接下来，sh init.sh运行脚本。\n最后，拍摄快照方便后期回退。\n然后通过已经初始化完成的 node01 完整克隆出 node02 和 node03，修改它两的网络配置与主机名即可。\n虚拟机初始化脚本init.sh完整内容如下：\n#!/bin/bash ## -bash: ./init.sh: /bin/bash^M: bad interpreter: No such file or directory ## vim 或者 vi 的命令模式下，输入命令 set fileformat=unix 即可解决上述问题 echo -e \u0026#34;\\e[1;44m【在 /opt 目录和 /var 目录下创建 yjx 目录，在 /usr/local 目录下创建 scripts 目录】\\e[0m\u0026#34; sleep 2 mkdir -p /opt/yjx /var/yjx /usr/local/scripts echo -e \u0026#34;\\e[1;44m【关闭并禁用 firewalld 防火墙】\\e[0m\u0026#34; sleep 2 systemctl stop firewalld systemctl disable firewalld systemctl status firewalld echo -e \u0026#34;\\e[1;44m【关闭 SELinux】\\e[0m\u0026#34; sleep 2 sed -i \u0026#39;/^SELINUX=/c SELINUX=disabled\u0026#39; /etc/selinux/config echo -e \u0026#34;\\e[1;44m【安装 wget】\\e[0m\u0026#34; sleep 2 yum -y install wget echo -e \u0026#34;\\e[1;44m【修改 yum 源为阿里源】\\e[0m\u0026#34; sleep 2 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum clean all yum makecache echo -e \u0026#34;\\e[1;44m【安装常用依赖】\\e[0m\u0026#34; sleep 2 yum -y install man man-pages telnet perl net-tools openssl-devel ntp lrzsz zip unzip tree vim rsync echo -e \u0026#34;\\e[1;44m【修改时区为 Asia/Shanghai】\\e[0m\u0026#34; sleep 2 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo -e \u0026#34;\\e[1;44m【与中国 NTP 时间服务器 cn.ntp.org.cn 进行时间同步】\\e[0m\u0026#34; sleep 2 yum info ntp \u0026amp;\u0026amp; yum -y install ntp \u0026amp;\u0026amp; ntpdate cn.ntp.org.cn echo -e \u0026#34;\\e[1;44m【修改 hosts 文件，添加集群环境机器 IP 与域名映射】\\e[0m\u0026#34; sleep 2 echo \u0026#34;192.168.88.100 basenode\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo \u0026#34;192.168.88.101 node01\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo \u0026#34;192.168.88.102 node02\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo \u0026#34;192.168.88.103 node03\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo -e \u0026#34;\\e[1;44m【安装 JDK 并设置环境变量】\\e[0m\u0026#34; sleep 2 rpm -ivh jdk-8u351-linux-x64.rpm echo \u0026#39;export JAVA_HOME=/usr/java/jdk1.8.0_351-amd64\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;export PATH=$JAVA_HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; /etc/profile source /etc/profile echo -e \u0026#34;\\e[1;44m【安装 Tomcat】\\e[0m\u0026#34; sleep 2 tar -zxf apache-tomcat-9.0.72.tar.gz -C /opt/yjx/ echo -e \u0026#34;\\e[1;44m【安装 MySQL】\\e[0m\u0026#34; sleep 2 rpm -e --nodeps `rpm -qa | grep mariadb` tar -xvf mysql-8.0.18-1.el7.x86_64.rpm-bundle.tar rpm -ivh mysql-community-common-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-client-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-server-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-devel-8.0.18-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-compat-8.0.18-1.el7.x86_64.rpm systemctl start mysqld systemctl enable mysqld temppasswd=`grep \u0026#34;A temporary password\u0026#34; /var/log/mysqld.log | awk \u0026#39;{print $NF}\u0026#39;` mysql -uroot -p$temppasswd --connect-expired-password \u0026lt;\u0026lt; EOF SET GLOBAL validate_password.policy = low; SET GLOBAL validate_password.length = 6; ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; USE mysql; UPDATE user SET host = \u0026#39;%\u0026#39; WHERE user = \u0026#39;root\u0026#39;; COMMIT; FLUSH PRIVILEGES; EXIT EOF systemctl restart mysqld echo -e \u0026#34;\\e[1;44m【添加时间同步服务至开机启动】\\e[0m\u0026#34; sleep 2 touch /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;#!/bin/bash\u0026#39; \u0026gt;\u0026gt; /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;yum info ntp \u0026amp;\u0026amp; yum -y install ntp \u0026amp;\u0026amp; ntpdate cn.ntp.org.cn\u0026#39; \u0026gt;\u0026gt; /usr/local/scripts/auto_ntpdate.sh chmod +x /usr/local/scripts/auto_ntpdate.sh echo \u0026#39;/usr/local/scripts/auto_ntpdate.sh\u0026#39; \u0026gt;\u0026gt; /etc/rc.d/rc.local chmod +x /etc/rc.d/rc.local echo -e \u0026#34;\\e[1;44m【删除 JDK Tomcat MySQL 安装包和虚拟机初始化脚本】\\e[0m\u0026#34; sleep 2 rm jdk* -rf rm apache-tomcat* -rf rm mysql* -rf rm init.sh -rf echo -e \u0026#34;\\e[1;41m【即将关闭计算机】\\e[0m\u0026#34; sleep 2 shutdown -h now 服务器相互免秘钥 # 生成密钥 # 分别在三台机器上运行以下命令生成密钥对：\nssh-keygen -t rsa -P \u0026#39;\u0026#39; -f ~/.ssh/id_rsa 运行以上命令后会在~/.ssh/目录下生成一对密钥对。\n[root@node01 ~]# ls ~/.ssh/ id_rsa id_rsa.pub known_hosts 取消主机名与 host 校验 # 分别在三台机器上修改/etc/ssh/ssh_config文件的配置，在Host *节点下配置以下信息：\n# 严格的密钥检查 no StrictHostKeyChecking no # 如果不希望生成已知主机列表文件，可以将已知主机列表文件信息写入黑洞（不会再生成 known_hosts 文件） #UserKnownHostsFile /dev/null 这样以后再也不会弹出将该主机添加到当前设备的已知主机列表中的提示信息了。\n如果将已知主机列表文件信息写入了黑洞，那么远程访问时会弹出以下警告：\nWarning: Permanently added \u0026#39;node02,192.168.88.102\u0026#39; (ECDSA) to the list of known hosts. 这个警告不影响任何操作，只是看着比较碍眼。解决办法：在文件夹~/.ssh/下创建config文件，命令如下：\nvim ~/.ssh/config 在新建的文件中写入如下内容：LogLevel=quiet。\n拷贝公钥 # 接下来把自己的公钥互相传递给其他主机，这个公钥文件必须放在对方主机的~/.ssh/authorized_keys文件中。可以使用命令将公钥文件自动传递过去，分别在三台机器运行以下命令：\nssh-copy-id -i ~/.ssh/id_rsa.pub root@node01 ssh-copy-id -i ~/.ssh/id_rsa.pub root@node02 ssh-copy-id -i ~/.ssh/id_rsa.pub root@node03 前面已经通过脚本修改了 hosts 文件，添加了集群环境机器 IP 与域名映射，所以这里可以直接使用主机名。\n传输文件测试是否已免密或者使用 ssh 协议登录对方主机进行测试：\n[root@localhost ~]# scp anaconda-ks.cfg root@node02:~ Warning: Permanently added \u0026#39;node02,192.168.88.102\u0026#39; (ECDSA) to the list of known hosts. anaconda-ks.cfg [root@localhost ~]# ssh root@node02 Warning: Permanently added \u0026#39;node02,192.168.88.102\u0026#39; (ECDSA) to the list of known hosts. Last login: Sat Jun 4 21:07:25 2022 from node01 测试没问题后，shutdown -h now关机，拍摄快照方便后期回退。\n集群启动脚本 # 在/usr/local/bin目录下创建对应服务的脚本：\n[root@node01 ~]# vim /usr/local/bin/tomcat tomcat脚本内容如下：\n#!/bin/bash user=$(whoami) case $1 in \u0026#34;start\u0026#34;) for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i Tomcat 启动 ====================\\e[0m\u0026#34; ssh $user@$i \u0026#34;/opt/yjx/apache-tomcat-9.0.72/bin/startup.sh\u0026#34; done ;; \u0026#34;shutdown\u0026#34;) for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i Tomcat 停止 ====================\\e[0m\u0026#34; ssh $user@$i \u0026#34;/opt/yjx/apache-tomcat-9.0.72/bin/shutdown.sh\u0026#34; done ;; esac 修改脚本权限为用户读写执行rwx，组读执行r-x，其他用户无权限r--：\n[root@node01 ~]# chmod 754 /usr/local/bin/tomcat JPS 脚本 # jps 是 JDK 提供的一个查看当前系统 Java 进程的小工具，全称是 Java Virtual Machine Process Status Tool。\n常用选项如下：\n-q：忽略输出的类名，Jar 名以及传递给 main 方法的参数，只输出 PID -m：输出传递给 main 方法的参数，如果是内嵌的 JVM 则输出为 null -l：输出应用程序主类的完整包名，或者是应用程序 JAR 文件的完整路径 -v：输出 JVM 的参数 -V：输出通过标记文件传递给 JVM 的参数(.hotspotrc 文件，或者通过参数-XX:Flags=\u0026lt;filename\u0026gt;指定的文件) -J：传递 JVM 参数到由 javac 调用的 java 加载器中，例如：-J-Xms512m，把启动内存设置为 512M。使用 -J 选项可以非常方便的向基于 Java 开发的底层虚拟机应用程序传递参数 顺便再创建一个查看所有服务器 JPS 进程的脚本。\n[root@node01 ~]# vim /usr/local/bin/jpsall jpsall脚本内容如下：\n#!/bin/bash # 获取当前用户名称 user=$(whoami) # $#：传递给脚本或函数的参数个数 params_count=$# # 如果没有参数，直接运行 \u0026#34;jps\u0026#34; if [ $params_count -lt 1 ] then for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i ====================\\e[0m\u0026#34; ssh $user@$i jps done exit fi # 如果有参数，运行 \u0026#34;jps -参数\u0026#34; for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i ====================\\e[0m\u0026#34; params=\u0026#34;\u0026#34; for p in $@ do params+=\u0026#34;$p \u0026#34; done ssh $user@$i \u0026#34;jps $params\u0026#34; done 修改脚本权限为用户读写执行rwx，组读执行r-x，其他用户读执行r-x：\n[root@node01 ~]# chmod 755 /usr/local/bin/jpsall 文件分发脚本 # 在/usr/local/bin目录下创建yjxrsync脚本，如下：\n[root@node01 ~]# vim /usr/local/bin/yjxrsync yjxrsync脚本内容如下：\n#!/bin/bash # 获取输入参数的个数 param_count=$# # 如果没有参数，直接退出 if [ $param_count -lt 1 ] then echo -e \u0026#34;\\e[1;31myjxrsync: You must pass in the file name parameter.\\e[0m\u0026#34; exit fi # 获取当前用户名称 user=$(whoami) # 如果有参数，遍历参数（文件或目录名称） for p in $@ do echo -e \u0026#34;\\e[1;34m==================== $p 开始同步 ====================\\e[0m\u0026#34; # basename：显示文件路径名的基本文件名，例如 /opt/bd 会显示 bd；/opt/bd/test.txt 会显示 test.txt file_name=$(basename $p) echo file_name=$file_name # 获取文件的上级目录的绝对路径 # -P：如果切换的目标目录是一个符号链接，则直接切换到符号链接指向的目标目录 parent_dir=`cd -P $(dirname $p); pwd` echo parent_dir=$parent_dir # 循环处理文件 for i in node01 node02 node03 do echo -e \u0026#34;\\e[1;34m==================== $i ====================\\e[0m\u0026#34; rsync -av --delete $parent_dir/$file_name $user@$i:$parent_dir done done 修改脚本权限为用户读写执行rwx，组读执行r-x，其他用户无权限---：\n[root@node01 ~]# chmod 750 /usr/local/bin/yjxrsync ","date":"2024-07-01","externalUrl":null,"permalink":"/docs/shell/","section":"Docs","summary":"Shell 编程概述 # Shell 本身并不是内核的一部分，它只是站在内核的基础上编写的一个应用程序，它和 QQ、迅雷、Firefox 等其它软件没有什么区别。然而 Shell 也有着它的特殊性，就是开机立马启动，并呈现在用户面前；用户通过 Shell 来使用 Linux，不启动 Shell 的话，用户就没办法使用 Linux。","title":"Shell","type":"docs"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs","type":"docs"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/hadoop-mapreduce/","section":"Tags","summary":"","title":"Hadoop MapReduce","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/","section":"KV先生","summary":"","title":"KV先生","type":"page"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/shell/","section":"Tags","summary":"","title":"Shell","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/%E9%87%87%E9%9B%86/","section":"Tags","summary":"","title":"采集","type":"tags"},{"content":"","date":"2024-07-01","externalUrl":null,"permalink":"/tags/%E8%BF%90%E7%BB%B4/","section":"Tags","summary":"","title":"运维","type":"tags"},{"content":" Hadoop_HDFS # Hadoop # Hadoop Common：Hadoop体系最底层的一个模块，是其他模块的基础设施\nHadoop Distributed File System：Hadoop分布式文件系统，是Hadoop的基石，负责存储数据\nHadoop YARN：Hadoop作业调度和资源管理框架。 负责资源调度\nHadoop MapReduce：Hadoop基于YARN的大数据并行处理程序，负责计算\n分布式文件系统架构——HDFS # 全称Hadoop Distributed File System，是一种分布式文件系统，被设计用来存储和处理大规模的数据集。\nFS\tFIle System\n​\t文件系统是一个基于硬盘的文件管理工具，用户操作文件系统可以和硬盘解耦\nDFS\tDistributed File System\n​\t分布式文件系统就是将数据存放在多台电脑上存储，HDFS是Mapreduce计算的基础\n文件切分思想 # ​\t不管文件的大小，所有文件都是由字节数组构成，切分文件就是将一个字节数组分成多分，切分后的数据拼接到一起，数据可以继续使用。根据数据的偏移量重新拼接，偏移量是当前数据在数组中的相对位置，类似下标。\nBlock拆分标准 # 1、拆分的数据块需要等大 数据计算的时候简化问题的复杂度，进行分布式算法设计的时候，数据不统一，算法很难设计；数据拉取的时候时间相对一致；通过偏移量就知道这个块的位置；相同文件，分成的数据块大小应该相等\n2、数据块 Block 数据被切分后的一个整体称之为块；在H1默认大小为64M,在H2及其以后默认大小为128M；同一个文件中，每个数据块大小要一致除了最后一个节点外，不同文件中，块的大小可以不一致，文件大小不同可以设置不同的块的数量；真实情况下，会根据文件大小和集群节点的数量综合考虑块的大小，数据块的个数 =Ceil（ 文件大小 / 每个块的大小）\n3、注意事项 HDFS中一旦文件被存储，数据不允许被修改，修改会影响偏移量，修改会导致数据倾斜，修改数据会导致蝴蝶效益；但是可以被追加，但是不推荐，追加设置需要手动打开；一般HDFS存储的都是历史数据，所以将来Hadoop的mr都用来进行离线数据的处理；块的大小一旦文件上传之后就不允许被修改，128m -512M。\nBlock数据安全 # 对存储数据做备份，备份的数据不能存放在一个节点上，使用数据的时候可以就近获取数据，所以备份的数量要小于等于节点的数量，一般每个数据块会有3个副本，相同副本是不会存放在同一个节点上，副本的数量可以变更，热数据，副本数可以多设置几个，冷数据可以减少副本数。\nHDFS的特点 # 高容错性：HDFS通过数据块复制来实现高容错性。每个数据块都会被复制多份（默认情况下为3份），存储在不同的节点上，这样即使某个节点发生故障，数据仍然可以从其他节点恢复。 大规模数据集的支持：HDFS能够处理PB级别的数据，非常适合海量数据的存储和处理。 流式数据访问：HDFS优化了对大型数据集的流式数据访问，适用于数据处理任务，如MapReduce作业，这些作业通常需要读取或写入整个文件，而不是随机访问。 简单的数据一致性模型：HDFS提供了简单的数据一致性模型，写入操作只在文件结束时完成，这简化了数据的一致性问题。 可扩展性：HDFS可以轻松地在更多的机器上扩展，以增加存储容量和处理能力。 不擅长低延迟访问，不擅长小文件上传分区，不擅长并发写入。 Hadoop基础架构 # 主备 -\u0026gt; 主从 -\u0026gt; ZKFC\nClient客户端 # Client发起文件上传或下载的请求到ANN，ANN会根据DN的资源及机架感知策略返回可用节点。\n直接和DN建立连接完成文件的上传下载。客户端设置文件分隔阈值，文件大小超过阈值时进行切分。\nNameNode（NN） # 功能 # 1、接受客户端的读写服务\rNameNode存放文件和Block的映射关系；NameNode会记录Block和DataNode的映射关系，不会持久化\r2、保存文件的元数据信息\r文件的归属、权限、大小、时间和Block信息，Block的位置信息需要每次开启集群的时候DN上报\r3、收集Block的信息\r系统启动时，DN将自己节点上存储的Block信息汇报给NN，NN接收请求之后重新生成映射关系。如果数据块的副本小于设置数，那么NN会将这个副本拷贝到其他节点。如果客户端需要读取或上传数据的时候，NN可以知道DN的健康情况，可以让客户端读取存活的DN节点。\r3、间隔时间没有心跳，认为DN出现异常\rDN超过10分30秒没有心跳，那么NN会将当前DN存储的数据转存到其他节点。 性能 # NN为了效率，所有的操作都在内存中完成，NN不会和磁盘进行任何数据交互。数据持久化的，掉电丢失。\nDataNode（DN） # 功能 # 1、存放的是文件的数据信息和验证文件完整性的校验信息\n2、汇报，启动时汇报之前先验证Block文件是否损坏，向NN汇报当前DN上的Block的信息。运行中，向NN保持心跳机制，客户可以向DN读写数据\n3、客户端读写数据的时候，先去NN查询file—block—dn的映射关系，然后客户端直接和dn建立连接读写数据\n4、NN和DN是主从关系，DN宕机时，部分文件块无法满足集群默认副本数时，NN会让其他资源充足的节点进行拷贝。\nHadoop-HA架构 # 设计思想 # hadoop2启用了主备节点切换模式；主节点出现异常的时候，集群直接切换备用节点。有独立的线程对主备节点进行健康状态监控；有选举机制去确认主从关系；实时存储日志的中间件。 Active NameNode（ANN） # ​\t为了元数据不丢失，对数据进行持久化落盘存储，为了快速响应客户端，元数据还需要放入内存。块和DN的映射关系只存入内存，因为DN可能宕机，持久化没意义。Client发起文件上传下载请求到ANN，ANN根据DN的资源情况和机架感知策略返回可用节点。Zookeeper主从选择解决ANN单点故障。\n​\t整个集群宕机，重启后恢复数据流程：fsimage文件读入内存 -\u0026gt; QJM中大于fsimage标号的edits（日志）文件读入内存 -\u0026gt; 再将QJM中最大编号的edits_inprogress文件读入内存 -\u0026gt; 等待所有DN心跳 -\u0026gt; 对外服务\nStandby NameNode（SNN） # SNN内存元数据和ANN内存元数据一致，作为备用可以在主宕机时顶替成为新的主。SNN和ANN中的磁盘元数据不一致，只有后ANN才会在磁盘中写入元数据，并实时写入QJM。DN向所有NN汇报心跳内容主要包括资源情况和DN与块的映射。SNN会对元数据进行压缩合并，时间维度、操作维度、维度检查、合并后的文件叫做fsimage（快照），合并后SNN会通知ANN拷贝走，fsimage最多存在近两个版本。\nDataNode（DN） # ​\t文件的Block数据存储在硬盘中，启动时同时向两个NN汇报Block信息，运行时同时和两个NN节点保持心跳机制。\nQuorum JurnaNode Manager（QJM） # 共享存储系统，小型分布式文件系统，最少三台构建为集群环境，自己内部实现了Paxos算法。只接受单个ANN的数据写入。\rANN产生日志文件的时候会同时发送到QJM，QJM通过Paxos算法实现同步。QJM正在生成的文件叫edits_inprogress_xxxxxxxx，已经完成的文件叫做edits_xxxxxx。元数据每隔2分钟生成一次，客户端上传文件时元数据试试写入内存和inprogress文件，2分钟后inprogress文件分割成edits文件。\rJournalNode不要求所有的JN节点都收到日志，只要有半数以上节点收到日志，那么本条日志就生效。SNN每隔一段时间就去QJM上取回最新的日志——SNN上日志可能不是最新，一次只能有一个NN处于活动状态。 Zookeeper Failover Controller（ZKFC） # 故障转移控制器，对NN的主备切换进行总体控制，能及时检测到NN的健康状况，在NN故障时借助ZK实现自动的贮备选举和切换，防止在NN宕机导致心跳受影响，ZKFC作为一个deamon进程从NN分离出来。\r启动时：集群启动时贮备节点的概念很模糊，先到先得，ZKFC只检查到一个节点是健康状态时直接设置，两个节点健康时投票，选出一个主节点一个备节点，并修改主备节点状态\r运行时：ZKFC启动时会创建HealthMonitor和ActiveStandbyElector这两个主要的内部组件，HealthMonitor主要负责检测NameNode的健康状态，ActiveStanDbyElector主要负责完成自动的主备选举，内部封装了zookeeper\r主备节点正常切换：NameNode在选举成功后，ActiveStandbyElector会在zk上创建一个ActiveStandbyElectorLock临时节点，没有选举成功的SNN中的ActiveStandbyElector会监听这个节点。如果ANN对应的HealthMonitor检测到NameNode的异常状态时，ZKFailvoerController会主动删除当前ZK上建立的额临时节点ASEL。如果ANN宕机，会话结束，临时节点ASEL被删除，从而也会自动进行一次主备切换。处于Standby状态的NN的ASE注册的监听器会收到这个节点的NodeDeleted事件，并创建ASEL临时节点，SNN被选举为ANN并切换状态。 Zookeeper # 解决Hadoop集群单点故障的问题，帮助选主以及主备切换，NN向ZK注册临时Znode谁先注册成功谁就是主。ANN宕机后SNN通过监听机制重新注册为ANN，ANN降级为SNN，Zookeeper通过ZKFC调用ANN转换方法，调用失败后ZK会根据配置中的解决方案，例如SSH杀死旧的ANN。\n为主备切换控制器提供主备选举支持，辅助投票，和ZKFC保持心跳机制，确定ZKFC的存活。\n脑裂 brain-split # 定义 脑裂是Hadoop2.X版本后出现的全新问题，实际运行过程中很有可能出现两个namenode同时服务于整个集群的情况，这种情况称之为脑裂。\n原因 脑裂通常发生在主从namenode切换时，由于ActiveNameNode的网络延迟、设备故障等问题，另一个NameNode会认为活跃的NameNode成为失效状态，此时StandbyNameNode会转换成活跃状态，此时集群中将会出现两个活跃的namenode。因此，可能出现的因素有网络延迟、心跳故障、设备故障等。\n脑裂场景 NameNode 可能会出现这种情况，NameNode 在垃圾回收（GC）时，可能会在长时间内整个系统无响应zkfc客户端也就无法向 zk 写入心跳信息，这样的话可能会导致临时节点掉线，备 NameNode 会切换到 Active 状态 这种情况可能会导致整个集群会有同时有两个Active NameNode\n脑裂问题的解决方案是隔离（Fencing）： 1.第三方共享存储：任一时刻，只有一个 NN 可以写入； 2.DataNode：需要保证只有一个 NN 发出与管理数据副本有关的命令； 3.Client需要保证同一时刻只有一个 NN 能够对 Client 的请求发出正确的响应。 (a) 每个NN改变状态的时候，向DN发送自己的状态和一个本次选举对应的序列号。 (b) DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回是认为该NN为新的active。 (c) 如果这时原来的active（比如GC）恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令。 解决方案 ActiveStandbyElector为了实现 fencing，当NN成为ANN之后创建Zookeeper临时节点ActiveStandbyElectorLock，创建ActiveBreadCrumb 的持久节点，这个节点里面保存了这个 Active NameNode的地址信息(node-01)Active NameNode的 ActiveStandbyElector在正常的状态下关闭 Zookeeper Session 的时候，会一起删除这个持久节点但如果 ActiveStandbyElector在异常的状态下关闭，那么由于 /hadoopha/${dfs.nameservices}/ActiveBreadCrumb是持久节点，会一直保留下来，后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController的方法对旧的 Active NameNode 进行 fencing。首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态；如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施。\nsshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死 shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离 在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。 新的主创建临时节点ActiveStandbyElectorLock，创建持久化节点ActiveBreadCrumb ，并将自己的主机地址Node02赋值给初始化节点 Federation 联邦机制 # HDFS Federation就是使得HDFS支持多个命名空间，并且允许在HDFS中同时存在多个Name Node。\n单NN的局限 # 1、Namespace（命名空间）的限制\rNN能存储的对象（文件+块）数目受到NN所在JVM的heap size的限制。Namesapce和Block Management紧密耦合。\r2、性能瓶颈\r整个HDFS文件系统的吞吐量受限于单个NN\r3、隔离问题\rHDFS上的一个实现程序可能影响整个HDFS上的运行程序\r4、集群可用性\rNN的宕机会导致整个HDFS集群宕机。\r5、纵向扩展目前的NN不可行\rNN的Heap空间扩大后启动时间太长；NN在Full GC的时候发生错误集群会宕机。 Federation # 联邦机制是为了解决NN压力的问题，防止NN出现OOM，底层DN公用，上层NN联邦，DN磁盘存储格式是按照NN的BlockPoolID构建目录分别存储，心跳也是只汇报对应的块池文件块信息。\nHadoop-HA 高可用集群环境搭建 # 目标环境 # 安装 # 解压即安装 # 将准备好的安装包上传至 node01，然后解压\n[root@node01 ~]# tar -zxvf hadoop-3.3.4.tar.gz -C /opt/yjx/\r[root@node01 ~]# rm hadoop-3.3.4.tar.gz -rf 修改配置文件 # 修改环境配置文件 hadoop-env.sh ：\n[root@node01 ~]# cd /opt/yjx/hadoop-3.3.4/etc/hadoop/\r[root@node01 hadoop]# vim hadoop-env.sh 在文件末尾添加以下内容：\nexport JAVA_HOME=/usr/java/jdk1.8.0_351-amd64\rexport HDFS_NAMENODE_USER=root\rexport HDFS_DATANODE_USER=root\rexport HDFS_ZKFC_USER=root\rexport HDFS_JOURNALNODE_USER=root\rexport YARN_RESOURCEMANAGER_USER=root\rexport YARN_NODEMANAGER_USER=root 修改核心配置文件 core-site.xml ：\n[root@node01 hadoop]# vim core-site.xml 在 configuration 节点中添加以下内容：\n​\thadoop.tmp.dir 属性的默认值为 /tmp/hadoop-${user.name} ，NameNode 会将 HDFS 的元数据存储在这个 /tmp 目录下，如果操作系统重启了，系统会清空 /tmp，导致 NameNode 元数据丢失，是个非常严重的问题，所以我们需要修改此路径。 ​\thadoop.http.staticuser.user=dr.who 默认值是一个不真实存在的用户，此用户权限非常小，不能访问不同用户的数据，但是这保证了数据的安全。也可以设置为 hdfs 和 hadoop 等具有较高权限的用户，但会导致能够登陆网页界面的人会看到其它用户的数据。实际设置请综合考虑，如无特殊需求，使用默认值就好。\n\u0026lt;!-- 设置 NameNode 节点的 URI(包括协议、主机名称、端口号)，用于 NameNode 与 DataNode 之间的通讯 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;hdfs://hdfs-yjx\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 Hadoop 运行时临时文件的存放位置，比如 HDFS 的 NameNode 数据默认都存放这个目录下 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/var/yjx/hadoop/ha\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置在 Web 界面访问数据时使用的用户名 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hadoop.http.staticuser.user\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;root\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 HA，需要一组 ZK 地址，以逗号分隔。被 ZKFailoverController 使用于自动失效备援 failover --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;ha.zookeeper.quorum\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01:2181,node02:2181,node03:2181\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 该参数表示可以通过 httpfs 接口访问 HDFS 的 IP 地址限制 --\u0026gt;\r\u0026lt;!-- 配置 root(超级用户) 允许通过 httpfs 方式访问 HDFS 的主机名、域名 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hadoop.proxyuser.root.hosts\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 通过 httpfs 接口访问的用户获得的群组身份 --\u0026gt;\r\u0026lt;!-- 配置允许通过 httpfs 方式访问的客户端的用户组 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hadoop.proxyuser.root.groups\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt; 修改 HDFS 配置文件 hdfs-site.xml ：\n[root@node01 hadoop]# vim hdfs-site.xml 在 configuration 节点中添加以下内容：\n\u0026lt;!-- 设置 nameservices 列表（联邦列表），逗号分隔 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.nameservices\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;hdfs-yjx\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置一个 NameNode 列表。hdfs-yjx 是指具体的 nameservice 名称，通常就是 dfs.nameservices 中配置的。值\r是预备配置的 NameNode 的 ID，ID 是自己取的，不重复就可以，例如 nn1,nn2 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.ha.namenodes.hdfs-yjx\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;nn1,nn2\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 NameNode 的 RPC 地址和端口 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.namenode.rpc-address.hdfs-yjx.nn1\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01:8020\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 NameNode 的 RPC 地址和端口 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.namenode.rpc-address.hdfs-yjx.nn2\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node02:8020\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 NameNode 的 HTTP 地址和端口 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.namenode.http-address.hdfs-yjx.nn1\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01:9870\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 NameNode 的 HTTP 地址和端口 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.namenode.http-address.hdfs-yjx.nn2\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node02:9870\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 QJM 共享存储系统服务器。在多个 NameNode 中共享存储目录，用于存放 edits 文件，该目录由 Active 写，\rStandby 读 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.namenode.shared.edits.dir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;qjournal://node01:8485;node02:8485;node03:8485/hdfs-yjx\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 journalnode 用于存放 edits 日志的目录，默认值为 /tmp/hadoop/dfs/journalnode --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.journalnode.edits.dir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/var/yjx/hadoop/ha/qjm\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置客户端连接 Active NameNode 所用的代理类 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.client.failover.proxy.provider.hdfs-yjx\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 HDFS-HA 功能的防脑裂方法。可以是内建的方法(例如 shell 和 sshfence)或者用户自定义的方法 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.ha.fencing.methods\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;sshfence\u0026lt;/value\u0026gt;\r\u0026lt;value\u0026gt;shell(true)\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置失效转移时使用的秘钥文件 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.ha.fencing.ssh.private-key-files\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/root/.ssh/id_rsa\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置故障转移功能是否开启，建议开启 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.ha.automatic-failover.enabled\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 设置 HDFS 默认的数据块副本数。可以在创建文件时指定，如果创建时未指定，则使用默认值 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;2\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt; 修改 workers ：\n[root@node01 hadoop]# vim workers 用以下内容替换文件内容：\nnode01\rnode02\rnode03 拷贝至其他节点 # 将 node01 已配置好的 hadoop 拷贝至 node02 和 node03。\n[root@node02 ~]# scp -r root@node01:/opt/yjx/hadoop-3.3.4 /opt/yjx/\r[root@node03 ~]# scp -r root@node01:/opt/yjx/hadoop-3.3.4 /opt/yjx/\r# 或者使用分发脚本\r[root@node01 ~]# yjxrsync /opt/yjx/hadoop-3.3.4/ 修改环境变量 # 三个节点修改环境变量 vim /etc/profile ，在文件末尾添加以下内容：\nexport HADOOP_HOME=/opt/yjx/hadoop-3.3.4\rexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 修改完成后 source /etc/profile 重新加载环境变量。\n启动 # 首先启动 ZooKeeper（三台机器都需要执行）。\nzkServer.sh start\rzkServer.sh status 然后启动 JournalNode（三台机器都需要执行）。\nhdfs --daemon start journalnode 最后格式化 NameNode 等相关服务并启动集群。\n# 格式化 node01 的 namenode（第一次配置的情况下使用）\r[root@node01 ~]# hdfs namenode -format\r# 启动 node01 的 namenode\r[root@node01 ~]# hdfs --daemon start namenode\r# node02 节点同步镜像数据\r[root@node02 ~]# hdfs namenode -bootstrapStandby\r# 格式化 zkfc（第一次配置的情况下使用）\r[root@node01 ~]# hdfs zkfc -formatZK\r# 启动 HDFS\r[root@node01 ~]# start-dfs.sh 后期只需要先启动 ZooKeeper 然后再启动 HDFS 即可。\nzkServer.sh start\rzkServer.sh status\r[root@node01 ~]# start-dfs.sh 访问 # 访问：http://192.168.100.101:9870/ 和 http://192.168.100.102:9870/\n测试文件上传 # [root@node01 ~]# hdfs dfs -mkdir -p /test\r[root@node01 ~]# hdfs dfs -put hadoop-3.3.4.tar.gz /test\r# 指定 Block 大小\r[root@node01 ~]# hdfs dfs -D dfs.blocksize=134217728 -put hadoop-3.3.4.tar.gz /test 关闭 # 先关闭 HDFS。\n[root@node01 ~]# stop-dfs.sh 再关闭 ZooKeeper（三台机器都需要执行）。\nzkServer.sh stop 环境搭建成功后 shutdown -h now 关机拍摄快照。\nHadoop 集群命令 # 命令分类 # hadoop fs: FS relates to a generic file system which can point to any file systems like local, HDFS etc. So this can be used when youare dealing with different file systems such as Local FS, HFTP FS, S3 FS, and others 该命令可以用于其他文件系统，不止是hdfs文件系统内，也就是说该命令的使用范围更广 hadoop dfs 专门针对hdfs分布式文件系统 hdfs dfs（推荐） 和上面的命令作用相同，相比于上面的命令更为推荐，并且当使用hadoop dfs时内部会被转为hdfs dfs命令\nHadoop FS命令 # 介绍 调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式。所有的的FS shell命令使用URI路径作为参数。URI格式是scheme://authority/path。对HDFS文件系统，scheme是hdfs，对本地文件系统，scheme是file。 其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。 一个HDFS文件或目录比如/parent/child可以表示成hdfs://namenode:namenodeport/parent/child，或者更简单 的/parent/child（假设你配置文件中的默认值是namenode:namenodeport）。 大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信 息会输出到stderr，其他信息输出到stdout。\n网址\nhttp://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html\n1 hadoop fs -ls \u0026lt;path\u0026gt;\r列出指定目录下的内容，支持pattern匹配。输出格式如filename（full path）\u0026lt;r n\u0026gt;size.n代表备份数。\r2 hadoop fs -lsr \u0026lt;path\u0026gt;\r递归列出该路径下所有子目录信息\r3 hadoop fs -du\u0026lt;path\u0026gt;\r显示目录中所有文件大小，或者指定一个文件时，显示此文件大小\r4 hadoop fs -dus\u0026lt;path\u0026gt;\r显示文件大小 相当于 linux的du -sb s代表显示只显示总计，列出最后的和 b代表显示文件大小时以byte为单位\r5 hadoop fs -mv \u0026lt;src\u0026gt; \u0026lt;dst\u0026gt;\r将目标文件移动到指定路径下，当src为多个文件，dst必须为目录\r6 hadoop fs -cp \u0026lt;src\u0026gt; \u0026lt;dst\u0026gt;\r拷贝文件到目标位置，src为多个文件时，dst必须是个目录\r7 hadoop fs -rm [skipTrash] \u0026lt;src\u0026gt;\r删除匹配pattern的指定文件\r8 hadoop fs -rmr [skipTrash] \u0026lt;src\u0026gt;\r递归删除文件目录及文件\r9 hadoop fs -rmi [skipTrash] \u0026lt;src\u0026gt;\r为了避免误删数据，加了一个确认\r10 hadoop fs -put \u0026lt;\u0026gt; ... \u0026lt;dst\u0026gt;\r从本地系统拷贝到dfs中\r11 hadoop fs -copyFromLocal\u0026lt;localsrc\u0026gt;...\u0026lt;dst\u0026gt;\r从本地系统拷贝到dfs中,与-put一样\r12 hadoop fs -moveFromLocal \u0026lt;localsrc\u0026gt;...\u0026lt;dst\u0026gt;\r从本地系统拷贝文件到dfs中，拷贝完删除源文件\r13 hadoop fs -get [-ignoreCrc] [-crc] \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;\r从dfs中拷贝文件到本地系统，文件匹配pattern，若是多个文件，dst必须是个目录\r14 hadoop fs -getmerge \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;\r从dfs中拷贝多个文件合并排序为一个文件到本地文件系统\r15 hadoop fs -cat \u0026lt;src\u0026gt;\r输出文件内容\r16 hadoop fs -copyTolocal [-ignoreCre] [-crc] \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;\r与 -get一致\r17 hadoop fs -mkdir \u0026lt;path\u0026gt;\r在指定位置创建目录\r18 hadoop fs -setrep [-R] [-w] \u0026lt;rep\u0026gt; \u0026lt;path/file\u0026gt;\r设置文件的备份级别，-R标志控制是否递归设置子目录及文件\r19 hadoop fs -chmod [-R] \u0026lt;MODE[,MODE]...|OCTALMODE\u0026gt;PATH\r修改文件权限， -R递归修改 mode为a+r,g-w,+rwx ,octalmode为755\r20 hadoop fs -chown [-R] [OWNER][:[GROUP]] PATH\r递归修改文件所有者和组\r21 hadoop fs -count[q] \u0026lt;path\u0026gt;\r统计文件个数及占空间情况，输出表格列的含义分别为：DIR_COUNT.FILE_COUNT.CONTENT_SIZE.FILE_NAME，如果加-q 的话，还会列出QUOTA,REMAINING_QUOTA,REMAINING_SPACE_QUOTA hdfs dfs命令 # -mkdir 创建目录　hdfs dfs -mkdir [-p] \u0026lt; paths\u0026gt;\r-ls　查看目录下内容，包括文件名，权限，所有者，大小和修改时间　hdfs dfs -ls [-R] \u0026lt; args\u0026gt;\r-put　将本地文件或目录上传到HDFS中的路径　hdfs dfs -put \u0026lt; localsrc\u0026gt; … \u0026lt; dst\u0026gt;\r-get　将文件或目录从HDFS中的路径拷贝到本地文件路径　hdfs dfs -get [-ignoreCrc] [-crc] \u0026lt; src\u0026gt; \u0026lt;localdst\u0026gt;　选项：-ignorecrc选项复制CRC校验失败的文件。-crc选项复制文件和CRC。\r-du　显示给定目录中包含的文件和目录的大小或文件的长度，用字节大小表示。　hdfs dfs -du [-s] [-h] URI [URI…]　选项：-s选项将显示文件长度的汇总摘要，而不是单个文件。-h选项将以“人可读”的方式格式化文件大小（例如64.0m而不是67108864）；第一列标示该目录下总文件大小，第二列标示该目录下所有文件在集群上的总存储大小和你的副本数相关（第二列内容=文件大小*副本数），第三列标示你查询的目录\r-dus　显示文件长度的摘要　hdfs dfs -dus \u0026lt; args\u0026gt;　注意：不推荐使用此命令。而是使用hdfs dfs -du -s。\r-mv　在HDFS文件系统中，将文件或目录从HDFS的源路径移动到目标路径。不允许跨文件系统移动文件。\r-cp　在HDFS文件系统中，将文件或目录复制到目标路径下　hdfs dfs -cp [-f] [-p | -p [topax] ] URI [ URI …]\u0026lt; dest\u0026gt;　选项：-f选项覆盖已经存在的目标。-p选项将保留文件属性[topx]（时间戳，所有权，权限，ACL，XAttr）。如果指定了-p且没有arg，则保留时间戳，所有权和权限。如果指定了-pa，则还保留权限，因为ACL是一组超级权限。确定是否保留原始命名空间扩展属性与-p标志无关。\r-copyFromLocal　从本地复制文件到hdfs文件系统（与-put命令相似）hdfs dfs -copyFromLocal \u0026lt;localsrc\u0026gt; URI　选项：如果目标已存在，则-f选项将覆盖目标。\r-copyToLocal　复制hdfs文件系统中的文件到本地 （与-get命令相似）　hdfs dfs -copyToLocal [-ignorecrc] [-crc] URI \u0026lt; localdst\u0026gt;　-rm　删除一个文件或目录　hdfs dfs -rm [-f] [-r|-R] [-skipTrash] URI [URI …]　选项：如果文件不存在，-f选项将不显示诊断消息或修改退出状态以反映错误。-R选项以递归方式删除目录及其下的任何内容。-r选项等效于-R。-skipTrash选项将绕过垃圾桶（如果已启用），并立即删除指定的文件。当需要从超配额目录中删除文件时，这非常有用。\r-cat　显示文件内容到标准输出上。　hdfs dfs -cat URI [URI …]\r-text　获取源文件并以文本格式输出文件。允许的格式为zip和TextRecordInputStream。　hdfs dfs -text\r-touchz　创建一个零长度的文件。　hdfs dfs -touchz URI [URI …]\r-stat　显示文件所占块数(%b)，文件名(%n)，块大小(%n)，复制数(%r)，修改时间(%y%Y)　hdfs dfs -stat URI\r[URI …]\r-tail　显示文件的最后1kb内容到标准输出　hdfs dfs -tail [-f] URI　选项：　-f选项将在文件增长时输出附加数据，如在Unix中一样。\r-count　统计与指定文件模式匹配的路径下的目录，文件和字节数　hdfs dfs -count [-q] [-h] \u0026lt; paths\u0026gt;\r-getmerge　将源目录和目标文件作为输入，并将src中的文件连接到目标本地文件（把两个文件的内容合并起来）　hdfsdfs -getmerge \u0026lt; src\u0026gt; \u0026lt; localdst\u0026gt; [addnl] 注：合并后的文件位于当前目录，不在hdfs中，是本地文件\r-grep　从hdfs上过滤包含某个字符的行内容　hdfs dfs -cat \u0026lt; srcpath\u0026gt; | grep 过滤字段\r-chown　hdfs上文件权限修改 hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]#修改文件的所有者 例如：hdfs dfs -chown -R Administrator:Administrator /user/ -distcp　最常用在集群之间的拷贝：hadoop distcp hdfs://master1:8020/foo/bar hdfs://master2:8020/bar/foo Java访问Hadoop集群 # 环境变量 # Windows 系统的环境变量配置： 解压 hadoop-3.3.4.zip，将解压后的文件夹存放到自己软件目录，例如：D:\\hadoop\\hadoop-3.3.4 将 D:\\hadoop\\hadoop-3.3.4\\bin 目录下的 winutils.exe 和 hadoop.dll 文件拷贝到 C:\\Windows\\System32 目录下 将 Hadoop 添加到环境变量 HADOOP_HOME \u0026ndash;\u0026gt; D:\\hadoop\\hadoop-3.3.4 HADOOP_USER_NAME \u0026ndash;\u0026gt; root Path \u0026ndash;\u0026gt; %HADOOP_HOME%\\bin;%HADOOP_HOME%\\sbin; JDK 的版本也设置的和服务器版本一致 修改当前 Window 的 hosts（C:\\Windows\\System32\\drivers\\etc\\hosts）文件，添加以下内容：\n192.168.100.101 node01\r192.168.100.102 node02\r192.168.100.103 node03 Java 访问 Hadoop # 打开 IDEA，创建一个普通的Maven项目。拷贝 Hadoop 的以下配置文件至项目 resources 目录：\ncore-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml log4j.properties\npom.xml 文件如下：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\r\u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34;\rxmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\rxsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-\r4.0.0.xsd\u0026#34;\u0026gt;\r\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;\r\u0026lt;groupId\u0026gt;com.yjxxt\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-demo\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt;\r\u0026lt;properties\u0026gt;\r\u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt;\r\u0026lt;maven.compiler.source\u0026gt;8\u0026lt;/maven.compiler.source\u0026gt;\r\u0026lt;maven.compiler.target\u0026gt;8\u0026lt;/maven.compiler.target\u0026gt;\r\u0026lt;!-- Hadoop 版本 --\u0026gt;\r\u0026lt;hadoop.version\u0026gt;3.3.4\u0026lt;/hadoop.version\u0026gt;\r\u0026lt;!-- commons-io 版本 --\u0026gt;\r\u0026lt;commons-io.version\u0026gt;2.11.0\u0026lt;/commons-io.version\u0026gt;\r\u0026lt;/properties\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-common\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-hdfs\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-mapreduce-client-common\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-mapreduce-client-core\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-mapreduce-client-jobclient\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;commons-io\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;commons-io\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${commons-io.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.janeluo\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;ikanalyzer\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2012_u6\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;junit-jupiter-api\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;5.9.2\u0026lt;/version\u0026gt;\r\u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;/project\u0026gt; HDFSTest.java 代码如下：\npackage com.yjxxt.hdfs;\rimport org.apache.hadoop.conf.Configuration;\rimport org.apache.hadoop.fs.FileSystem;\rimport org.apache.hadoop.fs.Path;\rimport org.junit.jupiter.api.BeforeEach;\rimport org.junit.jupiter.api.DisplayName;\rimport org.junit.jupiter.api.Test;\rimport java.io.IOException;\r@DisplayName(\u0026#34;HDFS 测试类\u0026#34;)\rpublic class HDFSTest {\rFileSystem fileSystem = null;\r@BeforeEach\rpublic void hdfsInit() throws IOException {\r// 加载配置文件\rConfiguration configuration = new Configuration(true);\r// 获取文件系统\rfileSystem = FileSystem.get(configuration);\r}\r@DisplayName(\u0026#34;文件上传与下载\u0026#34;)\r@Test\rpublic void testFileUploadAndDownload() throws IOException {\r// 文件上传\rPath srcPath = new Path(\u0026#34;D:\\\\_组件资源包\\\\后端\\\\JDK\\\\JDK8\\\\jdk-8u351-linux-x64.tar.gz\u0026#34;);\rPath destPath = new Path(\u0026#34;/jdk/jdk-8u351-linux-x64.tar.gz\u0026#34;);\rfileSystem.copyFromLocalFile(srcPath, destPath);\r// 文件下载\r// Path localPath = new Path(\u0026#34;E:\\\\test\u0026#34;);\r// Path hdfsPath = new Path(\u0026#34;/jdk/jdk-8u351-linux-x64.tar.gz\u0026#34;);\r// fileSystem.copyToLocalFile(hdfsPath, localPath);\r}\r} Hadoop3.x 其他特性 # Erasure Encoding # 简介 # HDFS默认情况下，Block的备份系数是3，一个原始数据块和其他2个副本。 其中2个副本所需要的存储开销各站100%，这样使得200%的存储开销 正常操作中很少访问具有低IO活动的冷数据集的副本，但是仍然消耗与原始数据集相同的资源量。\nEC技术 # EC(擦除编码)和HDFS的整合可以保持与提供存储效率相同的容错。 HDFS:一个副本系数为3，要复制文件的6个块，需要消耗6*3=18个块的磁盘空间 EC:6个数据块，3个奇偶校验块 擦除编码需要在执行远程读取时，对数据重建带来额外的开销，因此他通常用于存储不太频繁访问的数据\nNameNode # 在Hadoop3中允许用户运行多个备用的NameNode。例如，通过配置三个NameNode（1个Active NameNode和2个Standby NameNode）和5个JournalNodes节点，此时集群可以容忍2个NameNode节点故障。\n服务器端口 # 分类 应用 Hadoop 2.x port Hadoop 3 port NNPorts Namenode 8020 9820 NNPorts NN HTTP UI 50070 9870 NNPorts NN HTTPS UI 50470 9871 SNN ports SNN HTTP 50091 9869 SNN ports SNN HTTP UI 50090 9866 DN ports DN IPC 50020 9867 DN ports DN 50010 9866 DN ports DN HTTP UI 50075 9864 DN ports Namenode 50475 9865 DataNode # ​\t单个数据节点配置多个数据磁盘，在正常写入操作期间，数据被均匀的划分，因此，磁盘被均匀填充。在维护磁盘时，添加或者替换磁盘会导致DataNode节点存储出现偏移。这种情况在早期的HDFS文件系统中，是没有被处理的。\n​\tHadoop3通过新的内部DataNode平衡功能来处理这种情况，这是通过hdfs diskbalancer CLI来进行调用的。执行之后，DataNode会进行均衡处理。\nJDK # Hadoop3中，最低版本要求是JDK8，所以低于JDK8的版本需要对JDK进行升级，方可安装使用Hadoop3\n附录：配置文件详解 # core-site.xml # core-site.xml配置文件的作用：\r用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置。\rfs.defaultFS 参数的作用：\r#声明namenode的地址，相当于声明hdfs文件系统。我们可以指定某个ip地址，在ha模式中我们通常指定hdfs集群的逻辑名称\rhadoop.tmp.dir 参数的作用：\r#声明hadoop工作目录的地址。\rhadoop.http.staticuser.user 参数的作用：\r#在网页界面访问数据使用的用户名。默认值是一个不真实存在的用户，此用户权限很小，不能访问不同用户的数据。这保证了数据安全。也可以设置为hdfs和hadoop等具有较高权限的用户，但会导致能够登陆网页界面的人能看到其它用户数据。实际设置请综合考虑。如无特殊需求。使用默认值就好.\rha.zookeeper.quorum 参数的作用：\r#指定zookeeper集群的地址，让zookeeper帮我们管理hdfs集群。 hdfs-site.xml # hdfs-site.xml 配置文件的作用：\r#HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置.\rdfs.replication 参数的作用：\r#为了数据可用性及冗余的目的，HDFS会在多个节点上保存同一个数据块的多个副本，其默认为3个。而只有一个节点的伪分布式环境中其仅用保存一个副本即可，这可以通过dfs.replication属性进行定义。它是一个软件级备份。\rdfs.nameservices 参数的作用：\r#指定一个逻辑名称，该名称会自动去帮我们找打真实NameNode节点，这和咱们的keepalive的VIP有点类似！这个名称咱们可以自定义！但是我们正在引用时千万别引用错了！\rdfs.ha.namenodes.yinzhengjie-hdfs 参数的作用：\r#看到没，上面我们定义了dfs.nameservices的名称，现在我们就用到了，我们把定义的逻辑名称指向了各个namenode的别名\rdfs.namenode.rpc-address.yinzhengjie-hdfs.namenode1 参数的作用：\r#定义远程主机调用的服务器地址。\rdfs.namenode.http-address.yinzhengjie-hdfs.namenode1 参数的作用：\r#上面我们定义了集群的逻辑名称，也定义节点的逻辑名称，但是真正的ip地址我们始终没有指定，因此我们这一步骤就是指定dfs.namenode.rpc-address.yinzhengjie-hdfs.namenode1对应的IP地址。当然你写主机名称也是可以的，只不过嘛你得在/etc/hosts文件做映射或者配置DNS中有对应的A记录哟！\rdfs.namenode.shared.edits.dir 参数的作用：\r#配置JournalNode集群的地址。\rdfs.client.failover.proxy.provider.yinzhengjie-hdfs 参数的作用：\r#配置故障转移的代理类，这是HDFS客户端找到active NameNode类名的一个代理类，如果你不配置这个项的话，那么客户端连接active namenode地址时会报错！实际上故障转移就是通过这个代理类来实现的，我们只需要默认官网的配置即可！不需要手动修改！\rdfs.ha.fencing.methods 参数的作用：\r#指定ha出现故障时的隔离方法！\rdfs.ha.fencing.ssh.private-key-files 参数的作用：\r#指定隔离主机的私钥PATH。\rdfs.journalnode.edits.dir 参数的作用：\r#指定JN节点存放编辑日志的目录。\rdfs.ha.automatic-failover.enabled 参数的作用：\r#开启自动故障转移功能.\r– dfs.name.dir\r– NameNode 元数据存放位置\r– 默认值：使用core-site.xml中的hadoop.tmp.dir/dfs/name\r– dfs.block.size\r– 对于新文件切分的大小，单位byte。默认是64M,建议是128M。每一个节点都要指定，包括客户端。\r– 默认值：67108864\r– dfs.data.dir\r– DataNode在本地磁盘存放block的位置，可以是以逗号分隔的目录列表，DataNode循环向磁盘中写入数据，每个DataNode\r可单独指定与其它DataNode不一样\r– 默认值：${hadoop.tmp.dir}/dfs/data\r– dfs.namenode.handler.count\r– NameNode用来处理来自DataNode的RPC请求的线程数量\r– 建议设置为DataNode数量的10%，一般在10~200个之间\r– 如设置太小，DataNode在传输数据的时候日志中会报告“connecton refused\u0026#34;信息\r– 在NameNode上设定\r– 默认值：10\r– dfs.datanode.handler.count\r– DataNode用来连接NameNode的RPC请求的线程数量\r– 取决于系统的繁忙程度\r– 设置太小会导致性能下降甚至报错\r– 在DataNode上设定\r– 默认值：3\r– dfs.datanode.max.xcievers\r– DataNode可以同时处理的数据传输连接数\r– 默认值：256\r– 建议值：4096\r– dfs.permissions\r– 如果是true则检查权限，否则不检查（每一个人都可以存取文件）\r– 于NameNode上设定\r– 默认值：true\r– dfs.datanode.du.reserved\r– 在每个卷上面HDFS不能使用的空间大小\r– 在每个DataNode上面设定\r– 默认值：0\r– 建议为10737418240，即10G。需要结合MapReduce场景设置。\r– dfs.datanode.failed.volumes.tolerated\r– DataNode可以容忍损块的磁盘数量，超过这个数量DataNode将会离线，所有在这个节点上面的block将会被重新复制\r– 默认是0，但是在有多块磁盘的时候一般会增大这个值\r– dfs.replication\r– 在文件被写入的时候，每一块将要被复制多少份\r– 默认是3份。建议3份\r– 在客户端上设定\r通常也需要在DataNode上设定\r2、HDFS core-site.xml 参数配置\r– fs.default.name\r– 文件系统的名字。通常是NameNode的hostname与port\r– 需要在每一个需要访问集群的机器上指定，包括集群中的节点\r– 例如：hdfs://\u0026lt;your_namenode\u0026gt;:9000/\r– hadoop.tmp.dir\r– HDFS与本地磁盘的临时文件\r默认是/tmp/hadoop-${user.name}.需要在所有的节点中设定\r– fs.trash.interval\r– 当一个文件被删掉后，它会被放到用户目录的.Trash目录下，而不是立即删掉\r– 经过此参数设置的分钟数之后，再删掉数据\r– 默认是0，禁用此功能，建议1440（一天）\r– io.file.buffer.size\r– 设定在读写数据时的缓存大小，应该为硬件分页大小的2倍\r– 默认是4096，建议为65536 （ 64K） ","externalUrl":null,"permalink":"/docs/hadoop_hdfs/hadoop_hdfs/","section":"Docs","summary":"Hadoop_HDFS # Hadoop # Hadoop Common：Hadoop体系最底层的一个模块，是其他模块的基础设施","title":"","type":"docs"},{"content":" HBase # 基本概念 # 列族是什么，扩展是什么，关系型数据库，\n简介 # Apache HBase（Hadoop DataBase）是一个开源的、高可靠性、高性能、面向列（这里指列族，非列式存储）、可伸缩、实时读写的分布式数据库，其设计思想来源于 Google 的 BigTable 论文。利用 Hadoop HDFS 作为其文件存储系统，利用 ZooKeeper 作为其分布式协同服务。主要用来存储非结构化和半结构化的松散数据（列式存储 NoSQL 数据库）。\rHBase良好的分布式架构设计为海量数据的快速存储、随机访问提供了可能，基于数据副本机制和分区机制可以轻松实现在线扩容、缩容和数据容灾，是大数据领域中K-V数据结构存储最常用的数据方案\r注意：HBase是列组数据库（Column-Family Database），不是列式数据库（Column-Oriented Database）\r总结：HBase是运行在HDFS之上的面向列族的数据库管理系统。 特点 # 易扩展 # HBase 的扩展性主要体现在两个方面，一个是基于运算能力（HRegionServer） 的扩展，通过增加 HRegionSever 节点的数量，提升 HBase 上层的处理能力；另一个是基于存储能力的扩展（HDFS），通过增加 DataNode 节点数量对存储层的进行扩容，提升 HBase 的数据存储能力。 内容大 # HBase 单表可以有十亿行、百万列，数据矩阵横向和纵向两个维度所支持的数据量级都非常具有弹性。HBase 的主要作用就是面向 PB 级别数据的实时入库和快速随机访问。这主要源于上述易扩展的特点，使得 HBase 通过扩展来存储海量的数据 面向列 # HBase 是根据列族来存储数据的，列族下面可以有非常多的列。列式存储的最大好处就是，其数据在表中是按照某列存储的，这样在查询只需要少数几个字段时，能大大减少读取的数据量。还可以动态增加列，可单独对列进行各方面的操\r作。 多版本 # HBase 的每个列的数据存储支持多个 Version，比如住址列，可能有多个变更。 稀疏性 # 为空的列并不占用存储空间，表可以设计的非常稀疏。不必像关系型数据库那样需要预先知道所有列名然后再进行\rnull 填充。 高可靠 # WAL（Write Ahead Log）日志先行机制，保证数据写入的时候不会因为集群异常而导致写入数据丢失。Replication 机制，保证了集群在出现严重问题的时候，数据不会发生丢失或者损坏。HBase 底层使用 HDFS，本身也有备份。 高性能 # HBase 底层采用 LSM 树（Log-Structured Merge-Tree）数据结构进行存储，底层的 LSM 树数据结构和 RowKey 有序排列等架构上的独特设计，使得 HBase 写入性能非常高。HRegion 切分、主键索引、缓存机制使得 HBase 在海量数据下具备一定的随机读取性能，该性能针对 RowKey 的查询能够到达毫秒级别。LSM 树属于树形结构，最末端的子节点是以内存的方式进行存储的，内存中的小树会 Flush 到磁盘中（当子节点达到一定阈值以后，会放到磁盘中，且存入的过程会进行实时 Merge 成一个主节点，然后磁盘中的树定期会做 Merge 操作，合并成一棵大树，以优化读性能）。 应用 # HBase 是一种 NoSQL 数据库，这意味着它不像传统的 RDBMS 数据库那样支持 SQL 作为查询语言。HBase 是一种分布式存储的数据库，技术上来讲，它更像是分布式存储而不是分布式数据库，它缺少很多 RDBMS 系统的特性，比如列类型，辅助索引，触发器和高级查询语言等。\rHBase 不适合解决所有的问题，首先数据量要足够大，如果有十亿或百亿行数据，那么 HBase 是一个很好的选择，如果只有几百万行甚至不到的数据量，RDBMS 是一个很好的选择。因为数据量小的话，真正能工作的机器量少，剩余的机器都处于空闲的状态。其次，如果你不需要辅助索引，静态类型的列，事务等特性可以考虑 HBase。但是一个已经用 RDBMS的系统想要切换到 HBase，则需要重新设计系统。最后，保证硬件资源足够，每个 HDFS 集群在少于 5 个节点的时候，都不能表现的很好。因为 HDFS 默认的复制数量是 3，再加上一个 NameNode。其实 HBase 在单机环境下也能运行，但是请在开发环境中进行使用。\r适合 HBase 的应用：\r存储业务数据：车辆 GPS 信息，司机点位信息，用户操作信息，设备访问信息。\r存储日志数据：架构监控数据（登录日志，中间件访问日志，推送日志，短信邮件发送记录），业务操作日志信息。\r存储业务附件：UDFS 系统（去中心化文件系统）存储图像，视频，文档等附件信息。 属性 HBase RDBMS 数据类型 只有字符串 丰富的数据类型 数据操作 增删改查，不支持JOIN 各种各样的函数和表连接 存储模式 列式存储 表结构和行式存储 数据保护 更新后仍然保留旧版本 替换 可伸缩性 轻易增加节点 需要中间层牺牲性能 数据模型 # ​\t在HBase表中，一条数据拥有一个全局唯一的主键（RowKey）和任意数量的列（Column Qualifier），每个列的数据存储支持多个版本（Version），一列或多列组成一个列族（Column Family），同一个列族或列的数据在物理上存储在同一个HFile中。这样基于列存储的数据结构有利于数据缓存和查询。\n​\tHBase中定位一条数据需要通过：RowKey -\u0026gt; Column Family -\u0026gt; Column Qualifier -\u0026gt; Version\nHBase 表中的数据是疏松地存储的，因此用户可以动态地为数据定义各种不同的列。HBase 中的数据按主键排序（字典序），同时 HBase 会将表按主键划分为多个 HRegion 存储在不同的 HRegionServer 上，以完成数据的分布式存储和读取\nNameSpace # 命名空间类似于关系型数据库中的数据库的概念，他其实是表的逻辑分组。这种抽象为多租户相关功能奠定了基础。命名空间是可以管理维护的，可以创建，删除或更改命名空间。HBase 有两个特殊预定义的命名空间：\rdefault：没有明确指定命名空间的表将自动落入此命名空间\rhbase：系统命名空间，用于包含 HBase 的内部表和元数据表 Table # ​\tTable 和关系型数据库中的表一个意思，由行和列组成。\nRowKey # RowKey的概念和关系型数据库中的主键相似，是一行数据的唯一标识。RowKey 可以是任意字符串（最大长度是\r64KB，实际应用中长度一般为 10-100 Bytes），RowKey 以字节数组保存。存储数据时，数据会按照 RowKey 的字典序排序存储，所以设计 RowKey 时，要充分利用排序存储这个特性，将经常一起读取的行存放到一起。\r访问 HBase 数据的方式有三种：\r基于 RowKey 的单行查询；\r基于 RowKey 的范围查询；\r全表扫描查询。 Column Family # Column Family 即列族，HBase 基于列划分数据的物理存储，同一个列族中列的数据在物理上都存储在同一个 HFile中。一个列族可以包含任意多列，一般同一类的列会放在一个列族中，每个列族都有一组存储属性：\r是否应该缓存在内存中；\r数据如何被压缩或行键如何编码等。\rHBase 在创建表的时候就必须指定列族。HBase 的列族不是越多越好，官方推荐一个表的列族数量最好小于或者等于三，过多的列族不利于 HBase 数据的管理和索引 Column Qualifier # 列族的限定词，理解为列的唯一标识。但是列标识是可以改变的，因此每一行可能有不同的列标识。使用的时候必\r须 列族:列 ，列可以根据需求动态添加或者删除，同一个表中不同行的数据列都可以不同。 Timestamp # Timestamp 是实现 HBase 多版本的关键。在 HBase 中，使用不同的 Timestamp 来标识相同 RowKey 对应的不同版本的数据。相同 RowKey 的数据按照 Timestamp 倒序排列，默认查询的是最新的版本，当然用户也可以指定 Timestamp 的值来读取指定版本的数据。\rHBase 通过 RowKey 和 Column Family，Column Qualifier 来确定一个存贮单元，然后再通过时间戳来进行索引。时间戳的类型是 64 位整型，时间戳默认是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值，如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。\r为了避免数据存在过多版本而造成管理（包括存贮和索引）负担，HBase 提供了两种数据版本回收方案：\r一是保存数据的最后 n 个版本\r二是保存最近一段时间内的版本（比如最近七天） Cell # Cell 由 Row，Column Family，Column Qualifier，Version 组成。Cell 中的数据是没有类型的，全部使用字节码形式存贮，因为 HDFS 上的数据都是字节数组。\n架构模型 # HBase 可以将数据存储在本地文件系统，也可以存储在 HDFS 文件系统。在生产环境中，HBase 一般运行在 HDFS上，以 HDFS 作为基础的存储设施。用户通过 HBase Client 提供的 Shell 或 Java API 来访问 HBase 数据库，以完成数据的写入和读取。HBase 集群主要由 HMaster、HRegionServer 和 ZooKeeper 组成\nZookeeper # HBase 通过 ZooKeeper 来完成选举 HMaster、监控 HRegionServer、维护元数据集群配置等工作。\r选举HMaster：保证任何时候，都只有一个HMaster，实现HM主从节点的Failover\r监控HRegionServer（节点探活）：实时监控HRegionServer的状态，将HRegionServer的上下线信息实时报告给HM\r维护员数据和集群配置：存放整个HBase集群的元数据以及集群的状态信息包括：\r存储所有HRegiion的寻址入口（hbase:meta 元数据表），存储所有的元数据信息\r存储HBase的Schema，包括有哪些Table，每个Table有哪些Column Family. Client # HBase Client 为用户提供了访问 HBase 的接口，可以通过元数据表（客户端负责发送请求到数据库）来定位到目标数据的 HRegionServer。客户端连接的方式有很多种：\rHBase Shell\rJava API\r发送的请求主要包括：\rDDL：数据库定义语言（表的建立，删除，添加删除列族，控制版本）\rDML：数据库操作语言（增删改）\rDQL：数据库查询语言（查询，全表扫描，基于主键，基于过滤器）\rClient 维护着一些 Cache 来加快对 HBase 的访问，比如 HRegione 的位置信息。 HMaster # HMaster是HBase集群的主节点，负责整个集群的管理工作，HMaster可以实现高可用（Active和Backup），通过Zookeeper来维护主备节点的切换。HM主要工作如下\r1、管理分配：管理和分配HRegion，负责启动的时候分配HR到具体的HRegionServer，又或者在分割HRegion时管理HRegion的分配，管理用户对Table结构的DDL操作。\r2、负载均衡：一方面负责将用户的数据均衡的分布在各个HRegionServer上，防止HRegionServer数据倾斜过载。另一方面负责将用户的请求均衡分布在各个HRegionServer上，防止HRS请求过热。\r3、数据维护：发现是失效的HR，并将失效的HR分配到正常的HRS上，当某个HRS下线时，迁移其内部的HR到其他HRS上\r4、权限控制。 HRegionServer # HRegionServer直接对接用户的读写请求，是真正干活的节点，属于HBase具体数据的管理者。\r1、实时和HMaster保持心跳，汇报当前节点信息\r2、当接受到HMaster的命令创建表时，会分配一个HRegion对应一张表\r3、负责切分在运行过程中变大的HR\r4、当HRS意外关闭的时候，当前节点HR会被其他HRS管理\r5、维护HM分配给他的HR，处理这些HR的IO请求\r当客户端发送DML和DQL操作时，HR负责和客户端建立连接\rWAL：Write Ahead Log日志先行，记录了数据写入，更新日志，被用来做故障恢复\rMemStore：写缓存，数据首先会被写入到MemStore中，每个HRegion的每个Column Family都会有一个MemStore\r负责和底层的HDFS交互，存储数据（Hlog，HFile)到HDFS\rBlockCache：读缓存，在内存中存储了最常访问的数据，采用LRU机制进行淘汰\r当某个 HRegionServer 宕机后，ZooKeeper 会通知 HMaster 进行失效备援。下线的 HRegionServer 所负责的 HRegion 暂时停止对外提供服务，HMaster 会将该 HRegionServer 所负责的 HRegion 转移到其他 HRegionServer 上，并且会对下线的HRegionServer 进行日志重放，将 MemStore 中还未持久化到磁盘中的数据进行恢复。\r当某台 HRegionServer Failover 的时候，整个过程中 HRegion 中的数据是不可用的，因为它是缺失的。因此，HBase 属于 CP 架构，降低了可用性，具备强一致性读/写。设想一下，如果 Redo 过程中的 HRegion 能够响应请求，那么可用性提高了，则必然返回不一致的数据（因为 Redo 可能还没完成），那么 HBase 的一致性就降低了。 HRegion # 一个HRegionServer包含多个HRegion。HBase将表中的数据基于RowKey的不同范围划分到不同的HRegion上，每个HRegion都负责一定范围的数据存储和访问\rHRegion时HBase中分布式存储和负载的最小单元，不同的HRegoin可以分布在不同的HRegionServer上。每个表一开始只有一个HRgion，随着数据不断插入表，HRegion会被转移到其他的HRegionServer上，实现负载均衡。\r当Table中的行不断增多，就会有越来越多的HRegion。为了防止前期数据的处理都集中在一个HRS，我们可以根据自己的业务进行预分区。\r这样数据被划分到不同的HR上，每个HR都可以独立的进行镀锡，HB读写数据的时候还可以与多HR分布式并发操作，所以访问速度不会有太大的降低。 Split # HBase中的Split通过把数据分配到一定数量的HR来达到均衡负载，一个Table会被分配到一个或多个HR中，这些HR会被分配到一个或多个HRS中。在自动Split策略中，当一个HR达到一定大小就会自动Split成两个HR。Table在HR中是按照RowKey来排序的，并且一个RowKey所对应的行只会存储在一个HR中，这一点保证了HBase的强一致性。\rTable初始化的时候如果不配置的话，HB是不知道如何去Split HR的，因为HB不知道应该把哪个Row Key作为Split的开始点，如果我们可以大概预测到RowKey的分布，我们可以使用pre-spliting来帮助我们提起那Split HR。\r预测不准确会导致某个HR被集中访问过热。此时auto-spilit会默认按照10G切分。最好是先pre再auto\rHB在灭磁数据合并之后都会针对HR生成一个requestSplit请求，去执行checkSplit，检测FileSize是否达到阈值，超过的会被切分。\rHB自带两种pre-split算法，分别是HexStringSpllit和UniformSplit，如果我们的RowKey是16进制的字符串作为前缀，就适合用HSS。也可以用自己的Split算法。\r当一个HR达到一定大小，他会自动Split成两个HR。HB0.94版本后有三种默认的Split策略：ConstantSizeRegionSplitPolicy，IncreasingToUpperBoundRegionSplitPolicy 和\rKeyPrefixRegionSplitPolicy。\r在0.94版本之前 ConstantSizeRegionSplitPolicy 是默认和唯一的 Split 策略。当某个 Store（对应一个 Column Family）的大小大于配置值 hbase.hregion.max.filesize 的时候（默认 10G）HRegion 就会自动分裂。\r在0.94版本之后IncreasingToUpperBoundRegionSplitPolicy石墨认得到Split策略。这个策略中，最小的分裂大小和Table的某个HR个数有关，当StoreFile的值大于时会Split：\r# R 为同一个 Table 中在同一个 HRegionServer 中的 HRegion 的个数\rMin(R^2 * \u0026#34;hbase.hregion.memstore.flush.size\u0026#34;, \u0026#34;hbase.hregion.max.filesize\u0026#34;)\rKeyPrefixRegionSplitPolicy可以保证相同前缀的RowKey保存在同一个HR中。指定RowKey前缀位数划分HR，通过读取KeyPrefixRegionSplitPolicy。prefix_length属性，数字类型前缀长度。Split时按照长度对SplitPoint进行截取。适合固定前缀的RowKey。Table中没有设置该属性时，指定此策略效果等同使用ITUBRSP\r我们可以通过配置 hbase.regionserver.region.split.policy 来指定 Split 策略，也可以写我们自己的 Split 策略。 Store # 一个HR由多个Store组成，每个Store都对应一个Column Family，Store包含1个MenStore和若干个StoreFile组成。\rMemStore：作为HB的内存数据存储，数据的写操作会先到MemStore中，当MenStore中的数据增长到制定阈值（默认128M），HRS会启动FlushCache进程将MemStore中的数据写入StoreFile持久化存储，每次写入后都形成一个单独的StoreFile。当客户端检索时，现在MemStore中查找，如果MemStore中不存在，则会在StoreFile中继续查找。\rStoreFile：MemStore中的数据写道文件后就是StoreFile，StoreFile底层是以HFile格式保存。HB以StoreFile的大小判断是否需要切分HR当一个HR中所有StoreFile的大小和数量都增长到超过阈值，HMaster会把当前HR分割为两个，切分后其中一个HR会被转移大其他的HRS上，实现均衡负载。\rHFile：HFile和StoreFile是同一个文件，只不过站在HDFS的角度称这个文件为HFile，站在HB角度称这个文件为StoreFile，是HB在HDFS中存储数据的格式，包含多层的索引，在这样的HB检索的时候就不用完全加载整个文件。 HFile # 基本概念 # 官方文档：https://hbase.apache.org/book.html#hfile\nStoreFile(HFile) 是 HBase 最终存储数据的介质\rBlock：每个 HFile 由 N 个 Block 组成。\rKeyValue：每个Blokc是由多个KeyValue组成，KeyValue对象是数据存储的核心，KeyValue包装了一个字节数组，同时将偏移量offsets和lengths放入数组中，这个数组指定从哪里开始解析数据内容。\r注意：KeyValue 对象不跨 Block 存储，假如这里有一个 KeyValue 的大小为 8M，即使 Block-Size=64KB，当读取该KeyValue 的时候也是以一个连贯的 Block 进行读取。\rKeyValue 包装的字节数组的格式如下：\rKey Length：存储 Key 的长度，4 个字节。\rValue Length：存储 Value 的长度，4 个字节。\rKey（还可以被拆解）：存储数据的 Key，由 Row Key Length，Row Key，Column Family Length，\tColumn Family，Column Qualifier，TimeStamp，Key Type 组成。\rValue：存储 Column Qualifier 的值（用户写入的实际数据）。\r而 Key 又可以拆解成以下部分：\rRow Length：存储RowKey的长度，2字节\rRow：存储RowKey的实际内容，大小为Row Length\rColumn Family Length：存储Column Family长度，1字节\rColumn Family：存储Column Family实际内容\rColumn Qualifier：存储Column Qualifier实际内容，\rTimestamp：存储时间戳，8字节\rKeyType：存储Key类型，1字节。Type 分为 Put、Delete、DeleteColumn、\tDeleteFamilyVersion、DeleteFamily 等类型，标记这个 KeyValue 的类型\r由Data Block的结构可以看出，HBase中数据在最底层是以KeyValue的形式存储的，其中Key是一个比较复杂的复合结构。因为任意KeyValue中都包含Rowkey、Column Family以及Column Qualifier，因此这种存储方式实际上比直接存储Value占用更多的存储空间。这也是HB熊仔表结构设计时强调Rowkey、Column Family以及Column Qualifier尽可能设置短的根本原因。字节长，KV大磁盘消耗大。 文件结构 # 官方文档：https://hbase.apache.org/book.html#_hfile_format_2\r截止目前，HFile 一共有 3 个版本，要深入了解 HFile 的话，还得从第一个版本开始看起。\rHFile V1\r读取一个 HFile 时，会首先读取 Trailer，由 Trailer 指向数据块的 Data Block Index。Trailer 保存了每个段的起始位置（段的 Magic Number 用来做安全 Check），然后 Data Block Index 会被读取到内存中，这样，当检索某个 Key 时，不需要扫描整个 HFile，只需要从内存中找到 Key 所在的 Block，通过一次磁盘 IO 将整个 Block 读取到内存中，再找到需要的Key 即可\rData Block 是 HBase I/O 的基本单元，为了提高效率，HRegionServer 提供了基于 LRU 的 Block Cache 机制。每个Block Data 块的大小可以在创建 Table 的时候通过 BLOCKSIZE 参数指定\r大号的 Block 有利于顺序 Scan，小号的 Block 利于随机查询。每个 Data 块除了开头的 Magic 以外就是一个个KeyValue 拼接而成，Magic 内容就是一些随机数字，目的是防止数据损坏。\rHFile 的 Data Block，Meta Block 通常采用压缩的方式存储。具体使用方式如下。\r第一版的Block Index非常简单，只有两个：Data Block Index，Meta Block Index\rBlock Index包含：\rOffset (long)\rUncompressed size (int)\rKey (a serialized byte array written using Bytes.writeByteArray)\rKey length as a variable-length integer (VInt)\rKey bytes\rBlock Index的数量存放在Traniler，这样才能读取到Block Index的数据。这个版本BI有一个缺点，无法知道Block压缩后的数据大小，在这之后的解压过程是必要的。因此HFile读取器必须根据块之间的偏移量差来推断这个压缩的大小。所以在版本 2，解决了这个问题，在 Block Index 中增加了实际存储 Block 大小的数据（压缩后的数\r据大小）。 HFile V2/V3\r接下来介绍 HFile V2 和 V3 版本，其中 V2 在 0.92 引入，V3 在 0.98 引入。HFile V1 版本除了刚才提到的问题以外，还发现在实际使用过程中占用内存较多，HFile V2 版本针对此也进行了优化，HFile V3 版本基本和 V2 版本相同，只是在Cell 层面添加了 Tag 数组的支持（增加安全方面特性，为 Cell 级别增加 ACL）。\rHFile V2 在接口层面做了兼容，在读 HFile 的时候，支持 v1 和 v2；在写 HFile 的时候，只会写 v2 版本的 HFile。\rHFile V3 不和 V1、V2 兼容，因为在存储 KeyValue 时，会额外的存储 Tags，用于控制 ACL。\r另一方面，V2用到了load-onopen概念，在打开HFile的时候，加载那些必要的信息，包括Trailer，Trailer里面记录的HFile的必要信息。而其他数据就可以在用到的时候，通过Trailer再解析出来。\rHFile V2 主要分为四个部分：\rScanned Block Section ：顺序扫描 HFile 时，会被读取的数据块。这个部分包含 3 种数据块：Data Block，Leaf Index Block 以及 BloomBlock。\r其中 Data Block 中存储用户的 KeyValue 数据，\rLeaf Index Block 中存储索引树的叶子节点数据，\rBloom Block 中存储布隆过滤器相关数据。\rNon-Scanned Block Section ：顺序扫描 HFile 时，不会被读取的数据块。主要包括 Meta Block 和 Intermediate Level Data Index Blocks 两部分。\rLoad-On-Open-Section ：这部分数据会在 HRegionServer 打开 HFile 时直接加载到内存中，包括 Root Data Index、Meta Index Block、FileInfo 和布隆过滤器 MetaBlock。\rFileInfo 是固定长度的数据块，主要记录了文件的一些统计元信息，比较重要的是 AVG_KEY_LEN 和\rAVG_VALUE_LEN，分别记录了该文件中所有 Key 和 Value 的平均长度。\rRoot Data Index 表示 Data Blocke、Meta Block 和 Bloom Filter 的根索引。\r布隆过滤器 Metadata 记录了 HFile 中布隆过滤器的相关元数据。\rTrailer ：这部分主要记录了 HFile 的版本信息、其他各个部分的偏移值和寻址信息。\rHFile 文件由各种不同类型的 Block 构成，虽然这些 Block 的类型不同，但却拥有相同的数据结构。每个 Block 的大小可以在创建表列族的时候通过参数 BLOCKSIZE ＝\u0026gt; \u0026#39;65536\u0026#39; 进行指定默认为 64K。大号的 Block 有利于顺序 Scan，小号的 Block 利于随机查询，因而需要权衡。HBase 将 Block 块抽象为一个统一的 HFileBlock。HFileBlock支持两种类型，一种类型支持 Checkum，一种不支持\r不支持 Checkum 的 HFileBlock 主要包括两部分：BlockHeader 和 BlockData。其中 BlockHeader 主要存储 Block 元数据，BlockData 用来存储具体数据。Block 元数据中最核心的字段是 BlockType 字段，用来标示该 Block 块的类型。HBase 中定义了 8 种 BlockType，每种 BlockType 对应的 Block 都存储不同的数据内容，有的存储用户数据，有的存储索引数据，有的存储 Meta 元数据。对于任意一种类型的 HFileBlock，都拥有相同结构的 BlockHeader，但是 BlockData 结构却不尽相同。 Trailer Block\rTrailer Block主要记录了HFile的版本信息，各个部分的偏移值和寻址信息。\rHRegionServer在打开HFile时会加载所有HFile的Trailer，以及load-on-open到内存中。实际加载过程会先解析Trailer Block，再进一步加载load-on-open部分的数据。\r加载HFile Version版本信息，HBase 中 Version 包含 majorVersion 和 minorVersion 两部分，前者决定了 HFile 的主版本 V1、V2 还是 V3；后者在主版本确定的基础上决定是否支持一些微小修正，比如是否支持 Checksum 等。不同的版本使用不同的文件解析器对 HFile 进行读取解析。\rHBase会根据Version信息计算Trailer Block的大小，再根据Trailer Block大小加载整个HFile Trailer Block到内存中。Trailer Block 中包含很多统计字段\rTotalUncompressedBytes 表示 HFile 中所有未压缩的 KeyValue 总大小。\rNumEntries 表示 HFile 中所有 KeyValue 总数目。\rCompressionCodec 表示该 HFile 所使用的压缩算法，HBase 中压缩算法主要有 BZIP2、LZ4、LZO、SNAPPY、ZSTD 等，默认为 NONE，表示不使用压缩。可以在创建表列族的时候通过参数 COMPRESSION =\u0026gt; \u0026#39;SNAPPY\u0026#39; 进行指定。\rTrailer Block另外两个重要的字段是LoadOnOpenDataOffset和LoadOnOpenDataSize，前者表示load-on-open Section在整个HFile文件中的偏移量，后者表示大小。根据此偏移量以及大小，HBase 会在启动后将 load-on-open Section 的数据全部加载到内存中。load-on-open 部分主要包括 FileInfo 模块、Root Data Index\r模块以及布隆过滤器 Metadata 模块。\rFileInfo 是固定长度的数据块，主要记录了文件的一些统计元信息，比较重要的是 AVG_KEY_LEN 和\rAVG_VALUE_LEN，分别记录了该文件中所有 Key 和 Value 的平均长度。\rRoot Data Index 表示 Data Blocke、Meta Block 和 Bloom Filter 的根索引。\r布隆过滤器 Metadata 记录了 HFile 中布隆过滤器的相关元数据。 Data Block\rData Block是HBase中文件读取的最小单元，Data Block中主要存储用户的KeyValue数据，而KV结构式HB存储的核心，HB中所有数据都是以KV结构存储在Data Block中。\rKeyValue 由 4 个部分构成，分别为 Key Length、Value Length、Key 和 Value。其中，Key Length 和 Value Length 是两个固定长度的数值，Value 是用户写入的实际数据，Key 是一个复合结构，由多个部分构成：Rowkey、Column Family、Column Qualifier、TimeStamp 以及 KeyType。其中，KeyType 有四种类型，分别是 Put、Delete、DeleteColumn 和DeleteFamily。\r由 Data Block 的结构可以看出，HBase 中数据在最底层是以 KeyValue 的形式存储的，其中 Key 是一个比较复杂的复合结构。因为任意 KeyValue 中都包含 Rowkey、Column Family 以及 ColumnQualifier，因此这种存储方式实际上比直接存储Value 占用更多的存储空间。这也是 HBase 系统在表结构设计时经常强调 Rowkey、Column Family 以及 ColumnQualifier 尽可能设置短的根本原因。 布隆过滤器相关的 Block\r布隆过滤器对 HBase 的数据读取性能优化至关重要。HBase 是基于 LSM 树结构构建的数据库系统，数据首先写入内存，然后异步 Flush 到磁盘形成文件。这种架构天然对写入友好，而对数据读取并不十分友好，因为随着用户数据的不断写入，系统会生成大量文件，用户根据 Key 获取对应的 Value，理论上需要遍历所有文件，在文件中查找指定的 Key，这无疑是很低效的做法。\r使用布隆过滤器可以对数据读取进行相应优化，对于给定的 Key，经过布隆过滤器处理就可以知道该 HFile 中是否存在待检索 Key，如果不存在就不需要遍历查找该文件，这样就可以减少实际 IO 次数，提高随机读性能。布隆过滤器通常会存储在内存中，所以布隆过滤器处理的整个过程耗时基本可以忽略。\rHBase 会为每个 HFile 分配对应的位数组，KeyValue 在写入 HFile 时会先对 Key 经过多个 Hash 函数的映射，映射后将对应的数组位置为 1，get 请求进来之后再使用相同的 Hash 函数对待查询 Key 进行映射，如果在对应数组位上存在 0，说明该 get 请求查询的 Key 肯定不在该 HFile 中。当然，如果映射后对应数组位上全部为 1，则表示该文件中有可能包含待查询 Key，也有可能不包含，需要进一步查找确认（布隆过滤器的缺点）。\r可以想象，HFile 文件越大，里面存储的 KeyValue 值越多，位数组就会相应越大。一旦位数组太大就不适合直接加载到内存了，因此 HFile V2 在设计上将位数组进行了拆分，拆成了多个独立的位数组（根据 Key 进行拆分，一部分连续的 Key 使用一个位数组）。这样，一个 HFile 中就会包含多个位数组，根据 Key 进行查询时，首先会定位到具体的位数组，只需要加载此位数组到内存进行过滤即可，从而降低了内存开销。\r在文件结构上每个位数组对应 HFile 中一个 Bloom Block，因此多个位数组实际上会对应多个 Bloom Block。为了方便根据 Key 定位对应的位数组，HFile V2 又设计了相应的索引 Bloom Index Block。\r整个 HFile 中仅有一个 Bloom Index Block 数据块，位于 load-on-open 部分。Bloom Index Block 从大的方面看由两部分内容构成，其一是 HFile 中布隆过滤器的元数据基本信息，其二是构建了指向 Bloom Block 的索引信息。\rBloom Index Block 结构中 TotalByteSize 表示位数组大小，NumChunks 表示 Bloom Block 的个数，HashCount 表示Hash 函数的个数，HashType 表示 Hash 函数的类型，TotalKeyCount 表示布隆过滤器当前已经包含的 Key 的数目，TotalMaxKeys 表示布隆过滤器当前最多包含的 Key 的数目。\rBloom Index Entry 对应每一个 Bloom Block 的索引项，作为索引分别指向 Scanned block 部分的 Bloom Block，BloomBlock 中实际存储了对应的位数组。Bloom Index Entry 的结构见上图中间部分，其中 BlockKey 是一个非常关键的字段，表示该 Index Entry 指向的 Bloom Block 中第一个执行 Hash 映射的 Key。BlockOffset 表示对应 Bloom Block 在 HFile 中的偏移量。\r因此，一次 get 请求根据布隆过滤器进行过滤查找需要执行以下三步操作：\r首先根据待查找 Key 在 Bloom Index Block 所有的索引项中根据 BlockKey 进行二分查找，定位到对应的 Bloom IndexEntry。\r再根据 Bloom Index Entry 中 BlockOffset 以及 BlockOndiskSize 加载该 Key 对应的位数组。\r对 Key 进行 Hash 映射，根据映射的结果在位数组中查看是否所有位都为 1，如果不是，表示该文件中肯定不存在该Key，否则有可能存在。 索引相关的 Block\r根据索引层级的不同，HFile 中索引结构分为两种： single-level 和 multi-level ，前者表示单层索引，后者表示多级索引，一般为两级或三级。HFile V1 版本中只有 single-level 一种索引结构，V2 版本中引入多级索引。随着HFile文件越来越大，Data Block越来越多，索引数据也越来越大，已经无法全部加载到内存中，多级索引可以只加载部分索引，从而降低内存使用空间。同步隆过滤器内存使用问题一样。\rV2 版本 Index Block 有两类：Root Index Block 和 NonRoot Index Block。NonRoot Index Block 又分为 Intermediate Index Block 和 Leaf Index Block 两种。HFile 中索引是树状结构，Root Index Block 表示索引数根节点，Intermediate Index Block 表示中间节点，Leaf Index Block 表示叶子节点，叶子节点直接指向实际 Data Block，\r需要注意的是，这三种 Index Block 在 HFile 中位于不同的部分\rRoot Index Block 位于“load-on-open”部分，会在 HRegionServer 打开 HFile 时加载到内存中。\rIntermediate Index Block 位于“Non-Scanned block”部分，\rLeaf Index Block 位于“Scanned block”部分，Leaf Index Block 直接指向实际 Data Block。\rHFile 中除了 Data Block 需要索引之外，Bloom Block 也需要索引，Bloom 索引结构实际上采用了单层结构，Bloom Index Block 就是 Root Index Block 其中一种。对于 Data Block，由于 HFile 刚开始数据量较小，索引采用单层结构，只有 Root Index Block 一层索引，直接指向Data Block。当数据量慢慢变大，Root Index Block 大小超过阈值之后，索引就会分裂为多级结构，由一层索引变为两层，根节点指向叶子节点，叶子节点指向实际 Data Block，如果数据量再变大，索引层级就会变为三层。\rRoot Index Block\rRoot Index Block 表示索引树根节点索引块，既可以作为 Bloom Block 的直接索引，也可以作为 Data Block 多极索引树的根索引。对于单层和多级这两种索引结构，对应的 Root Index Block 结构略有不同，单层索引结构是多级索引结构的一种简化场景。本节以多级索引结构中的 Root Index Block 为例进行分析。\rIndex Entry 表示具体的索引对象，每个索引对象由 3 个字段组成：Block Offset 表示索引指向 Data Block 的偏移量，BlockDataSize 表示索引指向 Data Block 在磁盘上的大小，BlockKey 表示索引指向 Data Block 中的第一个 Key。　除此之外，还有另外 3 个字段用来记录 MidKey 的相关信息，这些信息用于在对 HFile 进行 Split 操作时，快速定位HFile 的切分点位置。需要注意的是单层索引结构和多级索引结构相比，仅缺少与 MidKey 相关的这三个字段。\rRoot Index Block 位于整个 HFile 的“load-on-open”部分，因此会在 HRegionServer 打开 HFile 时直接加载到内存中。此处需要注意的是，在 Trailer Block 中有一个字段为 DataIndexCount，表示 Root Index Block 中 Index Entry 的个数，只有知道 Entry 的个数才能正确地将所有 Index Entry 加载到内存。\rNonRoot Index Block\r当 HFile 中 Data Block 越来越多，单层结构的根索引会不断膨胀，超过一定阈值之后就会分裂为多级结构的索引结构。多级结构中根节点是 Root Index Block。而索引树的中间层节点和叶子节点在 HBase 中存储为 NonRoot Index Block，但从 Block 结构的视角分析，无论是中间节点还是叶子节点，其都拥有相同的结构。\r和 Root Index Block 相同，NonRoot Index Block 中最核心的字段也是 IndexEntry，用于指向叶子节点块或者 Data Block。不同的是，NonRoot Index Block 结构中增加了 Index Entry 的内部索引 Entry Offset 字段，Entry Offset 表示IndexEntry 在该 Block 中的相对偏移量（相对于第一个 Index Entry），用于实现 Block 内的二分查找。通过这种机制，所有非根节点索引块（包括 Intermediate Index Block 和 Leaf Index Block）在其内部定位一个 Key 的具体索引并不是通过遍历实现，而是使用二分查找算法，这样可以更加高效快速地定位到待查找 Key。 Why Fast # 分层索引\r无论是 Data Block Index 还是 Bloom Filter，都采用了分层索引的设计。Data Block 的索引，在 HFile V2 中最多可支持三层索引：\r最底层的 Data Block Index 称之为 Leaf Index Block，可直接索引到 Data Block；\r中间层称之为 Intermediate Index Block，\r最上层称之为 Root Data Index，Root Data index 存放在 Load-on-open Section 区域，会在 HRegionServer 打开 HFile时加载到内存中。\r最基本的额索引流程为：由Root Data Index索引到Intermediate Block Index，再由Intermediate Block Index 索引到lesf Index Block，最后由Leaf INdex Block 查找到对应的Data Block。最后由Leaf Index Block查找到对应的Data Block。在实际场景中，Intermediate Block Index基本上用不到。因此，索引逻辑被简化为：由 Root Data Index 直接索引到 Leaf Index Block，再由 Leaf Index Block 查找到的对应的 Data Block。\r交叉存储\rHFile V1 的设计导致了 HRegionServer 启动时间过长，需要加载很大的数据量，比如大量的 Bloom Filter，大量的Block Index。为了解决这个问题，V2 使得 HFile 增加了新特性，把 Bloom Filter 和 Block Index 打散，写到多个 Block 中去，这样就减少了 HFile 写入时候的内存 Offset 记录。并且这些打散的 Block Index 会有预定的长度。\rBloom Filter 包含 Bloom 元数据(Hash 函数类型，Hash 函数个数等)与位数组(Bloom Data)，为了避免每一次读取时加载所有的 Bloom Data，HFile V2 中将 Bloom Data 部分分成了多个小的 Bloom Block。\rBloom Data 数据也被当成一类 Inline Block，与 Data Block、Leaf Index Block 交叉存在，而关于 Bloom Filter 的元数据与多个 Bloom Block 的索引信息，也被存放在 Load-On-Open Section 部分。\r需要注意的是，关于 Bloom Filter 的配置类型信息，保存在 FileInfo 部分，可以在创建表列族的时候通过参数BLOOMFILTER =\u0026gt; \u0026#39;ROW\u0026#39; 进行指定。共包含三种类型：\r不启用；\r基于 Row 构建 Bloom Filter；\r基于 Row + Column 构建 Bloom Filter\r按需读取\r无论是 Data Block 的索引数据，还是 Bloom Filter 数据，都被拆成了多个 Block，基于这样的设计，无论是索引数据，还是 Bloom Filter，都可以按需读取，避免在读取阶段一次读入大量的数据，有效降低时延。 HLog # 一个HRegionServer只有一个HLog文件。负责记录数据的操作日志，当 HBase 出现故障时可以进行日志重放、故障恢复。例如磁盘掉电导致 MemStore 中的数据没有持久化存储到 StoreFile，这时就可以通过 HLog 日志重放来恢复数据。\rHLog 文件就是一个普通的 Hadoop Sequence File，Sequece File 的 Key 是 HLogKey 对象，Sequece File 的 Value 是HBase 的 KeyValue 对象，即本次的操作。\rHLogKey 中记录了写入数据的归属信息，除了 Table 和 HRegion 名称外，同时还包括 Sequence Number 和\rTimestamp：\rTimestamp：写入时间。\rSequence Number：起始值为 0，或者是最近一次存入文件系统中的 Sequence Number。\r数据被写入 WAL 后，会被加入到 MemStore 即写缓存。然后服务端就可以向客户端返回 ack 表示写数据完成。 HDFS # HDFS 为 HBase 提供底层数据存储服务，同时为 HBase 提供高可用支持。HBase 将 HLog 存储在 HDFS 上，当服务器发生异常宕机时，可以重放 HLog 来恢复数据。\r比如：MySQL 数据直接落入磁盘，HBase 数据落入 HDFS，HDFS 数据落入磁盘。 BlockCache # 为了提升读取性能，HBase实现了已汇总读缓存结构BlockCacche，客户端读取某个Block,首先会检查该Block是否存在于BlockCache，如果存在就直接加载出来，如果不存在就去 HFile 文件中加载，加载出来后再放到 BlockCache中，后续同一请求或者邻近数据查找请求可以直接从内存中获取，以避免昂贵的 IO 开销。BlockCache 主要用来缓存 Block。\rHBase 在实现中提供了两种缓存结构：MemStore 和 BlockCache。MemStore 作为 HBase 的写缓存，保存着数据的最近一次更新；BlockCache 作为 HBase 的读缓存，保存着最近被访问的数据块。 Block 是 HBase 中最小的数据读取单元，即数据从 HFile 中读取都是以 Block 为最小单元执行的。BlockCache 是 RegionServer 级别的，一个 RegionServer 只有一个 BlockCache，在 RegionServer 启动时完成\rBlockCache 的初始化工作。\r到目前为止，HBase 先后实现了 3 种 BlockCache 方案：\rLRUBlockCache ：最早的实现方案，也是默认的实现方案（LRUBlockCache + BucketCache）；\rSlabCache ：HBase 0.92 版本实现的第二种方案，参见 HBase-4027；\rBucketCache ：HBase 0.96 之后官方提供的另一种可选方案，参见 HBase-7404。\r这 3 种方案的不同之处主要在于内存管理模式，其中 LRUBlockCache 是将所有数据都放入 JVMHeap 中，交给 JVM进行管理。而后两种方案采用的机制允许将部分数据存储在堆外。BlockCache 默认是开启的，不需要做额外的事情去开启 BlockCache。如果想让某个列族不使用 BlockCache，可以通过以下命令关闭它：\rcreate \u0026#39;my-table\u0026#39;, {NAME =\u0026gt; \u0026#39;my-cf\u0026#39;, BLOCKCACHE =\u0026gt; \u0026#39;false\u0026#39;}\ralter \u0026#39;my-table\u0026#39;, CONFIGURATION =\u0026gt; {NAME =\u0026gt; \u0026#39;my-cf\u0026#39;, BLOCKCACHE =\u0026gt; \u0026#39;false\u0026#39;} LRUBlockCache # LRUBlockCache 是完全基于 JVM heap 的 LRU 方案，在 0.92 版本之前只有这种 BlockCache 的实现方案，LRU 就是Least Recently Used，即近期最少使用算法。读出来的Block会被放到BlockCache中待下次查询使用，当缓存满了之后会根据LRU的算法来淘汰Block。每次访问数据都会将其放在我们的队首，如果需要淘汰数据，只需要好淘汰队尾的即可这里有个问题，如果有个数据在 1 分钟内被访问了 1000 次，接下来 1 分钟没有访问该数据，但是有其他的数据被大量访问，就会导致这个热点数据被淘汰。\r# Java API 设置 in-memory\rHColumnDescriptor.setInMemory(true);\r# Shell API 设置 in-memory\rcreate \u0026#39;t\u0026#39;, {NAME =\u0026gt; \u0026#39;f\u0026#39;, IN_MEMORY =\u0026gt; \u0026#39;true\u0026#39;} ​\tLRUBlockCache被分为三个区域：\n区域名称 占用比例 说明 single-access 25% 单次读取区域。Block被独处课后先放到这个区域，当被读到多次后会升级到写一个区域 multi-accsss 50% 多次读取区域。缓存在单次读取区域的Block被多次访问会升级在这个区域 in-memory 25% 这个区域跟Block被访问了几次没有关系。只存放被设置了IN_MEMORY =\u0026gt;\u0026lsquo;true\u0026rsquo; 列族被设置为 IN-MEMORY 并不是意味着这个列族是存储在内存中的，这个列族依然是跟别的列族一样存储在硬盘\r上。一般Block被第一次读出后是放到single-access，只有当被访问多次以后才会放到multi-access，带有IN_MEMORY的BLock一开始就会被放在in_memory区域。这个区域的缓存有最高的存活时间，在需要淘汰Block的时候，这个区域的Block是最后被考虑的，这个属性仅仅是为了BlockCache而创造的。\r目前 BlockCache 的堆内内存方案就只有 LRUBlockCache，可以通过将 hfile.block.cache.size 设置为 0 来关闭该方案，但是不推荐。相关配置如下，默认为 0.4，表示分配给 StoreFile 使用的块缓存的最大堆(-Xmx setting)。\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hfile.block.cache.size\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;0.4\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r计算 HBase 中有多少内存可用于缓存的方法如下：number of region servers * heap size(-Xms1024m初始堆和-Xmx1024m最大堆) * hfile.block.cache.size * 0.99 BucketCache # 基本介绍 # BuckCache借鉴了SlabCache的创意，也用上了堆内存，分配了14中区域。\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hbase.bucketcache.bucket.sizes\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;4096,8192\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\rBucketCache的存储不一定要使用堆外内存，是可以自由自在的三种存储介质直接选择：堆外（offheap）、文件（file）、mmaped file（memory-mapped file内存映射文件）。通过设置 hbase.bucketcache.ioengine 为 offheap、file 或者mmaped file 来配置，如下。\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hbase.bucketcache.ioengine\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;offheap\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r每个 Bucket 的大小上限为最大尺寸的 Block * 4，比如可以容纳的最大的 Block 类型是 512KB，那么每个 Bucket 的大小就是 512KB * 4 = 2048KB。系统一启动BucketCache就会把可用的存储空间按照每个Bucket的大小上限均分为多个Bucket。如果划分完的数量比种类还少会报错，因为每一个类型至少要有一个Bucket。\rBuckCache可以自己划分内存空间，自己管理内存空间，Block放进去的时候考虑offset偏移量的，所以内存碎片少，GC时间段。使用BucketCache的好处：\r可以使用SSD硬盘的缓存策略。\r极大改进了SlabCache使用率低的问题\r配置灵活，适用多种场景。 相关配置 # BucketCache 相关配置项如下：\rhbase.bucketcache.ioengine ：使用的存储介质，可选值为offheap、file、mmaped file。默认为offheap。\rhbase.bucketcache.combinedcache.enabled ：是否打开组合模式（CombinedBlockCache）。默认为 true。\rhbase.bucketcache.size ：BucketCache 所占的大小，如果设置为 0.0~1.0，则代表了占堆内存的百分比。如果是大于 1 的值，则代表实际的 BucketCache 的大小，单位为 MB。为什么要一个参数两用这么别扭呢？因为 BucketCache既可以用于堆内存，还可以用于堆外内存和硬盘。默认值为 0.0 ，即关闭 BucketCache。\rhbase.bucketcache.bucket.sizes ：定义所有 Block 种类，默认为 14 种，种类之间用逗号分隔。单位为 B，每一种类型必须是 1024 的整数倍，否则会报异常： java.io.I0Exception: Invalid HFile block magic 。\r-XX:MaxDirectMemorySize ：这个参数不是在 hbase-site.xml 文件中配置的，而是 JVM 启动的参数。如果你不配置这个参数，JVM 会按需索取堆外内存；如果你配置了这个参数，你可以定义 JVM 可以获得的堆外内存上限。显而易见的，这个参数值必须比 hbase.bucketcache.size 大。 Combined BlockCahce（组合模式） # 具体地说就是把不同类型的 Block 分别放到 LRUCache 和 BucketCache 中。\rIndexBlock 和 BloomBlock 会被放到 LRUCache 中；\rDataBlock 被直接放到 BucketCache 中，所以会先去 LRUCache 查询以下，然后再去 BucketCache 查询真正的数据。\r其实这种实现是一种更合理的二级缓存，数据从一级缓存到二级缓存最后到硬盘，数据是从小到大，存储介质也是由快到慢。考虑到成本和性能的组合，比较合理的介质是：LRUCache 使用内存 =\u0026gt; BuckectCache 使用 SSD =\u0026gt; HFile 使用机械硬盘\r提示：设置 hbase.bucketcache.ioengine，开启 hbase.bucketcache.combinedcache.enabled 且 hbase.bucketcache.size 0 即可启用 Combined BlockCache。\r因为 BucketCache 自己控制内存空间，碎片比较少，所以 GC 时间大部分都比 LRUCache 短。\r在缓存全部命中的情况下，LRUCache 的吞土量是 BucketCache 的两倍；在缓存基本命中的情况下，LRUCache 的吞吐量跟 BucketCache 基本相等。\r读写延迟，IO 方面两者基本相等。\r缓存全部命中的情况下，LRUCache 比使用 file 模式的 BucketCacheCPU 占用率低一倍，但是跟其他情况下差不多。\r从整体上说 LRUCache 的性能好于 BucketCache，但由于 Full GC 的存在，在某些时刻 JVM 会停止响应，造成服务不可用。所以适当的搭配 BucketCache 可以缓解这个问题。 BucketCache 中 Block 的缓存写入/读取流程 # 读取 Block 的流程\rRAMCache 是一个存储 blockKey 和 Block 对应关系的 HashMap。\rWriteThead 是整个 Block 写入的中心枢纽，主要负责异步地将 Block 写入到内存空间。\rBucketAllocator 主要实现对 Bucket 的组织管理，为 Block 分配内存空间。\rIOEngine 是具体的内存管理模块，将 Block 数据写入对应地址的内存空间。\rBackingMap 也是一个 HashMap，用来存储 blockKey 与对应物理内存偏移量的映射关系，并且根据 blockKey 定位具体的 Block。图中实线表示 Block 写入流程，虚线表示 Block 缓存读取流程。\rBlock 缓存写入流程\r将 Block 写入 RAMCache。实际实现中，HBase 设置了多个 RAMCache，系统首先会根据 blockKey 进行 hash，根据hash 结果将 Block 分配到对应的 RAMCache 中。\rWriteThead 从 RAMCache 中取出所有的 Block。和 RAMCache 相同，HBase 会同时启动多个 WriteThead 并发地执行异步写入，每个 WriteThead 对应一个 RAMCache。\r每个 WriteThead 会遍历 RAMCache 中所有 Block，分别调用 bucketAllocator 为这些 Block 分配内存空间。\rBucketAllocator 会选择与 Block 大小对应的 Bucket 进行存放，并且返回对应的物理地址偏移量 offset。\rWriteThead 将 Block 以及分配好的物理地址偏移量传给 IOEngine 模块，执行具体的内存写入操作。\r写入成功后，将 blockKey 与对应物理内存偏移量的映射关系写人 BackingMap 中，方便后续查找时根据 blockKey 直接定位。\rBlock 缓存读取流程\r首先从 RAMCache 中查找。对于还没有来得及写入 Bucket 的缓存 Block，一定存储在 RAMCache 中。\r如果在 RAMCache 中没有找到，再根据 blockKey 在 BackingMap 中找到对应的物理偏移地址量 offset。\r根据物理偏移地址 offset 直接从内存中查找对应的 Block 数据。 压缩 BlockCache # HBASE-11331 引入了惰性 BlockCache 解压缩，通常称为压缩 BlockCache。启用压缩 BlockCache 时，数据和编码数据块会以它们在磁盘上的格式的形式缓存在 BlockCache 中，而不是在缓存之前解压缩和解密。与默认的模式不同点在于：默认情况下，在缓存一个数据块时，会先解压缩、解密，然后存入缓存（因为数据块是从 HDFS 取）。而惰性 BlockCache解压缩会直接将数据块存入缓存。\r对于托管的数据量超过缓存容量的 HRegionServer，启用此功能并使用 SNAPPY 压缩，实验证明：吞吐量增加 50%，平均延迟改善 30%，同时垃圾回收增加 80%，但是会增加总体 CPU 负载 2%。有关性能如何衡量和实现的更多详细信息，请参阅 HBASE-11331。对于托管的数据可以轻松适应缓存的 HRegionServer，或者如果您的工作负载对额外的 CPU 或垃圾回收负载比较敏感，可能只会获得较少的好处。\rBlockCache 的压缩功能默认是关闭的，要启用该功能，请在所有 HRegionServer 的 hbase-site.xml 中将\rhbase.block.data.cachcompressed 设置为 true。 交互方式 # 读写流程 # HBase中表的数据量能达到tb;pb级别，但大多数情况下能读取速度能达到毫秒级。通用做法是数据保持有序并尽可能将数据保存在内存中。数据存储上HBase将表切分成一个个的HRegion托管到HRegionServer上，类似关系型数据库的分区表，但比关系型数据库分区分库易用。 三层索引 # HBase 0.96 以前\r整个流程为：Client -\u0026gt; Zookeeper -\u0026gt; ROOT -\u0026gt; .META. -\u0026gt; 用户表的HRegion\r内部维护了两个特殊的表-ROOT-和.META.表，查找各种表的HRegion位置，这两张特殊的表也像HBase中的其他表一样会切分成多个HRegioin。-ROOT-表比.META.更特殊，不会切分超过一个HRegion，这样保证只需跳转三次就能定位到任意HRegion。\r-ROOT-：记录.META.表的HRegion信息\r.META.：记录用户表的HRegion数据\r而-ROOT-表的HRegion位置信息存放在ZooKeeper中，通过ZooKeeper可以找到-ROOT-的HRegion托管的HRegionServer。再通过-ROOT-找到.META.表的HRegion位置。.META.存放用户表的HRegion信息。\r当用户的表特别大时，用户的表的 HRegion 也会非常多。.META. 表存储了这些 HRegion 信息，也会变得非常大，这时.META. 自己也需要划分成多个 HRegion，托管到多个 HRegionServer 上。这时就出现了一个问题：当 .META. 被托管在多个RegionServer 上，如何去定位 .META. 呢？ HBase 的做法是用另外一个表来记录 .META. 的 HRegion 信息，就和.META.记录用户的表的 HRegion 一样，这个表就是 -ROOT- 表。ROOT- 表永远只有一个 HRegion，也就是说只会存放在一台 HRegionServer 上，这个信息至关重要，是所有客户端定位 HRegion 的入口，所以这个映射信息存储在 ZooKeeper 上面。 HBase 0.96 以后\r-ROOT-表被移除，.META.表的HRegion位置信息存放在ZooKeeper中，并将.META.表改名为hbase：meta\r整个流程为：Client -\u0026gt; ZooKeeper -\u0026gt; hbase：meta -\u0026gt; 用户的表的 HRegion\rhbase:meta表结构：\rrowkey:表名，格式：表名，起始键，Hregion的时间戳.Encode编码\rtable:state：表的状态，启用还是禁用\rinfo:state：HRegion的状态，正常状态下为OPEN\rinfo:server：HregionServer的地址和端口\rinfo:serverstartcode：HregionServer启动的13位时间戳\rinfo:sn：server和serverstartcode的组合\rinfo:seqnumDuringOpen：HRegion在线时长的二进制串\rinfo:regioninfo：HRegion的详细信息，如ENCODE、NAME、STARTKEY、ENDKEY等 读取数据流程 # 数据组织 # HBase 数据的组织架构，首先 Table 横向切割为多个 HRegion ，按照一个列族的情况，每一个HRegion 中包含一个 MemStore 和多个HFile文件。\r首先，在 HRegionServer 内部，会把读取可能涉及到的所有组件都初始化为对应的 Scanner 对象，针对 HRegion 的读取，封装为一个 RegionScanner 对象，而一个列族对应一个 Store，对应封装为 StoreScanner，在 Store 内部，MemStore则封装为 MemStoreScanner，每一个 HFile 都会封装为 StoreFileScanner。最后数据的查询就会落在 MemStoreScanner 和StoreFileScanner 上。\r这些 Scanner 首先根据 Scan 的 TimeRange 和 RowKey Range 会过滤掉一些，剩下的 Scanner 在 HRegionServer 内部组成一个最小堆 KeyValueHeap，该数据结构核心为一个 PriorityQueue 优先级队列，队列里按照 Scanner 指向的 KeyValue 排序。\rClient 访问 ZooKeeper，获取 hbase:meta 所在 HRegionServer 的节点信息；\rClient 访问 hbase:meta 所在的 HRegionServer，获取 hbase:meta 记录的元数据后先加载到内存中，然后再从内存中查询出 RowKey 所在的 HRegion （HRegion 所在的 HRegionServer）；\rClient 对 RowKey 所在的 HRegion 对应的 HRegionServer 发起读取数据请求；\rHRegionServer 构建 RegionScanner（需要查询的 RowKey 分布在多少个 HRegion 中就需要构建多少个\rRegionScanner），用于对该 HRegion 的数据检索；\rRegionScanner 构建 StoreScanner（HRegion 中有多少个 Store 就需要构建多少个 StoreScanner，Store 的数量取决于Table 的 ColumnFamily 的数量），用于对该列族的数据检索；\r所有的 StoreScanner 合并构建最小堆（已排序的完全二叉树）StoreHeap:PriorityQueue；\rStoreScanner 构建一个 MemStoreScanner 和一个或多个 StoreFileScanner（数量取决于 StoreFile 数量）；\r过滤掉能够确定所要查询的 RowKey 一定不在的 StoreFileScanner 或 MemStoreScanner（布隆过滤器）；\r经过筛选后留下的Scanner开始做读取数据的准备，将对应的 StoreFile 定位到满足的 RowKey 的起始位置；\r将所有的 StoreFileScanner 和 MemStoreScanner 合并构建最小堆 KeyValueHeap:PriorityQueue，排序的规则按照KeyValue 从小到大排序；从 KeyValueHeap:PriorityQueue 中经过一系列筛选后一行行的得到需要查询的 KeyValue。\r默认情况下：先从BlockCache查找数据，没有再从MemStore上查找，还没有再到StoreFile上查找。其中StoreFile的扫描会先舒勇Bloom Filter过滤那些不可能符合条件的HFile，然后使用Data Blokc Index快读定位Cell，并将其加载到BlockCache中，然后从BlockCache中读取，目的是为了加快后续的查询，然后再返回结果给客户端。 查询过程 # 第一步\r现在我们根据主键 RowKey 来查询对应的记录，通过 hbase:meta 表可以帮我们迅速定位到该记录所在的数据节点，以及数据节点中的 HRegion，由于记录在 1 个 HRegion 中，所以我们只需要查询这 2G 的 HRegion，就可以找到对应的记录。\r第二步\r由于 HBase 存储数据是按照列族存储的。其中的族列是分开存储的，我们只需要遍历相对应的族列就可以找到对应数据。\r第三步\r1 个列族在 HDFS 中会包含 1 个或者多个 HFile（StoreFile），由于 HBase 内存 Flush 到磁盘中的数据是排好序的，要查询的记录有可能在最前面，也有可能在最后面，按平均来算，我们只需遍历 2.5 个 HFile 就可以找到对应的记录。\r第四步\r每个 HFile 中，是以键值对（Key/Value）方式存储，只要遍历文件中的 Key 位置并判断符合条件即可。一般 Key 是有限的长度。\r第 N 步\rHFile 继续划分，有 Data Block，Data Block Index，Trailler 等组成，已经定位到 RowKey 所在的 HFile 时，会先读取HFile 的 Trailer 的信息以获取 Data Block Index 的位置。Data Block Index 的 Key 就是 Data Block 的 RowKey，所以通过 DataBlock Index 的 Key 就能精确的定位到要检索的 RowKey 在哪个 Data Block 上，然后直接将该 Data Block 读取到内存，需要注意的是这里的 Data Block 已经很小了（默认是 64K，不同于 HDFS 上的 Block 默认为 128M，HBase 的 HFile 中的 Block 要小的多）这样子足以读取该 Block 到内存中，将该 Block 进行遍历就能获取到需要的 RowKey 并取出数据。因为这里的Block 只有 64K，所以遍历会非常迅速。这就是为什么 HFile 的 Data Block 要设置的如此之小的原因。\r总结\r正因为以上流程，即使数据量剧增，也不会导致 HBase 的查询性能下降。同时，HBase 是一个面向列存储的数据库（列族机制），当表字段非常多时，可以把其中一些字段独立出来放在一部分机器上，而另外一些字段放到另一部分机器\r上，分散存储，分散列查询。\r正是由于这样复杂的存储结构和分布式的存储方式，保证了 HBase 海量数据下的查询效率。 写入数据流程 # Client 访问 ZooKeeper，获取 hbase:meta 所在 HRegionServer 的节点信息；\rClient 访问 hbase:meta 所在的 HRegionServer，获取 hbase:meta 记录的元数据后先加载到内存中，然后再从内存中查询出 RowKey 所在的 HRegion （HRegion 所在的 HRegionServer）；\rClient 对 RowKey 所在的 HRegion 对应的 HRegionServer 发起写入数据请求；\r建立连接后，首先将 DML 要做的操作写入到日志 HLog；\r然后将数据的修改更新到 MemStore 中，本次操作结束。一个 HRegion 由多个 Store 组成，一个 Store 对应一个列族，Store 包括位于内存中的 Memstore 和位于磁盘的 StoreFile，写操作先写入 MemStore；\r当 MemStore 数据达到阈值后（默认 128M），创建一个新的 MemStore；\r旧的 MemStore 将刷写为一个独立的StoreFile（HRegionServer会启动FlushCache进程写入StoreFile）并存放到HDFS，最后删除 HLog 中的历史数据。\r当 StoreFile 文件的数量增长到一定阈值后，系统会进行合并（次/小 Minor Compaction、主/大 Major Compaction）；在合并过程中会进行版本合并和删除工作，形成更大的 StoreFile；当一个 HRegion 所有 StoreFile 的大小和数量超过一定阈值后，会把当前的 HRegion 分割为两个，并由 HMaster 分配到相应的 HRegionServer 服务器，实现负载均衡。 数据刷写 # 触发时机 # 内存阈值\rHRegion 中的每个 MemStore 占用的内存超过相关阈值 hbase.hregion.memstore.flush.size 时会触发刷写，默认为 128MB。如果我们的数据增加得很快，到了4倍（默认，可修改）除了触发 MemStore 刷写之外，HBase 还会在刷写的时候阻塞所有写入该Store 的请求。\r内存总和\r整个 HRegionServer 的 MemStore 占用内存总和大于相关阈值时会触发刷写。如果达到了 HRegionServer 级别的刷写，当前 HRegionServer 的所有写操作将会被阻塞，这个阻塞可能会持续到分钟级别。\rHBase 为 HRegionServer 所有的 MemStore 分配了一定的写缓存，大小等于 hbase_heapsize(HRegionServer 占用的堆内存大小) * hbase.regionserver.global.memstore.size(默认为 0.4)。\r相关阈值计算公式为： hbase_heapsize * hbase.regionserver.global.memstore.size *\rhbase.regionserver.global.memstore.size.lower.limit(默认为 0.95) = MAX_SIZE 。例如：HBase 堆内存总共是 32G，MemStore 占用内存为：32 * 0.4 * 0.95 = 12.16G 将触发刷写。\r日志阈值\rHBase 使用了 WAL 机制（日志先行），当数据到达 HRegion 时是先写入日志的，然后再被写入到 MemStore。如果日志的数量越来越大，这就意味着 MemStore 中未持久化到磁盘的数据越来越多。当 HRegionServer 挂掉的时候，恢复时间将会变得很长，所以有必要在日志到达一定的数量时进行一次刷写操作。相关公式为：Math.max(32, hbase_heapsize *hbase.regionserver.global.memstore.size * 2 / logRollSize)。\r定期刷写\r当定时器到达 hbase.regionserver.optionalcacheflushinterval （默认值 3600000 毫秒，即 1 小时）时，HBase 会自动触发刷写。一般建议调大，比如 10 小时，因为很多场景下 1 小时 Flush 一次会产生很多小文件，一方面导致Flush 比较频繁，另一方面导致小文件很多，影响随机读性能。\r更新频率\r如果 HBase 的某个 HRegion 更新的很频繁，而且既没有达到自动刷写阀值，也没有达到内存的使用限制，但是内存中的更新数量已经足够多，比如超过 hbase.regionserver.flush.per.changes 参数配置，默认为 30000000 次，也会触发刷写。\r手动刷写\rShell 中通过 flush 命令\rhbase\u0026gt; flush \u0026#39;TABLENAME\u0026#39;\rhbase\u0026gt; flush \u0026#39;REGIONNAME\u0026#39;\rhbase\u0026gt; flush \u0026#39;ENCODED_REGIONNAME\u0026#39;\rhbase\u0026gt; flush \u0026#39;REGION_SERVER_NAME\u0026#39;\r注意\r以上所有条件触发的刷写操作最后都会检查对应的 Store 包含的 StoreFiles 文件数是否超过\rhbase.hstore.blockingStoreFiles 参数配置的个数，默认为 16。如果满足这个条件，那么当前刷写会被推迟到\rhbase.hstore.blockingWaitTime 参数设置的时间后再刷写。\r如果是阻塞刷写，HBase 还会请求 Compaction 压实处理或者 Split 分割操作。 刷写策略 # ","externalUrl":null,"permalink":"/docs/hbase/hbase/","section":"Docs","summary":"HBase # 基本概念 # 列族是什么，扩展是什么，关系型数据库，","title":"","type":"docs"},{"content":" Hive 3.1.2 # 架构_Driver；Hive基础；Hive高级；SQL，优化压缩存储；优化面试重要\n基本概念 # 简介 # ​\tHive用于解决海量结构化数据的统计分析，Hive是一个构建在Hadoop至上的数据分析工具，没有存储数据的能力，只有使用数据的能力。底层由HDFS来提供数据存储，可以将结构化的数据文件映射为一张数据库表，并且提供类似SQL的查询功能，本质上是将HQL转化成MapReduce程序。Hive是一个将SQL转换为MapReduce程序的工具，是一个MR的客户端。\n​\t总结：交互方式采用SQL，元数据存储在Derbyy或MySQL，数据存储在HDFS，分析数据底层是现实MR，执行程序运行在YARN上。\nHive和数据库 # ​\tHive采用了类似SQL的查询语言HQL（Hive Query Language）。\nHive 数据库 查询语言 HQL SQL 数据存储 HDFS 块设备或本地文件系统 数据更新 不支持修改和添加 支持修改和添加 执行方式 MapReduce Executor 执行延迟 高 低 可扩展性 高 低 数据规模 大 小 索引 0.8以后加入围图索引 支持多种索引 事务 0.14版本后支持事务 支持 1、查询语言：因为SQL被广泛应用在数据仓库中，因此，针对Hive的特性设计了类SQL语言HQL。\r2、数据存储：Hive是建立在Hadoop之上，所有的Hive都存储在HDFS中。而数据库则可以将数据保存在块设备或本地文件系统中\r3、数据更新：Hive是针对数据仓库应用设计，数据仓库内容就是读多写少，因此Hive不支持对数据进行修改和添加，所有税局都是在加载时确定好的。数据库中数据可以进行增删改。\r4、执行方式：Hive中大多数查询的执行时通过Hadoop提供的MapReduce实现，数据库有自己的执行引擎\r5、执行延迟：Hive在查询数据的时候，没有索引需要扫描整个表；MR框架本身就有较高的延迟，因此Hive延迟较高。数据库执行延迟地，但是数据规模小，难以计算处理大规模数据。\r6、可扩展性：Hive建立在Hadoop上，因此Hive的可扩展性和Hadoop的可扩展性一致。数据库因为ACID语义的严格限制扩展性非常有限。\r7、数据规模：Hive建立在集群上，通过MR计算，可以支持大规模数据，数据库不行。\r8、事务：Hive在0.14版本后开始支持事务，前提是文件格式必须为orc，必须分桶，必须显示声明transaction=true\r9、索引：3之后禁用。：Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于底层实现是 MapReduce 的原因， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。值得一提的是，Hive 在 0.8 版本后加入了位图索引。Hive 索引原理为：在指定列上建立索引，生成一张索引表（Hive的一张物理表），记录以下三个字段：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。在执行索引字段查询时候，首先额外生成一个 MapReduce Job，根据对索引列的过滤条件，从索引表中过滤出索引列的值对应的 HDFS 文件路径及偏移量，输出到 HDFS 上的一个文件中，然后根据这些文件中的 HDFS 路径和偏移量，筛选原始 Input 文件，生成新的 Split 作为整个 Job 的 Split，达到不用全表扫描的目的。 Hive优缺点 # MR面临学习成本高：复杂的查询逻辑开发难度大，约束Key和Value的类型，自定义排序规则，自定义分区器等等。\rHive操作接口采用类似SQL的语法，提供快速开发的能力，免去了写MR的过程，减少学习成本，功能扩展方便\r优点：\r操作接口采用类似 SQL 的语法，提供快速开发的能力（简单、容易上手）。\r免去了写 MapReduce 的过程，减少开发人员的学习成本。\rHive 的执行延迟比较高，因此 Hive 常用于离线数据分析，对实时性要求不高的场合。\rHive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。\rHive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。\r集群可自由拓展并且具有良好的容错性，节点出现问题 SQL 仍可完成执行。\r缺点：\rHive 的 HQL 表达能力有限，当逻辑需求特别复杂的时候，还是要借助 MapReduce。如迭代式无法表达计算，数据挖掘方面不擅长。\rHive 操作默认基于 MapReduce 引擎，而 MapReduce 引擎与其它的引擎（如 Spark 引擎）相比，特点就是慢、延\r迟高、不适合交互式查询，所以 Hive 也有这个缺点（这里讲的是默认引擎，Hive 是可以更改成其他引擎的）。\rHive 自动生成的 MapReduce 作业，通常情况下不够智能化。\rHive 调优比较困难，粒度较粗。 Hive 应用场景 # 日志分析：大部分互联网公司使用 Hive 进行日志分析，包括百度、淘宝等。统计网站一个时间段内的 PV、UV；多维度数据分析\r海量结构化数据的离线分析。 Hive架构 # 官网地址：https://cwiki.apache.org/confluence/display/Hive/Design\nClicent # Hive 允许 Client 连接的方式有三个 CLI（Hive Shell）、JDBC/ODBC（Java 访问 Hive）、WEBUI（浏览器访问 Hive）。JDBC/ODBC 访问中间件 Thrift 软件框架，跨语言服务开发。DDL DQL DML，整体仿写一套 SQL 语句 MetaStore # 元数据，数据的数据。元数据包括表名、表结构信息、表所属的数据库（默认是 default 库）、表的拥有者（权限信息）、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等。\r元数据的存放一般需要借助于其他的数据载体（Derby 或 MySQL），默认存放在自带的 Derby 数据库（单用户局限性）中，推荐使用 MySQL 进行存储。连接数据库需要提供：uri、username、password、driver。\r元数据服务的作用是：客户端连接MetaStore服务，MetaStore服务再去连接MySql数据库来存储元数据。有了MetaStore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySql数据库的用户名和密码，只需要连接MetaStore服务即可\rHive默认将元数据存储在Derby数据库中，但仅支持单线程操作，若有一个用户在操作，其他用户则无法使用，效率低；切换目录后重新进入Hive原来创建的数据库和表会找不到，所以一般用MySql数据库存储元数据。 Driver # 1、解析器（SQL Parser）：将SQL字符串转换成抽象的语法树AST，这一步一般用第三方工具完成；对AST进行语法分析，比如表是否存在，字段是否存在，SQL是有有误。\r2、编译器（Complier）：将AST编译生成逻辑执行计划\r3、优化器（Query Optimizer）：对逻辑执行计划进行优化\r4、执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。\rParser：将HQL语句解析成抽象的语法树AST（Abstract Syntax Tree）\rSemantic Analyzer：将抽象语法编译成查询块\rLogic Plan Generator：将查询块转换成逻辑查询计划\rLogic Optimizer：重写逻辑查询计划，优化逻辑执行计划\rPhysical Plan Gernerator：讲逻辑计划转化成物理计划MR job\rPhysical Optimizer：选择最佳的Join策略，优化物理执行计划，最后执行。 HDFS # Hive 的数据存储在 HDFS 中，计算由 MapReduce 完成。HDFS 和 MapReduce 是源码级别上的整合，两者结合最佳。解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。 Hive工作原理 # 当创建表的时候，需要指定 HDFS 文件路径，表和其文件路径会保存到 MetaStore，从而建立表和数据的映射关系。当\r数据加载入表时，根据映射获取到对应的 HDFS 路径，将数据导入。用户输入 SQL 后，Hive 会将其转换成MapReduce 或者 Spark 任务，提交到 YARN 上执行，执行成功将返回结果。 Hive 安装 # 安装方式 # Hive 环境搭建分为三种，我们搭建远程模式环境：内嵌模式；本地模式；远程模式\n内嵌模式 # 内嵌模式使用的是内嵌的 Derby 数据库来存储元数据，不需要额外起 MetaStore 服务。数据库和 MetaStore 服务都嵌入在主 Hive Server 进程中。内嵌模式属于默认方式，配置简单，但是一次只能一个客户端连接，适用于实验，不适用于生产环境。解压 Hive 安装包，启动 bin/hive 即可使用。\r缺点：不同路径启动 Hive，每一个 Hive 都拥有一套自己的元数据，无法共享。 本地模式 # 本地模式采用外部数据库来存储元数据，目前支持的数据库有：Oracle、MySQL、PostgreSQL、SQL Server。在这里我们使用 MySQL。本地模式不需要单独起 MetaStore 服务，用的是跟 Hive 在同一个进程里的 MetaStore 服务。也就是说当你启动一个 Hive 服务，里面默认会帮我们启动一个 MetaStore 服务。Hive 会根据 hive.metastore.uris 参数值来判断，如果为空，则为本地模式。\r缺点：每启动一次 Hive 服务，都内置启动了一个 MetaStore。耦合度太高 远程模式 # 远程模式下，需要单独起 MetaStore 服务，然后每个客户端都在配置文件里配置连接到该 MetaStore 服务。远程模式的 MetaStore 服务和 Hive 运行在不同的进程里。在生产环境中，建议用远程模式来配置 Hive Metastore。在这种情况下，其他依赖 Hive 的软件都可以通过 MetaStore 访问 Hive。\r远程模式下需要配置 hive.metastore.uris 参数来指定 MetaStore 服务运行的机器 IP 和端口，并且需要单独手动启动 MetaStore 服务。HiveServer2 是 Hive 启动的一个服务，客户端可以使用 JDBC/ODBC 协议，通过 IP + Port 的方式对其进行访问，达到并发访问的目的。 目标环境 # 注意：安装前确认当前集群已经安装了Mysql数据库和HA+YARN\n节点/功能 MetaStore HiveServer2 Client node01 √ √ node02 node03 √ 安装 # 解压 # 将准备好的安装包上传至 node01，然后解压：\n[root@node01 ~]# tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/yjx/\r[root@node01 ~]# rm apache-hive-3.1.2-bin.tar.gz -rf 修改配置文件 # 修改环境配置脚本文件 hive-env.sh ：\n[root@node01 ~]# cd /opt/yjx/apache-hive-3.1.2-bin/conf/\r[root@node01 conf]# cp hive-env.sh.template hive-env.sh\r[root@node01 conf]# vim hive-env.sh 在文件末尾添加以下内容：\nHive基于hadoop框架\rHADOOP_HOME=/opt/yjx/hadoop-3.3.4/\rexport HIVE_CONF_DIR=/opt/yjx/apache-hive-3.1.2-bin/conf\rexport HIVE_AUX_JARS_PATH=/opt/yjx/apache-hive-3.1.2-bin/lib 修改配置文件 hive-site.xml ：\n[root@node01 conf]# cp hive-default.xml.template hive-site.xml\r[root@node01 conf]# vim hive-site.xml 首先删除 configuration 节点中的所有内容，然后再在 configuration 节点中添加以下内容：\n注意：防止 xml 解析注释出问题，用 == 表示 --。实际为：hive --service metastore -p 9083 \u0026amp; | hive --service metastore\r\u0026lt;!-- 数据库相关配置 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;com.mysql.cj.jdbc.Driver\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;root\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;123456\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 自动创建表 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;datanucleus.schema.autoCreateAll\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 强制 MetaStore 的 schema 一致性，开启的话会校验在 MetaStore 中存储的信息的版本和 Hive 的 jar 包中的版\r本一致性，并且关闭自动 schema 迁移，用户必须手动的升级 Hive 并且迁移 schema。关闭的话只会在版本不一致时给出警\r告，默认是 false 不开启 --\u0026gt;\r\u0026lt;!-- 元数据校验 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.metastore.schema.verification\u0026lt;/name\u0026gt;\r\u0026lt;!-- MySQL8 这里一定要设置为 true，不然后面 DROP TABLE 可能会出现卡住的情况 --\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 美化打印数据 --\u0026gt;\r\u0026lt;!-- 是否显示表名与列名，默认值为 false --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.cli.print.header\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 是否显示数据库名，默认值为 false --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.cli.print.current.db\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- Hive 数据仓库的位置（HDFS 中的位置） --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/hive/warehouse\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- HiveServer2 通过 Thrift 访问 MetaStore --\u0026gt;\r\u0026lt;!-- 配置 Thrift 服务绑定的服务器地址，默认为 127.0.0.1 --\u0026gt;\r\u0026lt;!--\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.server2.thrift.bind.host\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;127.0.0.1\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r--\u0026gt;\r\u0026lt;!-- 配置 Thrift 服务监听的端口，默认为 10000 --\u0026gt;\r\u0026lt;!--\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.server2.thrift.port\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;10000\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r--\u0026gt;\r\u0026lt;!-- HiveServer2 的 WEBUI --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.server2.webui.host\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;node01\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.server2.webui.port\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;10002\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 指定 hive.metastore.uris 的 port，为了启动 MetaStore 服务的时候不用指定端口 --\u0026gt;\r\u0026lt;!-- hive ==service metastore -p 9083 \u0026amp; | hive ==service metastore --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;thrift://node01:9083\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt; 前面提到 Hive 实际上底层跑的仍然是 MapReduce 程序，那么我们需要让它拥有在 Hadoop 上运行的权限，修改 Hadoop 的配置文件 core-site.xml ：\n[root@node01 conf]# vim /opt/yjx/hadoop-3.3.4/etc/hadoop/core-site.xml 在 configuration 节点中末尾处添加以下内容：\n提示：这里的 root 表示安装 Hive 时的用户，实际上是为 Hive 在 Hadoop 上创建了一个代理用户。\r\u0026lt;!-- 该参数表示可以通过 httpfs 接口访问 HDFS 的 IP 地址限制 --\u0026gt;\r\u0026lt;!-- 配置 root(超级用户) 允许通过 httpfs 方式访问 HDFS 的主机名、域名 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hadoop.proxyuser.root.hosts\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;!-- 通过 httpfs 接口访问的用户获得的群组身份 --\u0026gt;\r\u0026lt;!-- 配置允许通过 httpfs 方式访问的客户端的用户组 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hadoop.proxyuser.root.groups\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt; 配置日志组件 # 首先创建日志目录：\n[root@node01 conf]# mkdir /opt/yjx/apache-hive-3.1.2-bin/logs\r[root@node01 conf]# cp hive-log4j2.properties.template hive-log4j2.properties\r[root@node01 conf]# vim hive-log4j2.properties\r将 property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name} 替换为：\rproperty.hive.log.dir = /opt/yjx/apache-hive-3.1.2-bin/logs 添加驱动包 # 将 MySQL 驱动包（注意自己的 MySQL 版本）添加到 Hive 的 lib 目录下：\r[root@node01 ~]# mv mysql-connector-java-8.0.18.jar /opt/yjx/apache-hive-3.1.2-bin/lib/ 拷贝至其他节点 # 将 node01 已配置好的 Hive 拷贝至 node02 和 node03。\r[root@node02 ~]# scp -r root@node01:/opt/yjx/apache-hive-3.1.2-bin /opt/yjx/\r[root@node03 ~]# scp -r root@node01:/opt/yjx/apache-hive-3.1.2-bin /opt/yjx/\r# 或者使用分发脚本\r[root@node01 ~]# yjxrsync /opt/yjx/apache-hive-3.1.2-bin\r将 node01 修改后的 Hadoop 的 core-stie.xml 配置文件拷贝至 node02 和 node03。\r[root@node01 ~]# scp /opt/yjx/hadoop-3.3.4/etc/hadoop/core-site.xml root@node02:/opt/yjx/hadoop-3.3.4/etc/hadoop/\r[root@node01 ~]# scp /opt/yjx/hadoop-3.3.4/etc/hadoop/core-site.xml root@node03:/opt/yjx/hadoop-3.3.4/etc/hadoop/\r# 或者使用分发脚本\r[root@node01 ~]# yjxrsync /opt/yjx/hadoop-3.3.4/etc/hadoop/core-site.xml 配置环境变量 # 三个节点修改环境变量 vim /etc/profile ，在文件末尾添加以下内容：\rexport HIVE_HOME=/opt/yjx/apache-hive-3.1.2-bin\rexport PATH=$HIVE_HOME/bin:$PATH\r修改完成后 source /etc/profile 重新加载环境变量。 启动 # 检查 MySQL 服务是否启动。\r[root@node01 ~]# systemctl status mysqld\r启动 ZooKeeper（三台机器都需要执行）。\rzkServer.sh start\rzkServer.sh status\r启动 HDFS + YARN。\r[root@node01 ~]# start-all.sh\r启动 JobHistory。\r[root@node01 ~]# mapred --daemon start historyserver\r初始化 hive 数据库（第一次启动时执行）。\r[root@node01 ~]# schematool -dbType mysql -initSchema\r启动 MetaStore 服务。\r# 前台启动，学习期间推荐使用这种方式\r[root@node01 ~]# hive --service metastore\r# 后台启动\r[root@node01 ~]# nohup hive --service metastore \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\r启动 HiveServer2 服务。\r# 前台启动，学习期间推荐使用这种方式\r[root@node01 ~]# hiveserver2\r# 后台启动\r[root@node01 ~]# nohup hiveserver2 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; 客户端连接 # 客户端连接方式一。\r[root@node03 ~]# hive\r# 退出命令行命令：exit;\r客户端连接方式二。\r[root@node03 ~]# beeline -u jdbc:hive2://node01:10000 -n root\r# 退出命令行命令：!exit 或者 !quit\r客户端连接方式三。\r方法一：直接执行 beeline -u jdbc:hive2://node01:10000 -n root\r方法二：先执行 beeline ，再执行 !connect jdbc:hive2://node01:10000 ，然后输入用户名，这个用户名就是安装 Hadoop 集群的用户名，无密码的话直接回车即可\r10000 为 hive.server2.thrift.port 的默认值（Thrift Server 的 TCP 端口默认为 10000）。 关闭 # 先关闭 HiveServer2 服务和 MetaStore 服务（前台启动的话直接 Ctrl + C 即可）。\r再关闭 JobHistory 和 Hadoop。\r[root@node01 ~]# mapred --daemon stop historyserver\r[root@node01 ~]# stop-all.sh\r再关闭 ZooKeeper（三台机器都需要执行）。\rzkServer.sh stop Hive 交互方式 # 第一种 # default 是默认的数据库，路径为 /hive/warehouse [root@node03 ~]# hive\r...\rhive (default)\u0026gt; SHOW DATABASES;\rOK\rdatabase_name\rdefault\rTime taken: 0.633 seconds, Fetched: 1 row(s)\rhive (default)\u0026gt; exit; 第二种* # 10000 为 hive.server2.thrift.port 的默认值（Thrift Server 的 TCP 端口默认为 10000）。\r[root@node03 ~]# beeline -u jdbc:hive2://node01:10000 -n root\r...\rConnecting to jdbc:hive2://node01:10000\rConnected to: Apache Hive (version 3.1.2)\rDriver: Hive JDBC (version 3.1.2)\rTransaction isolation: TRANSACTION_REPEATABLE_READ\rBeeline version 3.1.2 by Apache Hive\r0: jdbc:hive2://node01:10000\u0026gt; SHOW DATABASES;\r...\r+----------------+\r| database_name |\r+----------------+\r| default |\r+----------------+\r1 row selected (0.066 seconds)\r0: jdbc:hive2://node01:10000\u0026gt; !exit\rClosing: 0: jdbc:hive2://node01:10000 第三种 # 使用 -e 参数来直接执行 HQL 的语句。\rbin/beeline -u \u0026#34;jdbc:hive2://node01:10000/default\u0026#34; hive -e \u0026#34;SHOW DATABASES;\u0026#34;\r使用 -f 参数通过指定文本文件来执行 HQL 的语句。\rbin/beeline -u \u0026#34;jdbc:hive2://node01:10000/default\u0026#34; hive -f \u0026#34;default.sql\u0026#34; 总结 # MetaStore服务实际上就是一种Thrift服务，通过它我们可以获取到Hive元数据。通过Thriftf服务获取元数据的方式屏蔽了数据库访问需要驱动，URL，用户名，密码等细节\rHiveSver2是一个服务端接口，是远程客户端可以执行对Hive的查询并返回结果。一般来讲，我们认为HS2是用来提交查询的，而MS才是真正用来访问元数据的，所以推荐使用第二种，这种方式更加安全或合理。\r提示：Hive 4.0.0 版本开始 hive shell 也改为了 beeline 的连接方式。 元数据/数据类型 # 元数据 # 版本表 # 数据库相关表 # 视图相关表 # 文件存储信息相关表 # 表字段相关表 # 分区相关表 # 其他表 # 数据类型 # 基础数据类型 # 复杂数据类型 # Hive基础 # 数据库 # 创建数据库 # 创建一个数据库，数据库在 HDFS 上默认的存储路径是 /hive/warehouse/*.db CREATE DATABASE shop;\r避免要创建的数据库已经存在错误，可以使用 IF NOT EXISTS 选项来进行判断。（标准写法）\rCREATE DATABASE IF NOT EXISTS crm;\r指定数据库创建的位置（数据库在 HDFS 上的存储路径）。\rCREATE DATABASE IF NOT EXISTS school location \u0026#39;/hive/school.db\u0026#39;; 修改数据库 # 用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。\rALTER DATABASE school SET DBPROPERTIES(\u0026#39;createtime\u0026#39;=\u0026#39;20220803\u0026#39;); 数据库详情 # 显示所有数据库。\rSHOW DATABASES;\r可以通过 like 进行过滤。\rSHOW DATABASES LIKE \u0026#39;s*\u0026#39;;\r查看某个数据库的详情\rDESC DATABASE school;\rDESCRIBE DATABASE school;\r切换数据库。\rUSE school; 删除数据库 # 最简写法。\rDROP DATABASE school;\r如果删除的数据库不存在，最好使用 IF EXISTS 判断数据库是否存在。否则会报错： FAILED:\rSemanticException [Error 10072]: Database does not exist: school 。\rDROP DATABASE IF EXISTS school;\r如果数据库不为空，使用 CASCADE 命令进行强制删除。否则会报错： FAILED: Execution Error, return code\r40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. InvalidOperationException(message:Databaseschool is not empty. One or more tables exist.)\rDROP DATABASE IF EXISTS school CASCADE 数据表 # 语法 # CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name\r[(col_name data_type [column_specification] [COMMENT col_comment], ... [constraint_specification])]\r[COMMENT table_comment]\r[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]\r[CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_bucketsBUCKETS]\r[SKEWED BY (col_name, col_name, ...)\rON ((col_value, col_value, ...), (col_value, col_value, ...), ...)\r[STORED AS DIRECTORIES]\r[\r[ROW FORMAT row_format]\r[STORED AS file_format]\r| STORED BY \u0026#39;storage.handler.class.name\u0026#39; [WITH SERDEPROPERTIES (...)]\r]\r[LOCATION hdfs_path]\r[TBLPROPERTIES (property_name=property_value, ...)]\r[AS select_statement];\rCREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name\rLIKE existing_table_or_view_name\r[LOCATION hdfs_path];\rdata_type\r: primitive_type\r| array_type\r| map_type\r| struct_type\r| union_type\rprimitive_type\r: TINYINT\r| SMALLINT\r| INT\r| BIGINT\r| BOOLEAN\r| FLOAT\r| DOUBLE\r| DOUBLE PRECISION\r| STRING\r| BINARY\r| TIMESTAMP\r| DECIMAL\r| DECIMAL(precision, scale)\r| DATE\r| VARCHAR\r| CHAR\rarray_type\r: ARRAY \u0026lt; data_type \u0026gt;\rmap_type\r: MAP \u0026lt; primitive_type, data_type \u0026gt;\rstruct_type\r: STRUCT \u0026lt; col_name : data_type [COMMENT col_comment], ...\u0026gt;\runion_type\r: UNIONTYPE \u0026lt; data_type, data_type, ... \u0026gt; row_format\r: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]\r[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]\r[NULL DEFINED AS char]\r| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,\rproperty_name=property_value, ...)]\rfile_format:\r: SEQUENCEFILE\r| TEXTFILE -- (Default, depending on hive.default.fileformat configuration)\r| RCFILE -- (Note: Available in Hive 0.6.0 and later)\r| ORC -- (Note: Available in Hive 0.11.0 and later)\r| PARQUET -- (Note: Available in Hive 0.13.0 and later)\r| AVRO -- (Note: Available in Hive 0.14.0 and later)\r| JSONFILE -- (Note: Available in Hive 4.0.0 and later)\r| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname\rcolumn_constraint_specification:\r: [ PRIMARY KEY|UNIQUE|NOT NULL|DEFAULT [default_value]|CHECK [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY ]\rdefault_value:\r: [ LITERAL|CURRENT_USER()|CURRENT_DATE()|CURRENT_TIMESTAMP()|NULL ]\rconstraint_specification:\r: [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]\r[, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]\r[, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...)\rDISABLE NOVALIDATE\r[, CONSTRAINT constraint_name UNIQUE (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]\r[, CONSTRAINT constraint_name CHECK [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY ] 创建表 # CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name\r[(col_name data_type [COMMENT col_comment], ...)]\r[COMMENT table_comment]\r[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]\r[CLUSTERED BY (col_name, col_name, ...)\r[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]\r[ROW FORMAT row_format]\r[STORED AS file_format]\r[LOCATION hdfs_path]\r[TBLPROPERTIER(property_name=property_value,...)]\r字段解释说明：\rCREATE TABLE：创建一个指定名称的表。如果表名称已经存在，抛出异常；使用IF NOT EXISTS选项进行判断。\rEXTERNAL：创建一个外部表，在建表的同时指向实际数据的路径(LOCATION)。\r创建内部表时，会将数据移动到数据仓库指向的路径（默认位置）\r创建外部表：只记录数据所在路径，不对数据的位置做任何改变。\r删除表的时候：内部表的元数据会被一起删除，外部表只删除元数据，不删除数据。\rCOMMENT：为表和列添加注释\rPARTITIONED BY：创建分区表\rCLUSTERED BY：创建分桶表\rSORTED BY：排序方式，不常用\rROW FORMAT ： DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]\r[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH\rSERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 用户在建表的时候可以自定义SerDe或者使用自带的SerD。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED,将会使用自带的SerD（默认是LazySimpleSerDe类，只支持单字节分隔符）。\r在建表时，需要为表指定列，在指定表的列时也会自定义SerDe，Hive通过SerDe确定表的具体的列数据\rSerDe是Serilize/Deserilize的简称，用于序列化和反序列化。\rSTORED AS：指定存储文件类型，常用的存储文件类型有：SEQUENCEFILE——二进制文件；TEXTFILE——文本；\rRCFILE——列式存储格式文件，如果文件数据是纯文本，可以用STORED AS TEXTFILE。如果诗句需要压缩，使用STORED AS SEQUENCEFILE\rLOCATION：指定表在HDFS上的存储位置\rLIKE：允许用户复制现有表结构，不复制数据 案例一 # 已知一些简单的用户信息如下，将以下数据保存在 user.txt 文件中并上传至 HDFS（例如 /yjx/user.txt）：\r1,admin,123456,男,18\r2,zhangsan,abc123,男,23\r3,lisi,654321,女,16\r根据以上数据结构创建 t_user 表。\rCREATE DATABASE IF NOT EXISTS test;\rCREATE TABLE IF NOT EXISTS test.t_user (\rid int,\rusername string,\rpassword string,\rgender string,\rage int\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r最后执行以下语句将 user.txt 文件的数据载入 t_user 表。执行 LOAD DATA 语句后 user.txt 文件将会被移动\r到 t_user 表所在路径下（ /hive/warehouse/t_user/user.txt ）。\rLOAD DATA INPATH \u0026#39;/yjx/user.txt\u0026#39; INTO TABLE test.t_user;\r查询结果如下：SELECT * FROM test.t_user WHERE age \u0026lt;= 18;\r简单的查询不会执行 MapReduce，复杂的查询会调用 MapReduce 模板生成 MapReduce 任务。注意：复杂的查询如果不走 MR 模板，例如 count(*) 直接返回 0，请使用 LOAD DATA 正规的方式载入数据 案例二 # 已知一些复杂的用户信息如下，将以下数据保存在 person.txt 文件中并上传至 HDFS（例如 /yjx/person.txt）：\rsongsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing\ryangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing\r根据以上数据结构创建 t_person 表。\rUSE test;\rCREATE TABLE IF NOT EXISTS t_person (\rname string,\rfriends array\u0026lt;string\u0026gt;,\rchildren map\u0026lt;string,int\u0026gt;,\raddress struct\u0026lt;street:string ,city:string\u0026gt;\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rCOLLECTION ITEMS TERMINATED BY \u0026#39;_\u0026#39;\rMAP KEYS TERMINATED BY \u0026#39;:\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r最后执行以下语句将 person.txt 文件的数据载入 t_person 表。执行 LOAD DATA 语句后 person.txt 文件将会被移动到 t_person 表所在路径下（ /hive/warehouse/t_person/person.txt ）。\rLOAD DATA INPATH \u0026#39;/yjx/person.txt\u0026#39; INTO TABLE t_person;\r查询结果如下：SELECT * FROM t_person;\r查看数据表详细信息：0: jdbc:hive2://node01:10000\u0026gt; DESC t_person;\r0: jdbc:hive2://node01:10000\u0026gt; SHOW CREATE TABLE t_person; 案例三 # Hive 默认序列化类是 LazySimpleSerDe，其只支持使用单字节分隔符(Char)来加载文本数据，例如逗号、制表符、空格等等，默认的分隔符为“\\001”。根据不同文件的不同分隔符，我们可以通过在创建表时使用 ROW FORMAT DELIMITED 来指定文件中的分割符，确保其正确的将表中的每一列与文件中的每一列实现——对应的关系。\r可以通过 DESC FORMATTED t_sleuth_log; 命令来查看表的格式化详细信息。\r情况一：每一行数据的分隔符是多字节分隔符，例如 || 、 -- 等。\r将以下数据保存在 singer.txt 文件中并上传至 HDFS（例如 /yjx/singer.txt）。\r01||周杰伦||中国||台湾||男||稻香\r02||刘德华||中国||香港||男||笨小孩\r03||汪 峰||中国||北京||男||光明\r04||朴 树||中国||北京||男||那些花儿\r05||许 巍||中国||陕西||男||故乡\r06||Taylor Swift||美国||宾夕法尼亚||女||Blank Space\r07||Maroon 5||美国||加利福尼亚||男||Sugar\r根据以上数据结构创建 t_singer 表。\rCREATE TABLE IF NOT EXISTS t_singer (\rid string,\rname string,\rcountry string,\rprovince string,\rgender string,\rworks string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;||\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r最后执行以下语句将 singer.txt 文件的数据载入 t_singer 表。执行 LOAD DATA 语句后 singer.txt 文件将会被移动到 t_singer 表所在路径下（ /hive/warehouse/t_singer/singer.txt ）。\rLOAD DATA INPATH \u0026#39;/yjx/singer.txt\u0026#39; INTO TABLE t_singer;\r.查询结果如下：SELECT * FROM t_singer; 情况二：数据的字段中包含了分隔符。\r将以下数据保存在 gateway-server.log 文件中并上传至 HDFS（例如 /yjx/gateway-server.log）。\r2022-08-11 14:15:25.326 [gateway-server,,] [reactor-http-nio-4] DEBUG\r2022-08-11 14:15:25.326 [gateway-server,,] [reactor-http-nio-2] DEBUG\r2022-08-11 14:15:25.339 [gateway-server,95aa725089b757f8,95aa725089b757f8] [reactor-http-nio-4] DEBUG\r2022-08-11 14:15:25.692 [gateway-server,95aa725089b757f8,95aa725089b757f8] [reactor-http-nio-2] DEBUG\r2022-08-11 14:15:26.242 [gateway-server,,] [PollingServerListUpdater-o] DEBUG\r根据以上数据结构创建 t_sleuth_log 表。\rCREATE TABLE IF NOT EXISTS t_sleuth_log (\rdatetime string,\rtrace_id string,\rprocess string,\rlog_level string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39; \u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r最后执行以下语句将 singer.txt 文件的数据载入 t_singer 表。执行 LOAD DATA 语句后 singer.txt 文件将会被移动到 t_singer 表所在路径下（ /hive/warehouse/t_singer/singer.txt ）。\rLOAD DATA INPATH \u0026#39;/yjx/gateway-server.log\u0026#39; INTO TABLE t_sleuth_log;\r查询结果如下：SELECT * FROM t_sleuth_log; 解决方案\n方案一：替换分隔符。使用 MR 程序提前将数据清洗一遍（将多字节分隔符替换为单字节分隔符）。\r@Override\rprotected void map(LongWritable key, Text value, Context context)\rthrows IOException, InterruptedException {\r// 将多字节分隔符替换为单字节分隔符\rString line = value.toString().replaceAll(\u0026#34;\\\\|\\\\|\u0026#34;, \u0026#34;|\u0026#34;);\r// 写出数据\rcontext.write(new Text(line), NullWritable.get());\r}\r提示：如果是小文件直接写个 Java 程序或者使用文本工具自带的替换功能就可以搞定，如果是大文件建议使用 MR\r程序来进行处理 方案二：RegexSerDe。Hive 内置了很多的 SerDe 类，可以使用 RegexSerDe 正则序列化器来处理。推荐使用。\r官方文档：https://cwiki.apache.org/confluence/display/Hive/SerDe\tBuilt-in SerDes：Avro (Hive 0.9.1 and later)；ORC (Hive 0.11 and later)；RegEx；\rThrift；Parquet (Hive 0.13 and later)；CSV (Hive 0.14 and later)；JsonSerDe (Hive 0.12 and later in hcatalog-core)\r创建 t_singer 表并使用 RegexSerDe 序列化器\rCREATE TABLE IF NOT EXISTS t_singer (\rid string,\rname string,\rcountry string,\rprovince string,\rgender string,\rworks string\r)\rROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.RegexSerDe\u0026#39;\rWITH SERDEPROPERTIES (\u0026#34;input.regex\u0026#34; = \u0026#34;(\\\\d*)\\\\|\\\\|([\\\\u4e00-\\\\u9fa5\\\\w\\\\x20\\\\-]*)\\\\|\\\\|([\\\\u4e00-\r\\\\u9fa5\\\\w\\\\x20\\\\-]*)\\\\|\\\\|([\\\\u4e00-\\\\u9fa5\\\\w\\\\x20\\\\-]*)\\\\|\\\\|([\\\\u4e00-\\\\u9fa5\\\\w\\\\x20\\\\-]*)\\\\|\\\\|\r([\\\\u4e00-\\\\u9fa5\\\\w\\\\x20\\\\-]*)\u0026#34;);\r注意：RegexSerDe 采用 Java 标准。由于反斜杠是 Java String 类中的转义字符，因此必须使用双反斜杠来定义单反斜杠。例如，要定义 \\d ，必须在 Regex 中使用 \\\\d 。在 input.regex 中，以一个匹配组表示一个字段，例如 (\\d+) 。\r最后执行以下语句将 singer.txt 文件的数据载入 t_singer 表。执行 LOAD DATA 语句后 singer.txt 文件将会被移动到 t_singer 表所在路径下（ /hive/warehouse/t_singer/singer.txt ）。\rLOAD DATA INPATH \u0026#39;/yjx/singer.txt\u0026#39; INTO TABLE t_singer;\r查询结果如下：SELECT * FROM t_singer;\r创建 t_sleuth_log 表并使用 RegexSerDe 序列化器\rCREATE TABLE IF NOT EXISTS t_sleuth_log (\rdatetime string,\rtrace_id string,\rprocess string,\rlog_level string\r)\rROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.RegexSerDe\u0026#39;\rWITH SERDEPROPERTIES (\u0026#34;input.regex\u0026#34; = \u0026#34;([\\\\d\\\\x20-:\\\\.]*) (.*) (.*) ([a-zA-Z]*)\u0026#34;);\r最后执行以下语句将 singer.txt 文件的数据载入 t_singer 表。执行 LOAD DATA 语句后 singer.txt 文件将会被移动到 t_singer 表所在路径下（ /hive/warehouse/t_singer/singer.txt ）。\rLOAD DATA INPATH \u0026#39;/yjx/gateway-server.log\u0026#39; INTO TABLE t_sleuth_log;\r查询结果如下：SELECT * FROM t_sleuth_log; 方案三：自定义 InputFormat 继承 TextInputFormat，在 RecordReader 中对分割字符进行处理。将自定义的 InputFormat 类打成 jar 包，例如 MyInputFormat.jar 。将 MyInputFormat.jar 放到 hive/lib 目录中或者使用 Hive 的 add jar 命令，然后就可以建表了。由于 Hive 是基于Hadoop 集群运行的，所以 hadoop/lib 目录中也必须放入 MyInputFormat.jar。\r假设你的 InputFormat 类路径是 com.yjxxt.hive.input ，则建表语句为：\rCREATE TABLE IF NOT EXISTS t_sleuth_log (\rid int, name stirng, ...\r)\r-- 指定分隔符\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39; \u0026#39;\r-- 指定自定义的 InputFormat\rSTORED AS INPUTFORMAT \u0026#39;com.yjxxt.hive.input\u0026#39;\rOUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\u0026#39;;\rHiveIgnoreKeyTextOutputFormat 是系统自带的 OUTPUTFORMAT 类，也支持自定义。 表详情 # 显示所有数据表。\tSHOW TABLES;\r可以通过 like 进行过滤。\tSHOW TABLES LIKE \u0026#39;t*\u0026#39;;\r查看某个数据表的详情。\tDESC t_person;\rDESC FORMATTED t_person;\rDESCRIBE FORMATTED t_person; 重命名 # 内部表会同时修改文件目录，外部表因为目录是共享的，所以不会修改目录名称。\rALTER TABLE old_table_name RENAME TO new_table_name; 修改列 # -- 添加列\rALTER TABLE table_name ADD COLUMNS (new_col INT);\r-- 一次增加一个列(默认添加为最后一列)\rALTER TABLE table_name ADD COLUMNS (new_col INT);\r-- 可以一次增加多个列\rALTER TABLE table_name ADD COLUMNS (c1 INT, c2 STRING);\r-- 添加一列并增加列字段注释\rALTER TABLE table_name ADD COLUMNS (new_col INT COMMENT \u0026#39;a comment\u0026#39;);\r-- 更新列\rALTER TABLE table_name CHANGE old_col new_col STRING;\r-- 将列 a 的名称更改为 a1\rALTER TABLE table_name CHANGE a a1 INT;\r-- 将列 a1 的名称更改为 a2，将其数据类型更改为字符串，并将其放在列 b 之后\rALTER TABLE table_name CHANGE a1 a2 STRING AFTER b;\r-- 将 c 列的名称改为 c1，并将其作为第一列\rALTER TABLE table_name CHANGE c c1 INT FIRST 清空表 # TRUNCATE TABLE table_name;\r注意：清空表只能删除内部表的数据（HDFS 文件），不能删除外部表中的数据。\r在 SQL 中，TRUNCATE 属于 DDL 语句，主要功能是彻底删除数据，使其不能进行回滚。 删除表 # DROP TABLE table_name;\r注意：删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 内外部表 # 内部表 # ​\t内部表（managed table）即Hive管理的表，Hive内部表的管理既包含逻辑以及语法上的，也包含实际物理意义上的，即创建Hive内部表时，数据将真实存在于表所在的目录内，删除内部表时，物理数据也和文件一并删除。默认创建的是内部表。\n分析以下语句\r-- 创建表\rCREATE TABLE IF NOT EXISTS test.t_user(\rid int,\rusername string,\rpassword string,\rgender string,\rage int\r)\rROW FORMAT DELIMITED FILEDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r——载入数据\rLOAD DATA INPATH \u0026#39;/yjx/user/user.txt\u0026#39; INTO TABLE test.t_user;\r执行LOAD DATA语句后，user.txt文件会被移动到t_user表所在路径下（ /hive/warehouse/test.db/t_user/user.txt ）。此时如果使用DROP语句删除t_user后，其数据和表的元数据都会被删除 外部表 # ​\t外部表（external table），仅仅只是在逻辑和语法意义上的，即新建表只是指向一个外部目录。同样，删除时不物理删除外部目录，仅仅是将引用和定义删除。\n​\t一般情况下，企业内部都是使用外部表，因为会有多人操作数据仓库，坑你产生数据表误删操作，为了数据安全性，一般使用外部表，方便达到数据共享。外部表数据创建和删除完全由自己控制，Hive不管理这些数据。\n方案一：创建时指定数据位置*\r数据的位置可以在创建时指定，在 Hive 的数据仓库 /hive/warehouse/test.db 目录下不会创建 t_user 目录。\rCREATE EXTERNAL TABLE IF NOT EXISTS test_t_user(\rid int,\rusername string,\rpassword string,\rgender string,\rage int\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;\r-- 可以指定到某个目录（该目录下的文件都会被扫描到），也可以指定到某个具体的文件\rLOCATION \u0026#39;/yjx/user\u0026#39;;\r提示：如果 /yjx/user 目录不存在 Hive 会帮我们自动创建，我们只需要将 t_user 表所需的数据上传\r至 /yjx/user 目录即可。或者数据已经存在于 HDFS 某个目录，Hive 创建外部表时直接指定数据位置即可。 方案二：先建表再导入.也可以先单独建立外部表，建立时不指定数据位置，然后通过 LOAD DATA 命令载入数据。\rCREATE EXTERNAL TABLE IF NOT EXISTS text.t_user2(\rid int,\rusername string,\rpassword string,\rgender string,\rage int\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r在这种情况下，Hive的数据仓库 /hive/warehouse/test.db目录下会创建t_user2目录，然后通过LOAD DATA命令载入数据。\rLOAD DATA INPATH \u0026#39;/yjx/user/user.txt\u0026#39; INTO TABLE text.t_user2；\r注意：在这种情况下，执行 LOAD DATA 语句后 user.txt 文件将会被移动到 t_user2 表所在路径下\r（ /hive/warehouse/test.db/t_user2/user.txt ）。唯一的区别是，外部表此时如果使用 DROP 语句删除\rt_user2 后，只会删除元数据，也就是说 t_user2 目录和 user.txt 数据并不会被删除。 载入数据 # 基本语法：\rLOAD DATA [LOCAL] INPATH \u0026#39;datapath\u0026#39; [OVERWRITE] INTO TABLE student [PARTITION (partcol1=val1, ...)];\rLOAD DATA：加载数据；\r[LOCAL]：本地，不加则从HDFS中获取\rINPATH：数据的路径\r\u0026#39;datapath\u0026#39;：具体的路径，要参考本地还是HDFS\r[OBERWRITE]：覆盖，不加则是追加数据\rINTO TABLE：加入到表\rstudent：表名\r[PARTITION (partcol1=val1, ...)]分区\r加载本地数据：LOAD DATA LOCAL INPATH \u0026#39;/root/user.txt\u0026#39; INTO TABLE t_user;\r要加载的文件必须和 HiveServer2 在同一个节点，否则会报错： SemanticException Line 1:23 Invalid path\u0026#39;\u0026#39;/root/test.txt\u0026#39;\u0026#39;: No files matching path file\r加载 HDFS 数据：LOAD DATA INPATH \u0026#39;/yjx/user.txt\u0026#39; INTO TABLE t_user;\r加载并覆盖已有数据：LOAD DATA INPATH \u0026#39;/yjx/user.txt\u0026#39; OVERWRITE INTO TABLE t_user; 通过查询插入数据：\r-- 创建表\rCREATE TABLE t_user1 (\rid int,\rusername string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r-- 创建表\rCREATE TABLE t_user2 (\rid int,\rpassword string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;-\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r-- 插入查询结果\r-- 将查询结果插入一张表\r-- 追加\rINSERT INTO t_user1 SELECT id, username FROM t_user;\r-- 覆盖\rINSERT OVERWRITE TABLE t_user1 SELECT id, username FROM t_user;\rINSERT OVERWRITE TABLE t_user2 SELECT id, password FROM t_user;\r-- 将查询结果一次性存放到多张表（多重模式）\r-- 追加\rFROM t_user\rINSERT INTO t_user1 SELECT id, username\rINSERT INTO t_user2 SELECT id, password;\r-- 覆盖\rFROM t_user\rINSERT OVERWRITE TABLE t_user1 SELECT id, username\rINSERT OVERWRITE TABLE t_user2 SELECT id, password; 导出数据 # 通过SQL操作 # 将查询结果导出到本地 # 首先在 HiveServer2 的节点上创建一个存储导出数据的目录。\rmkdir -p /root/user\r执行以下命令将查询结果导出到本地。\r-- 将查询结果导出到本地\rINSERT OVERWRITE LOCAL DIRECTORY \u0026#39;/root/user\u0026#39; SELECT * FROM t_user;\r导出结果如下：\r[root@node01 ~]# cat /root/user/000000_0\r1admin123456男18\r2zhangsanabc123男23\r3lisi654321女16\r按指定的格式将数据导出到本地\r首先在 HiveServer2 的节点上创建一个存储导出数据的目录\rmkdir -p /root/person\r执行以下命令将查询结果按指定的格式导出到本地\r-- 按指定的格式将数据导出到本地\rINSERT OVERWRITE LOCAL DIRECTORY \u0026#39;/root/person\u0026#39;\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rCOLLECTION ITEMS TERMINATED BY \u0026#39;-\u0026#39;\rMAP KEYS TERMINATED BY \u0026#39;:\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;\rSELECT * FROM t_person;\r导出结果如下：\r[root@node01 ~]# cat /root/person/000000_0\rsongsong,bingbing-lili,xiao song:18-xiaoxiao song:19,hui long guan-beijing\ryangyang,caicai-susu,xiao yang:18-xiaoxiao yang:19,chao yang-beijing 查询结果输出到HDFS # 首先在 HDFS 上创建一个存储导出数据的目录。\rhdfs dfs -mkdir -p /yjx/export/user\r执行以下命令将查询结果导出到 HDFS。-- 将查询结果导出到 HDFS\rINSERT OVERWRITE DIRECTORY \u0026#39;/yjx/export/user\u0026#39;\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rSELECT * FROM t_user;\r导出结果如下 /yjx/export/user/000000_0 ：\r1,admin,123456,男,18\r2,zhangsan,abc123,男,23\r3,lisi,654321,女,16 通过HDFS操作 # 首先在 HDFS 上创建一个存储导出数据的目录。\rhdfs dfs -mkdir -p /yjx/export/person\r使用 HDFS 命令拷贝文件到其他目录\rhdfs dfs -cp /hive/warehouse/t_person/* /yjx/export/person 将元数据和数据同时导出 # 首先在 HDFS 上创建一个存储导出数据的目录。\rhdfs dfs -mkdir -p /yjx/export/person\r将元数据和数据同时导出。\r-- 将表结构和数据同时导出\rEXPORT TABLE t_person TO \u0026#39;/yjx/export/person\u0026#39;;\r注意：时间不同步，会导致导入导出失败。 基本查询 # Hive高级 # 分区/分桶 # 数据模型 # Hive 的数据模型主要有以下四种：单独一个表；对表分区；对表分区再分桶；对表直接分桶。\r在大数据中，最常见的一种思想就是分治，我们可以把大文件切割成一个个的小文件，这样每次操作小文件时就会容\r易许多。同样的道理，在 Hive 中也是支持的，我们可以把大的数据，按照每天或者每小时切分成一个个的小文件，这样\r去操作小文件就会容易许多，这就是分区、分桶的意思。 分区 # 使用分区技术，可以避免Hive全表扫描，提升查询效率，同时能够减少数据冗余进而提高特定分区查询分析的效率\r注意，逻辑上分区表和未分区表没有区别，物理上分区表会将数据按照分区键的键值对存储在表目录的子目录中，目录名为“分区间=键值”。可以把建立分区想象成建立文件夹，把一些相似的数据存放在文件夹中。\r使用分区表时，尽量利用分区字段进行查询，如果不是用分区字段查询就会全扫描，这样就失去了分区的意义。 静态分区 # ​\t分区表分为静态分区和动态分区，区别在于前者是手动指定，后者是通过数据来判断分区。根据分区的深度又分为单分区和多分区\n单分区 # 创建静态分区表语法（静态分区和动态分区的建表语句是一样的）：\r-- 单分区：创建分区表 PARTITIONED BY (分区字段名 分区字段类型)\r-- 多分区：创建分区表 PARTITIONED BY (分区字段名 分区字段类型, 分区字段名2 分区字段类型2)\rCREATE TABLE IF NOT EXISTS t_student (\rsno int,\rsname string\r) PARTITIONED BY (grade int)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;;\r注意： PARTITIONED BY () 括号中指定的分区字段名不能和表中的字段名一样。\r载入数据：LOAD DATA INPATH \u0026#39;/yjx/s1.txt\u0026#39; INTO TABLE t_student PARTITION (grade=1);\r查询数据：SELECT * FROM t_student WHERE grade=1;\r需要注意的是，分区查询会将分区中所有的数据都查询出来，所以如果文件中的数据本身已经出问题了，那么查询的\r结果也会出问题，请看以下案例。\r添加分区（也可以在载入数据时添加分区）：\rALTER TABLE t_student ADD IF NOT EXISTS PARTITION (grade=1);\r查看分区：SHOW PARTITIONS t_student;\r删除分区：\r-- ALTER TABLE 表名 DROP PARTITION(分区字段名=键值)\rALTER TABLE t_student DROP PARTITION (grade=1); 多分区 # 创建静态分区表语法（静态分区和动态分区的建表语句是一样的）：\r-- 单分区：创建分区表 PARTITIONED BY (分区字段名 分区字段类型)\r-- 多分区：创建分区表 PARTITIONED BY (分区字段名 分区字段类型, 分区字段名2 分区字段类型2)\rCREATE TABLE IF NOT EXISTS t_teacher (\rtno int,\rtname string\r) PARTITIONED BY (grade int, clazz int)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;;\r注意：前后两个分区的关系为父子关系，也就是 grade 文件夹下面有多个 clazz 子文件夹。\r分区表载入数据：\rLOAD DATA INPATH \u0026#39;/yjx/t1.txt\u0026#39; INTO TABLE t_teacher PARTITION (grade=1, clazz=1);\rLOAD DATA INPATH \u0026#39;/yjx/t2.txt\u0026#39; INTO TABLE t_teacher PARTITION (grade=1, clazz=2);\rLOAD DATA INPATH \u0026#39;/yjx/t3.txt\u0026#39; INTO TABLE t_teacher PARTITION (grade=1, clazz=3);\rLOAD DATA INPATH \u0026#39;/yjx/t4.txt\u0026#39; INTO TABLE t_teacher PARTITION (grade=2, clazz=1);\rLOAD DATA INPATH \u0026#39;/yjx/t5.txt\u0026#39; INTO TABLE t_teacher PARTITION (grade=2, clazz=2);\r也可以使用分区表的 INSERT 语句插入数据（会执行 MR 任务）：\rINSERT INTO TABLE t_teacher PARTITION (grade=2, clazz=3) VALUES (10, \u0026#39;jueyuan10\u0026#39;);\r查询数据：SELECT * FROM t_teacher WHERE grade=1 AND clazz=1;\r添加分区：ALTER TABLE t_teacher ADD IF NOT EXISTS PARTITION (grade=1, clazz=1);\r查看分区：SHOW PARTITIONS t_teacher;\r删除分区：\r-- ALTER TABLE 表名 DROP PARTITION(分区字段名=键值)\rALTER TABLE t_teacher DROP PARTITION (grade=1, clazz=1); 动态分区 # ​\t主要区别在于静态分区是手动指定，静态分区是通过数据来判断分区。静态分区实在编译时期通过用书传递来决定；动态分区只有在SQL执行时才能决定。\n开启动态分区首先要在 Hive 会话中设置以下参数：\r-- 开启动态分区支持（默认 true）\rSET hive.exec.dynamic.partition=true;\r-- 是否允许所有分区都是动态的，strict 要求至少包含一个静态分区列，nonstrict 则无此要求（默认 strict）\rSET hive.exec.dynamic.partition.mode=nonstrict;\r其余参数的详细配置如下：\r-- 每个 Mapper 或 Reducer 可以创建的最大动态分区个数（默认为 100）\r-- 比如：源数据中包含了一年的数据，如果按天分区，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错\rhive.exec.max.dynamic.partition.pernode=100;\r-- 一个动态分区创建可以创建的最大动态分区个数（默认为 1000）\rhive.exec.max.dynamic.partitions=1000;\r-- 全局可以创建的最大文件个数（默认为 100000）\rhive.exec.max.created.files=100000;\r-- 当有空分区产生时，是否抛出异常（默认为 false）\rhive.error.on.empty.partition=false;\r-- 是否开启严格模式 strict（严格模式）和 nostrict（非严格模式，默认）\rhive.mapred.mode=nostrict; 严格模式：\rHive通过参数hive.mapred.omde来设置是否开启严格模式。参数只有两个：strict严格模式和默认的nostrict非严格模式。\r开启严格模式，主要是为了禁止某些查询（这些船可能会造成意想不到的结果），目前主要禁止三种类型：\r分取差表时必须在WHERE语句后面指定分区段，否则不予魂虚限制性。 数据抽样 # 事务 # 原因 # Hive在设计之初，是不支持事务操作的，因为Hive的核心目标是将已存在的结构化数据文件映射成表，然后提供基于表的SQL分析处理器；是一款面向分析的工具。并且映射的文件存在HDFS中，其本身也不支持随机修改文件的数据。这个定位意味着HQL起初不支持UPDATE、DELETE语法，也就没有事务支持。\r从 Hive 0.14 版本开始，引入了事务特性，能够在 Hive 表上实现 ACID 语义，包括 INSERT/UPDATE/DELETE/MERGE 语句，以解决缓慢变化维表或部分数据不正确，需要更正的情况。Hive 3.0 又对该特性进行了优化，包括改进了底层的文件组织方式，减少了对表结构的限制，以及支持谓词下推和向量化查询。\r最终 Hive 支持了具有 ACID 语义的事务，但做不到和传统关系型数据库那样的事务级别，仍有很多局限如：\r1、不支持BEGIN、COMMIT、ROLLBACK所有操作自动提交\r2、事务表仅支持ORC文件格式\r3、标参数transactional必须为true\r4、外部表不能成为ACID表，不允许非ACID会话读取/写入表\r5、默认事务关闭，需要额外配置 实践 # 事务功能相关参数如下：\r# 开启 hive 并发\rSET hive.support.concurrency=true;\r# 配置事务管理类\rSET hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\r# 如果事务表配合分区分桶一起使用建议开启以下参数\r# 开启分桶功能\rSET hive.enforce.bucketing=true;\r# 启用自动压缩\rSET hive.compactor.initiator.on=true;\r# 这里的压缩线程数必须大于 0，理想状态和分桶数一致\rSET hive.compactor.worker.threads=2;\r# 是否允许所有分区都是动态的，strict 要求至少包含一个静态分区列，nonstrict 则无此要求（默认 strict）\rSET hive.exec.dynamic.partition.mode=nonstrict; 创建事务表：\rCREATE TABLE IF NOT EXISTS test.t_user(\rid int,\rname string,\rage int\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY\rSTORED AS ORC\rTBLPROPERTIES(\u0026#34;transactional\u0026#34;=\u0026#34;true\u0026#34;);\r-- 事务表并分区分桶\rCREATE TABLE IF NOT EXISTS test.t_user(\rid int,\rname string,\rage int\r)\rPARTITIONED BY (time string)\rCLUSTERED BY (id) INTO 2 BUCKETS\rROW FORMAT DELIMITED FIELDS TERMINATED BY\rSTORED AS ORD\r-- 开启事务\rTBLPROPERTIES(\u0026#34;transactional\u0026#34;=\u0026#34;true\u0026#34;); 测试：\rINSERT INTO test.t_user VALUES (1，\u0026#34;张三\u0026#34;,18),(2，\u0026#34;张三\u0026#34;,18),(3，\u0026#34;张三\u0026#34;,18);\r因为使用的是事务表，所以写入数据时，INSERT 语句会在一个事务中运行。它会创建名为 delta 的目录，存放事务的信息和表的数据。\r目录名称的格式为 delta_minWID_maxWID_stmtID ，即 delta 前缀、写事务的 ID 范围、以及语句 ID。\r所有INSERT语句都会创建delta目录。UPDATE语句也会创建delta目录，但会先创建一个delete目录，即先删除后插入。delete目录的前缀是delete_delta。Hive会为所有的事务生成一个全局唯一的ID，包括读写操作。针对写事务，Hive还会创建一个写事物ID（WriteID），该ID在表范围内唯一。写事务ID会编码到delta和delete目录的名称中。语句ID（StatementID）则是当一个事务中有多条写入语句时使用，作为唯一标识。\r测试 UPDATE：UPDATE test.t_user SET name = \u0026#34;老张\u0026#34;, age = 58 WHERE id = 1;\r测试 DELETE：DELETE FROM test.t_user WHERE id = 1; 压缩 # 随着写操作的积累，表中的 delta 和 delete 文件会越来越多。事务表的读取过程中需要合并所有文件，数量一多势必会影响效率。此外，小文件对 HDFS 这样的文件系统也是不够友好的。因此，Hive 引入了压实（Compaction）的概念，分为 Minor 和 Major 两类。\rMinor Compaction会将金所有的delta文件压实为一个文件，delete也压实为一个。压实后的结果文件名中会包含写事务ID范围，同事省略掉语句ID。压实过程是在 Hive Metastore 中运行的，会根据一定阈值自动触发。我们也可以使用如下语句人工触发：\rALTER TABLE test.t_user COMPACT \u0026#39;minor\u0026#39;;\rMajor Compaction会将所有文件合并为一个文件，以base_N的形式命名，其中 N 表示最新的写事务 ID。已删除的数据将在这个过程中被剔除。需要注意的是，在 Minor 或 Major Compaction 执行之后，原来的文件不会被立刻删除。这是因为删除的动作是在另一个名为 Cleaner 的线程中执行的。因此，表中可能同时存在不同事务 ID 的文件组合，这在读取过程中需要做特殊处理。 索引 # 提示：Hive 3.0.0 版本中索引已被废弃\r原理\rHive 在指定列上建立索引时，会产生一张索引表（Hive 的一张物理表），里面的字段包括，索引列的值、该值对应的HDFS 文件路径、该值在文件中的偏移量。\r在执行索引字段查询时候，首先额外生成一个 MapReduce Job，根据对索引列的过滤条件，从索引表中过滤出索引列对应的 HDFS 文件路径及偏移量，输出到 HDFS 上的一个文件中，然后根据这些文件中的 HDFS 路径和偏移量，筛选原始Input 文件，生成新的 Split，作为整个 Job 的 Split，达到不用全表扫描的目的。\rHive 索引使用过程繁杂，而且性能一般，在 Hive 3.0 中已被移除，在工作环境中不推荐优先使用，在分区数量过多或查询字段不是分区字段时，索引可以作为补充方案同时使用。推荐使用 ORC 文件格式的索引类型进行查询。\r提示：工作中优先考虑使用物化视图和列式存储文件格式来加快查询速度，大表则分区分桶，使用 SMB Join。\r废弃\r由于 Hive 是针对海量数据存储的，创建索引需要占用大量的空间，最主要的是 Hive 索引无法自动进行刷新，也就是当新的数据加入时候，无法为这些数据自动加入索引；\rHive 索引使用过程繁杂，且性能一般；\r在可以预见到分区数据非常庞大的情况下，分桶和索引常常是优于分区的。而分桶由于 SMB Join 对关联键要求严格，所以并不是总能生效；\rHive 的索引与关系型数据库中的索引并不相同，比如，Hive 不支持主键或者外键；\r很多时候会优先考虑使用物化视图和列式存储文件格式来加快查询速度，大表则分区分桶，使用 SMB Join。 视图/物化视图 # 补：SQL没敲\n视图 # ​\t视图是一个虚拟的表，只保存定义，不实际存储数据，实际查询的时候改写SQL去问实际的数据表。不同于直接操作数据表，视图是依据SELECT来创建的，所以操作视图会根据创建视图的SELECT语句生成一张虚拟的表，然后在表上做SQL操作。\n创建视图语法：\r-- 创建视图\rCREATE VIEW [IF NOT EXISTS] [db_name.]view_name (\u0026lt;列名1\u0026gt;, \u0026lt;列名2\u0026gt;, ...) AS \u0026lt;SELECT语句\u0026gt;;\r-- 查看视图\rSHOW TABLES;\r-- 查看某个视图\rDESC view_name;\r-- 查看某个视图详细信息\rDESC FORMATTED view_name;\r-- 修改视图\rALTER VIEW [db_name.]view_name AS \u0026lt;SELECT语句\u0026gt;;\r-- 删除视图\rDROP VIEW [IF EXISTS] [db_name.]view_name [, \u0026lt;视图名2\u0026gt;, ...];\r视图实践：\r-- 查询部门经理人中薪水最低的部门名称\r-- 不使用视图，推导过程\r-- 查询部门经理人，处理NULL和去重\rSELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr;\r-- 查询部门经理人的薪水和部门编号\rSELECT empno, sal, deptno FROM emp WHERE empno IN (\rSELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr\r);\r-- 查询部门经理人中的最低薪水\rSELECT empno, sal, deptno FROM emp WHERE empno IN (\rSELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr\r) ORDER BY sal LIMIT 1;\r-- 查询部门经理人中薪水最低的部门名称\rSELECT e.empno, e.sal, e.deptno, d.dname\rFROM (\rSELECT empno, sal, deptno FROM emp WHERE empno IN (\rSELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr\r) ORDER BY sal LIMIT 1\r) e\rINNER JOIN dept d ON e.deptno = d.deptno;\r-- 使用视图\r-- 创建视图：查询部门经理人，处理NULL和去重\rCREATE VIEW IF NOT EXISTS vw_emp_mgr (mgr) AS\rSELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr;\r-- 创建视图：查询部门经理人的薪水和部门编号\rCREATE VIEW IF NOT EXISTS vw_emp_mgr_sal (empno, sal, deptno) AS\rSELECT empno, sal, deptno FROM emp WHERE empno IN (\rSELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr\r);\r-- 创建视图：查询部门经理人中的最低薪水\rCREATE VIEW IF NOT EXISTS vw_emp_mgr_minsal (empno, minsal, deptno) AS\rSELECT empno, sal minsal, deptno FROM emp WHERE empno IN (\rSELECT mgr FROM emp WHERE mgr IS NOT NULL GROUP BY mgr\r) ORDER BY sal LIMIT 1;\r-- 使用视图查询部门经理人中薪水最低的部门名称\rSELECT e.empno, e.minsal, e.deptno, d.dname\rFROM vw_emp_mgr_minsal e\rINNER JOIN dept d ON e.deptno = d.deptno; 总结：\rHive 中的视图是一种虚拟表，只保存定义，不实际存储数据；\r通常从真实的物理表查询中创建生成视图，也可以从已经存在的视图上创建新视图；\r创建视图时，将冻结视图的结构，如果删除或更改基础表，则视图将失败；\r视图是用来简化操作的，不缓冲记录，也不会提高查询性能。\r优点：\r通过视图可以提高数据的安全性，将真实表中特定的列提供给用户，保护数据隐私；\r上层的业务查询和底层数据表解耦，业务上可以查的一张表，但是底层可能映射的是三张或多张表的数据；\r修改底层数据模型只需要重建视图即可，不需要上层业务修改业务逻辑；\r降低查询复杂度，优化查询语句（注意不是提高查询效率）。\r缺点：\r无法再对视图进行优化，而且并没有提升查询速度，只是使上层的业务逻辑变得更清晰简洁。 物化视图 # ​\t物化视图 Materialized View，是一个包括查询结果的数据库对象，可以用预先计算并保存表链接或聚集等耗时较多的操作结果。在执行查询时，可以避免进行这些耗时的操作，从而快速得到结果。\n​\t使用物化视图的目的就是通过预计算，提高查询性能，所以要占用一定的存储空间。Hive还提供了物化视图的查询自动重写（基于 Apache Calcite 实现）和物化视图存储选择机制，可以本地存储在 Hive，也可以通过用户自定义Storage Handlers 存储在其他系统（如 Apache Druid）。\n​\tHive引进物化视图的目的就是为了优化数据查询访问的效率，相当于从数据预处理的角度优化数据访问。\n​\t注意：物化事务图只可以在事务表上创建。\n物化视图语法：\r-- 创建物化视图\rCREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name\r[DISABLE REWRITE]\r[COMMENT materialized_view_comment]\r[PARTITIONED ON (col_name, ...)]\r[CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)]\r[\r[ROW FORMAT row_format]\r[STORED AS file_format] | STORED BY \u0026#39;storage.handler.class.name\u0026#39; [WITH SERDEPROPERTIES (...)]\r]\r[LOCATION hdfs_path]\r[TBLPROPERTIES (property_name=property_value, ...)]\rAS\r\u0026lt;query\u0026gt;;\r-- 物化视图是一种特殊的数据表，可以使用 SHOW TABLES 等语法\r-- 删除物化视图\rDROP MATERIALIZED VIEW [db_name.]materialized_view_name; 物化视图实践：\r-- 创建事务表\rCREATE TABLE IF NOT EXISTS test.emp (\rEMPNO int,\rENAME varchar(255),\rJOB varchar(255),\rMGR int,\rHIREDATE date,\rSAL decimal(10,0),\rCOMM decimal(10,0),\rDEPTNO int\r)\rSTORED AS ORC\rTBLPROPERTIES(\u0026#34;transactional\u0026#34;=\u0026#39;true\u0026#39;);\r-- 载入数据\rINSERT INTO test.emp SELECT * FROM scott.emp;\r-- 创建物化视图\rCREATE MATERIALIZED VIEW test.emp_analysis\rAS\rSELECT\rdeptno,\rCOUNT(*) cnt,\rAVG(sal) avg_sal,\rMAX(sal) max_sal,\rMIN(sal) min_sal\rFROM test.emp GROUP BY deptno;\r物化视图的创建过程会比较慢，因为 SELECT 查询执行的数据会自动落地。“自动”也即在 Query 的执行期间，任何用户对该物化视图是不可见的，执行完毕之后物化视图可用。默认情况下，创建好的物化视图可被用于查询优化器\rOptimizer 查询重写，在物化视图创建期间可以通过 Disable Rewrite 参数设置禁止使用。 刷新物化视图 # 增量刷新：\r当物化视图满足一定条件时，默认会执行增加刷新，即只刷新原始源表中的变动会影响到的数据，增量刷新会减少重\r建步骤的执行时间。要执行增量刷新，物化视图的创建语句和更新源表的方式都须满足一定条件：\r物化视图只使用了事务表；\r如果物化视图中包含 GROUP BY，则该物化视图必须存储在 ACID 表中，因为它需要支持 MERGE 操作。对于由 Scan-Project-Filter-Join 组成的物化视图，不存在该限制。\r定时刷新:\r可以通过 SET hive.materializedview.rewriting.time.window=10min; 设置定期刷新，默认为 0min。该参数也可以作为建表语句的一个属性，在建表时设置。\r全量刷新：\r若只用 INSERT 更新了源表数据，可以对物化视图进行增量刷新。若使用 UPDATE、INSERT 更新了源表数据，那么只\r能进行重建，即全量刷新(REBUILD)。\r当数据源变更（新数据插入 Inserted、数据被修改 Modified），物化视图也需要更新以保持数据一致性，需要用户主\r动触发 Rebuild，命令如下：ALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD;\r注意：如果一张表创建了许多物化视图，那么在数据写入这张表时，可能会消耗许多机器的资源，比如数据带宽占\r满、存储增加等等 查询重写 # 物化视图创建后即可用于相关查询的加速，并且优化器能够利用其定义语义来使用物化视图自动重写传入的查询，从\r而加快查询的执行速度。即：用户提交查询 Query，若该 Query 经过重写后可以命中已经存在的物化视图，则直接通过物化视图查询数据返回结果，以实现查询加速。\r物化视图是否开启重写查询功能可以通过全局参数控制 set hive.materializedview.rewriting=true; ，默认为true。除此之外，用户还可以选择性的控制指定的物化视图的查询重写机制，命令如下：\rALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE;\r查询重写案例：\r-- 创建事务表\rCREATE TABLE IF NOT EXISTS test.emp (\rEMPNO int,\rENAME varchar(255),\rJOB varchar(255),\rMGR int,\rHIREDATE date,\rSAL decimal(10,0),\rCOMM decimal(10,0),\rDEPTNO int\r)\rSTORED AS ORC\rTBLPROPERTIES(\u0026#34;transactional\u0026#34;=\u0026#39;true\u0026#39;);\r-- 载入数据\rINSERT INTO test.emp SELECT * FROM scott.emp;\r-- 创建事务表\rCREATE TABLE IF NOT EXISTS test.dept (\rDEPTNO int,\rDNAME varchar(255),\rLOC varchar(255)\r)\rSTORED AS ORC\rTBLPROPERTIES(\u0026#34;transactional\u0026#34;=\u0026#39;true\u0026#39;);\r-- 载入数据\rINSERT INTO test.dept SELECT * FROM scott.dept;\r假设我们要经常获取有关 1981 年之后按不同时期聘用的员工及其部门的信息，我们可以创建以下物化视图：\rCREATE MATERIALIZED VIEW test.mv1\rAS\rSELECT empno, dname, hiredate\rFROM test.emp AS e\rINNER JOIN test.dept AS d ON e.deptno = d.deptno\rWHERE hiredate \u0026gt;= \u0026#39;1981-01-01\u0026#39;;\r然后，以下查询提取有关 1982 年第一季度雇用的员工的信息（该查询会触发查询重写）：\rSELECT empno, dname\rFROM test.emp AS e\rINNER JOIN test.dept AS d ON e.deptno = d.deptno\rWHERE hiredate \u0026gt;= \u0026#39;1982-01-01\u0026#39; AND hiredate \u0026lt;= \u0026#39;1982-03-31\u0026#39;;\r在实际查询时，Hive 将使用物化视图重写传入的查询，包括实例化扫描之上的补偿谓词。查询重写的等价 SQL 如下：\rSELECT empno, dname\rFROM test.mv1\rWHERE hiredate \u0026gt;= \u0026#39;1982-01-01\u0026#39; AND hiredate \u0026lt;= \u0026#39;1982-03-31\u0026#39;; 高级查询 # :补SQL\n行转列——一行变多行 # 创建表：\rCREATE TABLE IF NOT EXISTS t_movie1 (\rid int,\rname string,\rtypes string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r载入数据：\rLOAD DATA INPATH \u0026#39;/yjx/movie1.txt\u0026#39; INTO TABLE t_movie1;\rEXPLODE爆发\rEXPLODE() 可以将 Hive 一行中复杂的 Array 或者 Map 结构拆分成多行，那如何将某个列的数据转为数组呢？可以配置 SPLIT 函数一起使用。\rSELECT EXPLODE(SPLIT(types, \u0026#34;-\u0026#34;)) FROM t_movie1;\r如果我还想查看一下数组中这些电影类型隶属于哪个电影，需要配合侧视图 LATERAL VIEW 一起使用。\r-- movie_type 是侧视图别名\rSELECT id, name, type\rFROM t_movie1,\r-- 生成侧视图（表）AS 后面是侧视图的字段\rLATERAL VIEW EXPLODE(SPLIT(types, \u0026#34;-\u0026#34;)) movie_type AS type; 列转行——多行变一行 # 创建表：\rCREATE TABLE IF NOT EXISTS t_movie2 (\rid int,\rname string,\rtype string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r载入数据：LOAD DATA INPATH \u0026#39;/yjx/movie2.txt\u0026#39; INTO TABLE t_movie2;\rCOLLECT_SET/COLLECT_LIST\rCOLLECT_SET() 和 COLLECT_LIST() 可以将多行数据转成一行数据，区别就是 LIST 的元素可重复而 SET 的元素是去重的。\rSELECT id, name,\rCONCAT_WS(\u0026#39;:\u0026#39;, COLLECT_SET(type)) AS type_set,\rCONCAT_WS(\u0026#39;:\u0026#39;, COLLECT_LIST(type)) AS type_list\rFROM t_movie2\rGROUP BY id, name;\rMySQL 实现方式： GROUP_CONCAT([DISTINCT] 要连接的字段 [ORDER BY 排序字段 ASC/DESC] [SEPARATOR\r‘分隔符’]) URL解析 # 侧视图 LATERAL VIEW 配合 PARSE_URL_TUPLE 函数可以实现 URL 字段的一列变多列。\r创建表：\rCREATE TABLE IF NOT EXISTS t_mall (\rid int,\rname string,\rurl string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;;\r载入数据：LOAD DATA INPATH \u0026#39;/yjx/mall.txt\u0026#39; INTO TABLE t_mall;\r实践：\rSELECT * FROM t_mall;\rSELECT PARSE_URL_TUPLE(url, \u0026#39;PROTOCOL\u0026#39;, \u0026#39;HOST\u0026#39;, \u0026#39;PATH\u0026#39;, \u0026#39;QUERY\u0026#39;) FROM t_mall;\rSELECT a.id, a.name, b.protocol, b.host, b.path, b.query\rFROM t_mall a,\rLATERAL VIEW\rPARSE_URL_TUPLE(url, \u0026#39;PROTOCOL\u0026#39;, \u0026#39;HOST\u0026#39;, \u0026#39;PATH\u0026#39;, \u0026#39;QUERY\u0026#39;) b AS protocol, host, path, query; JSON 解析 # ​\tJSON数据格式是数据存储及数据处理中最常见的结构化数据格式之一，一般公司会将数据以JSON格式存储在HDFS中，当构建数据仓库是时，需要对JSON格式的税局进行处理和分析，那么就需要在Hive中对JSON格式的数据进行解析读取\n​\tHive提供了两种解析JSON数据的方式，可以根据不同需求选择合适的方式对JSON数据进行处理。\n使用JSON函数处理：\rGET_JSON_OBJECT(json_txt,path)\r第一个参数：指定要解析的 JSON 字符串\r第二个参数：指定要返回的字段，通过 $.column_name 的方式来指定\rJSON_TUPLE(jsonStr,p1,p2,...pn)\r第一个参数：指定要解析的 JSON 字符串\r第二个参数：指定要返回的第 1 个字段\r……\r使用 JsonSerDe：建表时指定 JSON 序列化器，加载 JSON 文件到表中时会自动解析为对应的表格式 JSON 函数\r创建表：\rCREATE TABLE IF NOT EXISTS t_user_json (\ruser_json string\r);\r载入数据：LOAD DATA INPATH \u0026#39;/yjx/user.json\u0026#39; INTO TABLE t_user_json;\rGET_JSON_OBJECT(JSON_TXT, PATH) 实践：\rSELECT * FROM t_user_json;\rSELECT\rGET_JSON_OBJECT(user_json, \u0026#39;$.id\u0026#39;) AS id,\rGET_JSON_OBJECT(user_json, \u0026#39;$.username\u0026#39;) AS username,\rGET_JSON_OBJECT(user_json, \u0026#39;$.gender\u0026#39;) AS gender,\rGET_JSON_OBJECT(user_json, \u0026#39;$.age\u0026#39;) AS age\rFROM t_user_json;\rJSON_TUPLE(jsonStr, p1, p2, ..., pn) 实践：\rSELECT\rJSON_TUPLE(user_json, \u0026#39;id\u0026#39;, \u0026#39;username\u0026#39;, \u0026#39;gender\u0026#39;, \u0026#39;age\u0026#39;) AS (id, username, gender, age)\rFROM t_user_json; JsonSerDe\t官方文档：https://cwiki.apache.org/confluence/display/Hive/SerDe\rHive 内置了很多的 SerDe 类，可以使用 JsonSerDe 序列化器来处理。建表时指定 JSON 序列化器，加载 JSON 文件到表中时会自动解析为对应的表格式。\rBuilt-in SerDes：\rAvro (Hive 0.9.1 and later)\rORC (Hive 0.11 and later)\rRegEx\rThrift\rParquet (Hive 0.13 and later)\rCSV (Hive 0.14 and later)\rJsonSerDe (Hive 0.12 and later in hcatalog-core)\r创建 t_user_json2 表并使用 JsonSerDe 序列化器。\rCREATE TABLE IF NOT EXISTS t_user_json2 (\rid int,\rusername string,\rgender string,\rage int\r)\rROW FORMAT SERDE \u0026#39;org.apache.hive.hcatalog.data.JsonSerDe\u0026#39;\rSTORED AS TEXTFILE;\r载入数据：LOAD DATA INPATH \u0026#39;/yjx/user.json\u0026#39; INTO TABLE t_user_json2;\r实践：SELECT * FROM t_user_json2; 窗口函数 # 补：SQL\n定义 # ​\t官方文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics\n窗口函数是用于分析的一类函数，理解窗口函数要从聚合函数说起。聚合函数是将某列中多行的值合并为一行，比如SUM,COUNT等。这列函数往往无法单独与列一起查询。\r而窗口函数可为窗口中的每行都返回一个值。就是在查询的结果上多出一列，这一列可以使聚合的值，也可以是排序值。比如：SELECT ename, COUNT(*) OVER() FROM emp;\r窗口函数和GROUP BY聚合函数区别在于：窗口函数仅仅只会将结果附加到当前的结果上，不会对已有的行或列做任何修改。GROUP BY对于各个组它仅仅只会保留一行聚合结果。 窗口函数语法 # ​\t窗口函数是指OVER()函数，窗口是由一个OVER子句定义的多行记录。窗口函数分为三类：聚合型窗口函数、分析性窗口函数和去执行窗口函数\n语法：\rSELECT XX函数（）OVER（PARTITION BY 用于分组的列ORDER BY 用于排序的列ROWS/RANGE BETWEEN 开始位置AND结束位置）；\rXX函数() ：聚合型窗口函数/分析型窗口函数/取值型窗口函数\rOVER() ：窗口函数\rPARTITION BY ：后跟分组的字段，划分的范围被称为窗口\rORDER BY ：决定窗口范围内数据的排序方式\r移动窗口 ：\r移动方向：\rCURRENT ROW ：当前行\rPRECENDING ：向当前行之前移动\rFOLLOWING ：向当前行之后移动\rUNBOUNDED ：起点或终点（一般结合 PRECEDING，FOLLOWING 使用）\rUNBOUNDED PRECEDING ：表示该窗口第一行（起点）\rUNBOUNDED FOLLOWING ：表示该窗口最后一行（终点）\r移动范围： ROWS 和 RANGE 基本使用 # 如果 OVER 不提供分组方法，则将所有数据分为一组，如下：\rSELECT ename, deptno,\rAVG(sal) OVER()\rFROM emp;\r以上将所有员工的薪水进行了平均计算，然后显示在每行数据后边。\rPARTITION BY：PARTITION BY 的作用和 GROUP BY 是类似，用于分组。\rSELECT ename, deptno,\rAVG(sal) OVER(PARTITION BY deptno) AS avgsal\rFROM emp;\rORDER BY：\r在每个窗口（分组）内，如果我们想按每个人的薪水进行排序，可以使用 ORDER BY 子句，这里我们用 RANK() 指定序\r号，相同的薪水序号是一样的：\rSELECT ename, deptno, sal,\rRANK() OVER(PARTITION BY deptno ORDER BY sal) AS salorder\rFROM emp;\r当 ORDER BY 与聚合函数一起使用时，会形成顺序聚合，如 SUM 聚合与 ORDER BY 结合使用时，就实现类似于累计和的效果：\rSELECT ename, deptno, sal,\rSUM(sal) OVER(PARTITION BY deptno ORDER BY sal) AS sumsal\rFROM emp; 总结：\r与 GROUP BY 的区别：\r结果数据形式\t窗口函数可以在保留原表中的全部数据\rGROUP BY 只能保留与分组字段聚合的结果\r排序范围不同\r窗口函数中的 ORDER BY 只是决定着窗口里的数据的排序方式\r普通的 ORDER BY 决定查询出的数据以什么样的方式整体排序\rSQL 顺序\rGROUP BY 先进行计算\r窗口函数在 GROUP BY 后进行计算 移动窗口（滑动窗口） # ​\t有时候需要根据数据的前后重新分配窗口，比如在股票、气温数据等场景下，数据的前后会有影响，就适用移动窗口。\n移动方向：\rCURRENT ROW ：当前行\rPRECENDING ：向当前行之前移动\rFOLLOWING ：向当前行之后移动\rUNBOUNDED ：起点或终点（一般结合 PRECEDING，FOLLOWING 使用）\rUNBOUNDED PRECEDING ：表示该窗口第一行（起点）\rUNBOUNDED FOLLOWING ：表示该窗口最后一行（终点）\r移动范围：\rROWS ：ROWS 后定义窗口从哪里开始（当前行也参与计算），与 BETWEEN 搭配可以表示范围。如果省略 BETWEEN仅指定一个端点，那么将该端点视为起点，终点默认为当前行。ROWS 会根据 ORDER BY 子句排序后，按分组后排序列的顺序取前 N 行或后 N 行进行计算（当前行也参与计算）。物理行。\rROWS 2 PRECEDING ：窗口从当前行的前两行开始计算，计算到当前行；\rROWS BETWEEN 2 PRECEDING AND CURRENT ROW ：等同于上一句；\rROWS BETWEEN CURRENT ROW AND 2 FOLLOWING ：窗口从当前行开始计算，计算到当前行的后两行；\rROWS BETWEEN 2 PRECEDING AND 1 FOLLOWING ：窗口从当前行的前两行开始计算，计算到当前行的下一\r行，当前行也参与计算；\rROWS UNBOUNDED PRECEDING ：窗口从第一行（起点）计算到当前行；\rROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ：等同于上一句；\rROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING ：窗口从当前行计算到最后一行（终点）；\rROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING ：窗口从第一行（起点）计算到当前行下一行；\rROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ：窗口从第一行（起点）计算到最后一\r行（终点）。\rRANGE ：RANGE 后定义窗口从哪里开始（当前行也参与计算），与 BETWEEN 搭配可以表示范围。如果省略\rBETWEEN 仅指定一个端点，那么将该端点视为起点，终点默认为当前行。RANGE 会根据 ORDER BY 子句排序后，按\r分组后排序列的值的整数区间取前 N 行或后 N 行进行计算（相同排序值的行都会被算进来，当前行也参与计算）。\rRANGE 的窗口范围子句语法与 ROWS 一模一样，唯一的区别就在于 RANGE 会根据 ORDER BY 子句排序后，按分组后排序列的值的整数区间取前 N 行或后 N 行进行计算（相同排序值的行都会被算进来，当前行也参与计算）\r整数区间解释：如果窗口范围子句为 RANGE BETWEEN 2 PRECEDING AND CURRENT ROW ，假设排序列的值为\r1 2 3 4 5 7 8 11，计算规则如下： 1 =\u0026gt; 1，因为前面没有任何行，所以只有自己\r2 =\u0026gt; 1 + 2，因为前面只有一行，所以只加了 1\r3 =\u0026gt; 1 + 2 + 3，前面两行加当前行\r4 =\u0026gt; 2 + 3 + 4，前面两行加当前行\r5 =\u0026gt; 3 + 4 + 5，前面两行加当前行\r7 =\u0026gt; 5 + 6 + 7，因为 6 不存在，所以实际上只加了 5\r8 =\u0026gt; 6 + 7 + 8，因为 6 不存在，所以实际上只加了 7\r11 =\u0026gt; 9 + 10 + 11，因为 9 和 10 都不存在，所以实际上只有自己\r当 ORDER BY 缺少窗口范围子句时，窗口范围子句默认为： RANGE BETWEEN UNBOUNDED PRECEDING AND\rCURRENT ROW 。\r当 ORDER BY 和窗口范围子句都缺失时，窗口范围子句默认为： ROWS BETWEEN UNBOUNDED PRECEDING AND\rUNBOUNDED FOLLOWING 。 SELECT ename, deptno, sal,\r-- 统计所有人薪资\rSUM(sal) OVER() AS sumsal1,\r-- 按部门统计所有人薪资（范围字句默认为：ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING）\rSUM(sal) OVER(PARTITION BY deptno) AS sumsal2,\r-- 起点到终点的窗口聚合，和 sumsal2 结果一样\rSUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED\rFOLLOWING) AS sumsal3,\r-- 按部门统计所有人薪资，实现累计和的效果（范围字句默认为：RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT\rROW）\rSUM(sal) OVER(PARTITION BY deptno ORDER BY sal) AS sumsal4,\r-- 起点到当前行的窗口聚合，和 sumsal4 结果一样\rSUM(sal) OVER(PARTITION BY deptno ORDER BY sal RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS\rsumsal5,\r-- 起点到当前行的窗口聚合，为了让大家区别 ROWS 和 RANGE（观察 sumsal5 和 sumsal6 的结果）\rSUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS\rsumsal6,\r-- 前面一行和当前行的窗口聚合\rSUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS sumsal7,\r-- 前面一行和当前行和后面一行的窗口聚合\rSUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS sumsal8,\r-- 当前行到终点的窗口聚合\rSUM(sal) OVER(PARTITION BY deptno ORDER BY sal ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS\rsumsal9\rFROM emp; 分析性窗口函数 # RANK() ：间断，相同值同序号，例如 1、2、2、2、5。\rDENSE_RANK() ：不间断，相同值同序号，例如 1、2、2、2、3。\rROW_NUMBER() ：不间断，序号不重复，例如 1、2、3、4、5（2、3 可能是相同的值）。\rSELECT ename, deptno, sal,\rRANK() OVER(PARTITION BY deptno ORDER BY sal) AS salorder1,\rDENSE_RANK() OVER(PARTITION BY deptno ORDER BY sal) AS salorder2,\rROW_NUMBER() OVER(PARTITION BY deptno ORDER BY sal) AS salorder3\rFROM emp;\rPERCENT_RANK() ：计算小于当前行的值在所有行中的占比，类似百分比排名。可以用来计算超过了百分之多少的\r人。计算某个窗口或分区中某个值的累积分布。假定升序排序，则使用以下公式确定累积分布：小于 x 的行数 / 窗口\r或 PARTITION 分区内的总行数。其中 x 等于 ORDER BY 子句中指定的列的当前行中的值。\rCUME_DIST() ：计算小于等于当前行的值在所有行中的占比。\rNTILE(N) ：如果把数据按行数分为 N 份，那么该行所属的份数是第几份。注意：N 必须为 INT 类型。\rSELECT ename, deptno, sal,\rPERCENT_RANK() OVER(PARTITION BY deptno ORDER BY sal) AS percent_rank_sal,\rCUME_DIST() OVER(PARTITION BY deptno ORDER BY sal) AS cume_dist_sal,\rNTILE(3) OVER(PARTITION BY deptno ORDER BY sal) AS ntile_sal\rFROM emp; 取值型窗口函数 # LAG(COL, N, DEFAULT_VAL) ：往前第 N 行数据，没有数据的话用 DEFAULT_VAL 代替。\rLEAD(COL, N, DEFAULT_VAL) ：往后第 N 行数据，没有数据的话用 DEFAULT_VAL 代替。\rFIRST_VALUE(EXPR) ：分组内第一个值，但是不是真正意义上的第一个，而是截至到当前行的第一个。\rLAST_VALUE(EXPR) ：分组内最后一个值，但是不是真正意义上的最后一个，而是截至到当前行的最后一个。\rSELECT ename, deptno, sal,\rFIRST_VALUE(sal) OVER(PARTITION BY deptno ORDER BY sal) AS firstsal,\rLAST_VALUE(sal) OVER(PARTITION BY deptno ORDER BY sal) AS lastsal,\rLAG(sal, 2, 1) OVER(PARTITION BY deptno ORDER BY sal) AS lagsal,\rLEAD(sal, 2, -1) OVER(PARTITION BY deptno ORDER BY sal) AS leadsal\rFROM emp; 自定义函数 # ​\tHive 自身已经提供了非常丰富的函数，比如：COUNT/MAX/MIN 等，但是数量与功能有限，如果无法满足需求，可以通过自定义函数来进行扩展。自定义函数分为三种：\nUDF（User Defined Function）：普通函数，一进一出，比如 UPPER, LOWER UDAF（User Defined Aggregation Function）：聚合函数，多进一出，比如 COUNT/MAX/MIN UDTF（User Defined Table Generating Function）：表生成函数，一进多出，比如 LATERAL VIEW EXPLODE() 官方文档：https://cwiki.apache.org/confluence/display/Hive/HivePlugins（注意区分已过时的用法）\n创建项目 # 完整 pom.xml 文件如下：\r\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\r\u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34;\rxmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\rxsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-\r4.0.0.xsd\u0026#34;\u0026gt;\r\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;\r\u0026lt;groupId\u0026gt;com.yjxxt\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hive-demo\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt;\r\u0026lt;properties\u0026gt;\r\u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt;\r\u0026lt;maven.compiler.source\u0026gt;8\u0026lt;/maven.compiler.source\u0026gt;\r\u0026lt;maven.compiler.target\u0026gt;8\u0026lt;/maven.compiler.target\u0026gt;\r\u0026lt;!-- Hadoop 版本控制 --\u0026gt;\r\u0026lt;hadoop.version\u0026gt;3.3.4\u0026lt;/hadoop.version\u0026gt;\r\u0026lt;!-- Hive 版本控制 --\u0026gt;\r\u0026lt;hive.version\u0026gt;3.1.2\u0026lt;/hive.version\u0026gt;\r\u0026lt;/properties\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hadoop-hdfs\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hadoop.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;hive-exec\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${hive.version}\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;/project\u0026gt; 自定义 UDF # ​\t实现UDF的方式有两种，第一种是比较简单的形式，继承UDF类通过evaluate方法实现，目前已过时。第二种是继承GenericUDF重写initialize方法、evaluate方法、getDisplayString方法。这里是第二种\n将自定义 UDF 程序打成 jar 包并上传至 HDFS，例如： /yjx/hive-demo-1.0-SNAPSHOT.jar 。\r在 Hive 中定义自定义函数。\rCREATE FUNCTION HELLO_UDF AS \u0026#39;com.yjxxt.hive.udf.HelloUDF\u0026#39;\rUSING JAR \u0026#39;hdfs:///yjx/hive-demo-1.0-SNAPSHOT.jar\u0026#39;;\r重新加载函数。\r-- 重新加载函数\rRELOAD FUNCTIONS;\r-- 查看函数详细信息\rDESC FUNCTION EXTENDED HELLO_UDF;\r测试函数\rSELECT HELLO_UDF(ename) FROM emp LIMIT 5;\r移除函数。\rDROP [TEMPORARY] FUNCTION [IF EXISTS] [DBNAME.]FUNCTION_NAME;\r-- 移除函数\rDROP FUNCTION HELLO_UDF;\r-- 重新加载函数\rRELOAD FUNCTIONS;\r注意：客户端会缓存同名的类，会导致自定义函数定义失败，可以通过重启客户端来解决。 自定义 UDAF # ​\t实现 UDAF 的方式有两种，第一种是比较简单的形式，先继承 UDAF 类，然后使用静态内部类实现 UDAFEvaluator 接口，目前已过时。第二种是先继承 AbstractGenericUDAFResolver 类重写 getEvaluator 方法，然后使用静态内部类实现GenericUDAFEvaluator 接口。这里主要讲第二种。\ninit() ：初始化，一般负责初始化内部字段，通常初始化用来存放最终结果的变量。\riterate() ：处理每一条输入记录，每次对一个新的值进行聚合计算时都会调用该方法，一般会根据计算结果更新\r用来存放最终结果的变量。\rterminatePartial() ：终止部分，部分聚合结果的时候调用该方法，必须返回一个封装了聚合计算当前状态的对\r象。\rmerge() ：接受来自 terminatePartial 的返回结果，进行合并。Hive 合并两部分聚合的时候回调用这个方法。\rterminate() ：终止方法，返回最终聚合函数结果。\r// 确定各个阶段输入输出参数的数据格式 ObjectInspectors\rpublic ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException\r// 中间缓存的暂存结构，用于接收中间运行时需要暂存的变量数据\rpublic static abstract class AbstractAggregationBuffer implements AggregationBuffer\r// 保存数据聚集结果的类\rpublic abstract AggregationBuffer getNewAggregationBuffer() throws HiveException;\r// 重置聚集结果\rpublic abstract void reset(AggregationBuffer agg) throws HiveException;\r// Map 阶段，迭代处理 SQL 传过来的列数据\rpublic abstract void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException;\r// Map 与 Combiner 结束返回结果，得到部分数据聚集结果\rpublic abstract Object terminatePartial(AggregationBuffer agg) throws HiveException;\r// Combiner 合并 Map 返回的结果，还有 Reducer 合并 Mapper 或 Combiner 返回的结果\rpublic abstract void merge(AggregationBuffer agg, Object partial) throws HiveException;\r// Reduce 阶段，输出最终结果\rpublic abstract Object terminate(AggregationBuffer agg) throws HiveException;\r将自定义 UDAF 程序打成 jar 包并上传至 HDFS，例如： /yjx/hive-demo-1.0-SNAPSHOT.jar 。\r-- 重新加载函数\rRELOAD FUNCTIONS;\r-- 查看函数详细信息\rDESC FUNCTION EXTENDED FIELD_LEN;\r测试函数。\rSELECT deptno, COUNT(*) cnt, FIELD_LEN(ename) flen FROM emp GROUP BY deptno;\r移除函数。\rDROP [TEMPORARY] FUNCTION [IF EXISTS] [DBNAME.]FUNCTION_NAME;\r-- 移除函数\rDROP FUNCTION FIELD_LEN;\r-- 重新加载函数\rRELOAD FUNCTIONS;\r注意：客户端会缓存同名的类，会导致自定义函数定义失败，可以通过重启客户端来解决。 自定义 UDTF # ​\tUDTF（User Defined Table Generating Function）：表生成函数，一进多出，用于处理单行数据，并生成多个数据行。实现 UDF 需要继承的 GenericUDTF，然后重写父类的三个抽象方法，输出后有几列，在 initialize 中定义，主要处理逻辑在 process 中实现。\n将自定义 UDTF 程序打成 jar 包并上传至 HDFS，例如： /yjx/hive-demo-1.0-SNAPSHOT.jar\r在 Hive 中定义自定义函数。\rCREATE FUNCTION HELLO_UDTF AS \u0026#39;com.yjxxt.hive.udtf.HelloUDTF\u0026#39;\rUSING JAR \u0026#39;hdfs:///yjx/hive-demo-1.0-SNAPSHOT.jar\u0026#39;;\r-- 重新加载函数\rRELOAD FUNCTIONS;\r-- 查看函数详细信息\rDESC FUNCTION EXTENDED HELLO_UDTF;\r准备数据 /yjx/movie.json 创建表并载入数据。\r-- 创建表\rCREATE TABLE IF NOT EXISTS t_movie3 (\rmovie_json string\r);\r-- 载入数据\rLOAD DATA INPATH \u0026#39;/yjx/movie.json\u0026#39; INTO TABLE t_movie3;\r测试函数。\rSELECT HELLO_UDTF(movie_json) FROM t_movie3; 案例练习 # Hive压缩/存储 # 压缩 # 在某些情况下，我们还是需要对数据做压缩处理。压缩技术能够有效减少存储系统的读写字节数，提高网络带宽和磁盘空间的效率。Hive 相当于 Hadoop 的客户端，所以 Hive 的压缩分两部分完成，一部分是 Hadoop 的压缩，一部分是 Hive 的。\r其他参考Hadoop压缩 存储方式 # 当今的数据处理大致可分为两大类：OLTP 和 OLAP。\nOLTP 事务处理 # OLTP 联机事务处理（On-Line Transaction Processing）：OLTP 是传统关系型数据库的主要应用，来执行一些基本的、日常的事务处理，比如数据库记录的增、删、查、改等。\r数据计算和数据存储分开，所有用户发过来的请求都是一个事件 Event，事件处理从关系型数据库中查询并进行返\r回。特点实时性很好，来一个事件处理一个事件，额外数据存储在关系型数据库中。最大问题是能够同时处理的数据有\r限，数据库做连表查询的代价很高。 OLTP 分析处理 # 行式存储（Row-oriented） # 查询满足条件的一整行数据的时，只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更\r快。\r传统的关系型数据库，如 Oracle、DB2、MySQL、SQL SERVER 等都是采用行式存储，在基于行式存储的数据库中，数\r据是按照行数据为基础逻辑存储单元进行存储的，一行中的数据在存储介质中以连续存储形式存在。\rText File 和 Sequence File 的存储格式都是基于行存储的。\r这种存储格式比较方便进行 INSERT/UPDATE 操作，不足之处就是如果查询只涉及某几个列，它会把整行数据都读取出\r来，不能跳过不必要的列读取。当然数据比较少，一般没啥问题，如果数据量比较大就比较影响性能，还有就是由于\r每一行中，列的数据类型不一致，导致不容易获得一个极高的压缩比，也就是空间利用率不高。 列式存储（Column-oriented） # 列存将每一列的数据连续存储。相比于行式存储，列式存储在分析场景下有着许多优良的特性：\r在行存模式下，数据按行连续存储，所有列的数据都存储在一个 Block 中，不参与计算的列在 IO 时也要全部读出，读\r取操作被严重放大。而列存模式下，只需要读取参与计算的列即可，极大的减低了 IO Cost，加速了查询。\r同一列中的数据属于同一类型，压缩效果显著。列存往往有着高达十倍甚至更高的压缩比，节省了大量的存储空间，\r降低了存储成本。\r更高的压缩比意味着更小的 Data Size，从磁盘中读取相应数据耗时更短。\r自由的压缩算法选择。不同列的数据具有不同的数据类型，适用的压缩算法也就不尽相同。可以针对不同列类型，选\r择最合适的压缩算法。\r高压缩比意味着同等大小的内存能够存放更多数据，系统 Cache 效果更好。官方数据显示，通过使用列存，在某些分\r析场景下，能够获得 100 倍甚至更高的加速效应。\rINSERT/UPDATE 很麻烦或者不方便，不适合扫描小量的数据。\r列式存储相对于行式存储来说，是新兴的 HBase、HPVertica、EMCGreenplum、ClickHouse 等分布式数据库均采用的存储方式。在基于列式存储的数据库中， 数据是按照列为基础逻辑存储单元进行存储的，一列中的数据在存储介质中以\r连续存储形式存在。 存储格式 # 文件格式是定义数据文件系统中存储的一种方式，可以在文件中存储各种数据结构，特别是 ROW、COLUMN、MAP，\r数组以及字符串，数字等。在 Hadoop 中，没有默认的文件格式，格式的选择取决于用途。而选择一种优秀、适合的数据\r存储格式是非常重要的。选择合适的文件格式会带来以下好处：保证读取/写入的速度;对压缩支持友好;文件可被切分;\r支持 Schema 的更改（Schema 指的是一组相关联的数据库对象，包含：表、字段、字段类型、索引、外键、等等）;\r某些文件格式是为了其通用性而设计的（如 MapReduce、Spark、Flink），而其他文件则是针对特定的场景，特定的数据特征等。 Text File # 文本文件在非 Hadoop 领域很常见，在 Hadoop 领域更常见。数据一行一行到排列，每一行都是一条记录。以典型的UNIX 方式换行符 \\n 终止。可以将每一行转为一个 JSON 串，让数据带有结构。文本文件是可以被切分的，但如果对文本文件进行压缩，则必须使用支持切分文件的压缩编解码器，例如 BZip2，否则只会被一个 MR 程序处理。\r默认格式，存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大。\r应用场景\r仅在需要从 Hadoop 中直接提取数据，或直接从文件中加载大量数据的情况下，才建议使用纯文本格式或 CSV。\r优点\r简单易读、轻量级。\r缺点\r读写速度慢。\r不支持块压缩，在 Hadoop 中对文本文件进行压缩/解压缩会有较高的读取成本，因为需要将整个文件全部压缩或\r者解压缩。\r无法切分压缩文件（会导致产生较大的 MapTask）。 Sequence File # Sequence File 是 Hadoop API 提供的一种支持二进制格式存储，存储方式为行存储，其具有使用方便、可分割、可压缩的特点。Sequence File 支持三种压缩选择：NONE，RECORD，BLOCK。Record 压缩率低，一般建议使用 BLOCK 压缩。\rSequence 最初是为 MapReduce 设计的，因此和 MapReduce 集成很好。在 Sequence File 中，每个数据都是以一个 Key和一个 Value 进行序列化存储的，内部使用了 Hadoop 的标准的 Writable 接口实现序列化和反序列化。Sequence File 中的数据是以二进制格式存储的，这种格式所需的存储空间小于文本的格式。\r应用场景\r通常把 Sequence File 作为中间数据存储格式。例如：将大量小文件合并放入一个 SequenceFIle 中。\rSequence File 的压缩方式有两种，“记录压缩”（Record Compression）和“块压缩”（Block Compression）。如果是记录压缩，则只压缩 Value 的值。如果是块压缩，则将多条记录一并压缩，包括 Key 和 Value。具体格式如下面两图所示\r优点\r与文本文件相比更紧凑，支持块级压缩。\r压缩文件内容的同时，支持将文件切分。\r序列文件在 Hadoop 和其他支持 HDFS 的项目兼容很好，例如：Spark。\r让我们摆脱文本文件迈出的第一步。\r可以作为大量小文件的容器。\r缺点\r对于具有 SQL 类型的 Hive 支持不好，需要读取和解压缩所有字段。\r不存储元数据，并且对 Schema 扩展的唯一方式是在末尾添加新字段。 Map File # Map File 是排序后的 Sequence File，Map File 由两部分组成，分别是 data 和 index。\rindex 作为文件的数据索引，主要记录了每个 Record 的 Key 值，以及该 Record 在文件中的偏移位置。在 MapFile 被访问的时候，索引文件会被加载到内存，通过索引映射关系可迅速定位到指定的 Record 所在文件位置，因此，相对Sequence File 而言，Map File 的检索效率是高效的，缺点是会消耗一部分内存来存储 index 数据。\r需注意的是，MapFile 并不会把所有 Record 都记录到 index 中去，默认情况下每隔 128 条记录存储一个索引映射。当然，记录间隔可人为修改，通过 MapFIle.Writer 的 setIndexInterval() 方法，或修改 io.map.index.interval 属性；另外，与Sequence File 不同的是，Map File 的 KeyClass 一定要实现 WritableComparable 接口，即 Key 值是可比较的。 Avro File # Apache Avro 是与语言无关的序列化系统，由 Hadoop 创始人 Doug Cutting 开发。Avro 是基于行的存储格式，它会在每个文件中包含 JSON 格式的 Schema 定义，从而提高了互操作性并允许 Schema 的变化（删除列、添加列）。 除了支持可切分以外，还支持块压缩。Avro 是一种自描述格式，它将数据的 Schema 直接编码存储在文件中，可以用来存储复杂结构的数据。Avro 还可以进行快速序列化，生成的序列化数据也比较小。\r应用场景\r适合需要操作大量的列（数据比较宽），写入频繁的场景。\r优点\rAvro 是与语言无关的数据序列化系统，以 JSON 格式存储 Schema ，使其易于被任何程序读取和解释。\rAvro 格式是 Hadoop 的一种基于行的存储格式，被广泛用作序列化平台。\r数据本身以二进制格式存储，使其在 Avro 文件中紧凑且高效。\r序列化和反序列化速度很快。\rAvro 文件是可切分的、可压缩的，非常适合在 Hadoop 生态系统中进行数据存储。\r缺点\r行式存储效率较低。例如：读取 15 列中的 2 列数据，基于行式存储就需要读取数百万行的 15 列。而列式存储只\r需要读取这 2 列即可。\r列式存储因为是将同一列（类）的数据存储在一起，压缩率要比行式存储高。 RC File # RC File 是为基于 MapReduce 的数据仓库系统设计的数据存储结构。它结合了行存储和列存储的优点（先水平划分数据，然后再垂直划分），可以满足快速数据加载和查询，有效利用存储空间以及适应高负载的需求。RC File 保证了同一行中的数据位于同一节点中，因此具有较低的元组重构成本。其次，RC File 可以利用按列的数据压缩并跳过不必要的列读取。RC File 是由二进制键/值对组成的 flat 文件，它与 Sequence File 有许多相似之处。在数仓中执行分析时，这种面向列的存储非常有用。\r注意：无法直接将数据加载到 RC File 中。需要先将数据加载到另一个表中，然后通过查询另一张表的方式写入到新创建的 RC File 中。\r应用场景\r常用在 Hive 中。RC File 可将数据分为几组行，将数据存储在行的列中。\rRC File 首先将行水平划分为行拆分（Row Group），然后以列方式垂直划分每个行拆分（Columns）。\rRC File 将行拆分的元数据存储为 Record 的 Key，并将行拆分的所有数据存储在 Value。\r作为行存储，RC File 保证同一行中的数据位于同一节点中。\r作为列存储，RC File 可以利用列数据压缩，并跳过不必要的列读取。\r优点\r基于列式的存储，更好的压缩比。\r利用元数据存储来支持数据类型。\r支持 Split。\r缺点\rRC File 不支持 Schema 扩展，如果要添加新的列，则必须重写文件，这会降低操作效率。 ORC File # Apache ORC（Optimized Row Columnar，优化行列）是 Apache Hadoop 生态系统面向列的开源数据存储格式，它与Hadoop 环境中的大多数计算框架兼容。ORC 代表“优化行列”，它以比 RC 更为优化的方式存储数据，提供了一种非常有效的方式来存储关系数据，然后存储 RC 文件。ORC 将原始数据的大小最多减少 75％，数据处理的速度大大提高。\r注意：无法直接将数据加载到 ORC File 中。需要先将数据加载到另一个表中，然后通过查询另一张表的方式写入到新创建的 ORC File 中。\r应用场景\r常用在 Hive 中。\r每个 ORC 文件由 1 个或多个 Stripe 组成，每个 Stripe 一般为 HDFS 的块大小，每个 Stripe 包含多\r条记录，这些记录按照列进行独立存储，每个 Stripe 里有三部分组成，分别是 Index Data，Row Data，Stripe Footer。\rPostscript：提供了解释文件其余部分的必要信息，包括文件的页脚和元数据部分的长度，文件的版本以及使用的常规压缩类型（例如 none，zlib，LZO，LZ4，Zstd 或 Snappy）。\rFile Footer：包含文件正文的布局，类型架构信息，行数以及每个列的统计信息。\rStripe 条带数据块：文件正文分为条带。每个条带都是自包含的，只能使用自己的字节与文件的 Footer 和 Postscript结合使用。每个条带仅包含整行，因此行不会跨越条带边界。Stripes 有三个部分：条带中行的一组索引，数据本身和条带页脚。索引和数据部分都按列分割，因此只需要读取所需列的数据。\r列统计：列统计信息的目标是，对于每个列，编写器记录计数并根据其他有用字段的类型进行记录。对于大多数原始\r类型，它记录最小值和最大值；对于数字类型，它还存储总和。真实列数据块，其中又分为 Index Data（记录每列的\r索引信息），Raw Data（记录原始数据），Stripe Footer（记录每列的统计信息，min/max/sum 等）。\r每个文件有一个 File Footer，这里面存的是每个 Stripe 的行数，每个 Column 的数据类型信息等；每个文件的尾部是一个 PostScript，这里面记录了整个文件的压缩类型以及 File Footer 的长度信息等。在读取文件时，会 Seek 到文件尾部读PostScript，从里面解析到 File Footer 长度，再读 File Footer，从里面解析到各个 Stripe 信息，再读各个 Stripe，即从后往前读。\r优点\r比 Text File，Sequence File 和 RC File 具备更好的的性能。\r列数据单独存储。\r带类型的数据存储格式，使用类型专用的编码器。\r轻量级索引。\r缺点\r与 RC 文件一样，ORC 也是不支持列扩展的。如果要添加新的列，则必须重写文件，这会降低操作效率。 Parquet File # Parquet File 是另一种列式存储的结构，由 Twitter + Cloudera 开源，被 Hive、Spark、Drill、Impala、Pig 等支持，目前已成为 Apache 的顶级项目。和 ORC File 一样，Parquet 也是基于列的二进制存储格式，可以存储嵌套的数据结构。当指定要使用列进行操作时，磁盘输入/输出操效率很高。Parquet 与 Cloudera Impala 兼容很好，并做了大量优化。Parquet 还支持块压缩。与 RC 和 ORC 文件不同的是，Parquet Serdes 支持有限的 Schema 扩展。在 Parquet 中，可以在结构的末尾添加新列。\r关于 Hive 对 Parquet 文件的支持的一个注意事项：Parquet 列名必须小写，这一点非常重要。如果 Parquet 文件包含大小写混合的列名，则 Hive 将无法读取该列。\r行组（Row Group）：每一个行组包含一定的行数，在一个 HDFS 文件中至少存储一个行组，类似于 ORC 的 Stripe 的概念。列块（Column Chunk）：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可以使用不同的算法进行压缩。页（Page）：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可以使用不同的编码方\r式。\r通常情况下，在存储 Parquet 数据的时候会按照 Block 大小设置行组的大小，由于一般情况下每一个 Mapper 任务处理数据的最小单位是一个 Block，这样可以把每一个行组由一个 Mapper 任务处理，增大任务执行并行度。\r一个 Parquet 文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的 Magic Code，用于校\r验它是否是一个 Parquet 文件，Footer length 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的 Schema 信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在 Parquet 中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引。\r优点：和 ORC 文件一样，它非常适合进行压缩，具有出色的查询性能，尤其是从特定列查询数据时，效率很高\r缺点：与 RC 和 ORC 一样，Parquet 也具有压缩和查询性能方面的优点，与非列文件格式相比，写入速度通常较慢。 Parquet vs ORC # ORC 文件格式压缩比 Parquet 要高，Parquet 文件的数据格式 Schema 要比 ORC 复杂，占用的空间也就越高。\rORC 文件格式的读取效率要比 Parquet 文件格式高。\r如果数据中有嵌套结构的数据，则 Parquet 会更好。\rHive 对 ORC 的支持更好，对 Parquet 支持不好，ORC 与 Hive 关联紧密。\rORC 还可以支持 ACID、Update 操作等。\rSpark 对 Parquet 支持较好，对 ORC 支持不好。\r为了数据能够兼容更多的查询引擎，Parquet 也是一种较好的选择。\r表的文件存储格式尽量采用 Parquet 或 ORC，不仅降低存储量，还优化了查询，压缩，表关联等性能。 Hive 存储实践 # Sequence File # -- 创建表\rCREATE TABLE IF NOT EXISTS t_weather_seq (\rid int,\rprovince string,\rcity string,\rarea_code int,\rweather string,\rtemperature int,\rwind_direction string,\rwind_power string,\rhumidity int,\rreport_time string,\rcreate_time string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;\r-- 默认是 TEXTFILE\rSTORED AS SEQUENCEFILE;\r-- 载入数据（只有 TEXTFILE 才可以使用 LOAD DATA 的方式）\rINSERT INTO TABLE t_weather_seq SELECT * FROM t_weather;\r文件格式虽然变了，但是 HQL 语句还是可以正常的执行。 ORC File # -- 创建表\rCREATE TABLE IF NOT EXISTS t_weather_orc_none (\rid int,\rprovince string,\rcity string,\rarea_code int,\rweather string,\rtemperature int,\rwind_direction string,\rwind_power string,\rhumidity int,\rreport_time string,\rcreate_time string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;\r-- 默认是 TEXTFILE\rSTORED AS ORCFILE\r-- 设置 ORCFILE 不使用压缩\rTBLPROPERTIES(\u0026#39;orc.compress\u0026#39;=\u0026#39;NONE\u0026#39;);\r-- 载入数据（只有 TEXTFILE 才可以使用 LOAD DATA 的方式）\rINSERT INTO TABLE t_weather_orc_none SELECT * FROM t_weather;\r-- 设置 ORCFILE 使用 SNAPPY 压缩\rTBLPROPERTIES(\u0026#39;orc.compress\u0026#39;=\u0026#39;SNAPPY\u0026#39;);\rORC 文件默认采用 ZLIB 压缩。ZLIB 压缩率比 Snappy 高，但是 ZLIB 解压缩速率很低。 Parquet File # -- 创建表\rCREATE TABLE IF NOT EXISTS t_weather_parquet_none (\rid int,\rprovince string,\rcity string,\rarea_code int,\rweather string,\rtemperature int,\rwind_direction string,\rwind_power string,\rhumidity int,\rreport_time string,\rcreate_time string\r)\rROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39;\rLINES TERMINATED BY \u0026#39;\\n\u0026#39;\r-- 默认是 TEXTFILE\rSTORED AS PARQUETFILE\r-- 设置 PARQUETFILE 不使用压缩（UNCOMPRESSED、GZIP、LZO、SNAPPY）\rTBLPROPERTIES(\u0026#39;parquet.compression\u0026#39;=\u0026#39;UNCOMPRESSED\u0026#39;);\r-- 载入数据（只有 TEXTFILE 才可以使用 LOAD DATA 的方式）\rINSERT INTO TABLE t_weather_parquet_none SELECT * FROM t_weather;\r-- 设置 PARQUETFILE 使用 SNAPPY 压缩（UNCOMPRESSED、GZIP、LZO、SNAPPY）\rTBLPROPERTIES(\u0026#39;parquet.compression\u0026#39;=\u0026#39;SNAPPY\u0026#39;);\r注意 ORC 是 xxx.compress ，Parquet 是 xxx.compression 。在实际的项目开发中，Hive 表的数据存储格式一般选择 ORC 或 Parquet。压缩方式一般选择 Snappy 或 LZO。 Hive 优化 # EXPLAIN 执行计划 # 使用过 SQL 的同学应该都知道 EXPLAIN，它可以帮助开发人员分析 SQL 问题，使用 EXPLAIN 可以将 SQL 语句的执行过程打印出来，帮助我们选择更好的索引和写出更优化的查询语句。\rHive SQL 也提供了这个功能，Hive 和 SQL 的不同之处在于 Hive 会根据底层计算引擎将其转化为 MR/Tez/Spark 的 Job去运行，所以 Hive SQL 的 EXPLAIN 也可以看到 MR 相关的执行流程。\r在 Hive 中，EXPLAIN 会将 HQL 语句的依赖关系、实现步骤、实现过程进行解析返回，有助于我们了解 HQL 语句在底层是如何实现数据的查询与处理的，辅助我们对 Hive 进行优化。\r官方文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain 语法 # 常用语法命令：\rEXPLAIN [EXTENDED]|CBD|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE|query\rEXTENDED：体统一些额外信息，通常是文件名这样的物理信息。\rCBO：输出由Calcite优化器生成的计划。CBO（Cost Based Optimizer对查询进行动态评估，获取最佳物理计划）\rAST：输出查询的抽象语法树。在2.1.0删除可能导致OOM错误，在4.0.0修复。\rDEPENDENCY：以JSON格式返回查询所依赖的表和分区信息。\rAUTHORIZATION：显示执行查询需要授权的条目。\rLOCKS：这对于了解系统将获得哪些锁很有用。LOCKS 从 Hive 3.2.0 开始支持。\rVECTORIZATION：显示是否启用了向量化查询，以及为什么没有启用的原因。\rANALYZE：用实际的行数注释计划。从 Hive 2.2.0 开始支持。\rquery：Hive SQL 语句。 组成部分 # 查询计划由以下几个部分组成：\rThe Abstract Syntax Tree for the query：抽象语法树\rThe dependencies between the different stages of the plan：计划不同阶段之间的依赖关系\rThe description of each of the stages：阶段描述 剖析 # 我们将上述结果拆分看，先从最外层开始，包含两个大的部分：\rSTAGE DEPENDENCIES： 各个 Stage 之间的依赖性；\rSTAGE PLANS： 各个 Stage 的执行计划。\r先看第一部分 STAGE DEPENDENCIES，包含两个 Stage，Stage-1 是根 Stage，说明这是开始的 Stage，Stage-0 依赖Stage-1，Stage-1 执行完成以后执行 Stage-0。\r再看第二部分 STAGE PLANS，里面有一个 Map Reduce，一个 MR 的执行计划分为两个部分：\rMap Operator Tree：Map 端的执行计划树；Reduce Operator Tree：Reduce 端的执行计划树。\r这两个执行计划树里面包含这条 SQL 语句的 Operator：\rMap 端第一个操作肯定是加载表，所以就是 TableScan 表扫描操作，常见的属性：\ralias：表名称\rStatistics：表统计信息，包含表中数据条数，数据大小等\rSelect Operator：查询操作，常见的属性：\rexpressions：需要的字段名称及字段类型\routputColumnNames：输出的列名称\rStatistics：表统计信息，包含表中数据条数，数据大小等\rGroup By Operator：分组聚合操作，常见的属性：\raggregations：显示聚合函数信息\rmode：聚合模式，值有 hash：随机聚合，就是 hash partition；partial：局部聚合；final：最终聚合\rkeys：分组的字段，如果没有分组，则没有此字段\routputColumnNames：聚合之后输出列名\rStatistics：表统计信息，包含分组聚合之后的数据条数，数据大小等\rReduce Output Operator：输出到 Reduce 的操作，常见属性：\rsort order：值为空不排序；值为 + 正序排序，值为 - 倒序排序；值为 +- 排序的列为两列，第一列为正序，\t第二列为倒序\rFilter Operator：过滤操作，常见的属性：\rpredicate：过滤条件，如 SQL 语句中的 WHERE id \u0026gt;= 1，则此处显示 (id \u0026gt;= 1)\rMap Join Operator：Join 操作，常见的属性：\rcondition map：Join 方式，如 Inner Join 0 to 1 Left Outer Join 0 to 2\rkeys：Join 的条件字段\routputColumnNames：Join 完成之后输出的字段\rStatistics：Join 完成之后生成的数据条数，大小等\rFile Output Operator：文件输出操作，常见的属性\rcompressed：是否压缩\rtable：表的信息，包含输入输出文件格式化方式，序列化方式等\rFetch Operator：客户端获取数据操作，常见的属性：\rlimit：值为 -1 表示不限制条数，其他值为限制的条数 实践 # JOIN\rJOIN 语句会过滤 null 值吗？执行以下 SQL 查看执行计划：\rEXPLAIN SELECT * FROM emp e INNER JOIN dept d ON e.deptno = d.deptno;\r从上述结果可以看到 predicate: deptno is not null (type: boolean) 这样一行，说明 JOIN 时会自动过滤掉关联字段为 null 值的情况，但 LEFT JOIN、RIGHT JOIN 或 FULL JOIN 是不会自动过滤的，大家可以自行尝试下。\rGROUP BY\rGROUP BY 分组语句会进行排序吗？执行以下 SQL 查看执行计划：\rEXPLAIN SELECT deptno, AVG(sal) FROM emp GROUP BY deptno;\r我们看 Group By Operator，里面有 keys: deptno (type: int) 说明按照 deptno 进行了分组，再往下看还有sort order: + ，说明是按照 deptno 字段进行正序排序的。\r执行效率\r说明 Hive 底层会自动帮我们进行优化，所以这两条 SQL 语句执行效率是一样的。这种优化叫做谓词下推。 SQL 优化 # RBO 优化 # ​\tRule-Based Optimization，简称 RBO：基于规则优化的优化器，是一种经验式、启发式的优化思路，优化规则都已经预先定义好了，只需要将 SQL 往这些规则上套就可以。简单的说，RBO 就像是一个经验丰富的老司机，基本优化套路全都知道。例如谓词下推、列裁剪、常量替换等等。\n谓词下推 # ​\t谓词下推（Predicate Pushdown）基本思想：将过滤表达式尽可能移动至靠近数据源的位置，以使真正执行时能直接跳过无关的数据。\n谓词，用来描述或判定客体性质、特征或者客体之间关系的词项。比如“x 大于 y”中的“大于”就是一个谓词。在计算机中，谓词下推（Predicate Pushdown）概念中的谓词指返回 Boolean 值即 true 和 false 的函数，或是隐式转换为 Boolean 的函数：\r如 SQL 中的谓词主要有 LKIE 、 BETWEEN 、 IS NULL 、 IS NOT NULL 、 IN 、 EXISTS 等。\r如 Spark 中的 input.filter(x =\u0026gt; x \u0026gt;= 5) 。\r在文件格式使用 Parquet 或 ORC 时，甚至可能整块跳过不相关的文件。而 Hive 中的谓词下推主要思想是把过滤条件下推到 Map 端，提前执行过滤，以减少 Map 到 Reduce 的传输数据，提升整体性能。简而言之，就是在不影响结果的情况下，尽量将过滤条件提前执行。在传统数据库的查询系统中谓词下推作为优化手段很早就出现了，谓词下推的目的就是通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能。\r对于 JOIN(INNER JOIN)、FULL OUTER JOIN，条件写在 ON 后面，还是 WHERE 后面，性能上面没有区别；\r对于 LEFT OUTER JOIN，右侧的表写在 ON 后面、左侧的表写在 WHERE 后面，性能上有提高；\r对于 RIGHT OUTER JOIN，左侧的表写在 ON 后面、右侧的表写在 WHERE 后面，性能上有提高；\r当条件分散在两个表时，谓词下推可按上述结论 2 和 3 自由组合。\r如果在表达式中含有不确定函数，整个表达式的谓词将不会被 Pushed，例如 RAND() 。因为不确定函数，在编译的时候无法得知，所以，整个表达式不会被 Pushed。\rHive 开启 PPD 命令如下：\r-- 开启 PPD，默认为 true\rSET hive.optimize.ppd=true; 列裁剪\u0026amp;常量替换 # 列裁剪（Column Pruning）表示扫描数据源的时候，只读取那些与查询相关的字段。\r常量替换（Constant Folding）表示将表达式提前计算出结果，然后使用结果对表达式进行替换。假设我们在部门编号上加的过滤条件是 deptno \u0026gt; 5 + 5 ，Catalyst 会使用 ConstantFolding 规则，自动帮我们把条件变成 deptno \u0026gt; 10 。再比如，我们在 SELECT 语句中，掺杂了一些常量表达式，Catalyst 也会自动地用表达式的结果进行替换。 CBO 优化 # ​\tCBO（Cost-Based Optimization）意为基于代价优化的策略，它需要计算所有可能执行计划的代价，并挑选出代价最小的执行计划。\n传统的数据库，成本优化器做出最优化的执行计划是依据统计信息来计算的。Hive 的成本优化器也一样，Hive 在提供最终执行前，优化每个查询的执行逻辑和物理执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等。\r要使用基于成本的优化，需要在查询开始时设置以下参数：\r# 开启 CBO 优化，默认为 true\rSET hive.cbo.enable=true;\r# 统计 SQL 的查询结果是否从统计信息中获取，默认为 true\rSET hive.compute.query.using.stats=true;\r# 是否统计列信息，默认为 false\rSET hive.stats.fetch.column.stats=true;\r# 是否统计分区信息，默认为 true。3.1.1 版本被废弃，不允许用户修改该属性，因为禁用分区状态的获取可能会导致分区表出现问题\rSET hive.stats.fetch.partition.stats=true;\r然后统计表的相关信息才能使用 CBO 优化：\r# 新创建的表或者分区，插入数据时是否统计其信息，默认为 true\r# 新创建的表或者分区，如果通过 INSERT OVERWRITE 的方式插入数据，那么 Hive 会自动将该表或分区的统计信息更新到元数据\rSET hive.stats.autogather=true;\r# 对于已经存在表或分区可以通过 ANALYZE 命令手动更新其 Statistics 信息\r# 统计全表的所有分区的信息\rANALYZE TABLE 表名 COMPUTE STATISTICS;\r# 只统计文件数和文件大小，不扫描文件行数，执行较快\rANALYZE TABLE 表名 COMPUTE STATISTICS NOSCAN;\r# 统计指定字段的信息\rANALYZE TABLE 表名 COMPUTE STATISTICS FOR COLUMNS 列名1,列名2,列名n;\r# 统计指定分区的信息\rANALYZE TABLE 表名 PARTITION(分区列1=值1, 分区列2=值2) COMPUTE STATISTICS;\r对于非分区表列的 Statics 信息存在 Hive 元数据表 TABLE_COL_STATS 中；\r对于分区表列的 Statics 信息存在 Hive 元数据表 PART_COL_STATS 中。 JOIN 优化 # 而 Hive JOIN 的底层是通过 MapReduce 来实现的，Hive 实现 JOIN 时，为了提高 MapReduce 的性能，提供了多种 JOIN 方案：\t小表 JOIN 大表的 Map Join\r大表 JOIN 大表的 Reduce Join，Reduce Join 又分为以下两种：\rBucket Map Join（中型表和大表 JOIN）\rSort Merge Bucket Join（大表和大表 JOIN） Map Join # ​\tMap Join 顾名思义，就是在 Map 阶段进行表之间的连接。而不需要进入到 Reduce 阶段才进行连接。这样就节省了在 Shuffle 阶段时要进行的大量数据传输。从而起到了优化作业的作用。Map Join 简单说就是在 Map 阶段将小表读入内存，顺序扫描大表完成 Join。\n通过 MapReduce Local Task 将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会对HashTableFiles 进行压缩。\rMapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务。\r应用场景：小表 JOIN 大表或者小表 JOIN 小表。\r实现原理：\rMap Join 会把小表全部读入内存中，在 Map 阶段直接拿另外一个表的数据和内存中的表数据做匹配，由\r于在 Map 阶段进行了 JOIN 操作，底层不需要经过 Shuffle，这样就不会由于数据倾斜导致某个 Reduce 上落数据太多而失败，但是需要占用内存空间存放小表数据。\r具体使用：尽量使用 Map Join 来实现 JOIN 过程，Hive 中默认开启了 Map Join：\r-- 是否开启自动转为 Map Join 功能，默认为 true\rSET hive.auto.convert.join=true;\r-- 2.0 版本之前的控制参数\r-- 如果打开上面这个参数，当参与 Join 的表小于下面参数指定的值，将转换为 Map Join，默认为 25000000B\rSET hive.mapjoin.smalltable.filesize=25000000;\r-- 2.0 版本新增的控制参数\r-- 假设参与 Join 的表(或分区)有 N 个，是否启用基于输入文件的大小，将多个 Map Join 合并为一个，默认为 true SET hive.auto.convert.join.noconditionaltask=true;\r-- 假设参与 Join 的表(或分区)有 N 个，如果打开上面这个参数，并且有 N-1 个小表(或分区)的大小总和小于下面参数指定的值，那么会直接将多个 Map Join 转换为 1 个，默认为 10000000B\rSET hive.auto.convert.join.noconditionaltask.size=10000000;\r使用优化的 Map Join 过程中没有 Shuffle 是通过本地的一个 HashTable 较小的表（较小的表的识别可以通过元数据信息判断）生成 HashTable Files 文件，并保存到 HDFS 的临时缓存当中，然后通过与 Map 出来的另一个表进行直接匹配，得出结果，因此过程中没有 Shuffle，不需要网络，所以效率相对来说较快，即为优化。 Reduce Join # Map 端的主要工作：为来自不同表或文件的 key/value 对，打标签以区别不同来源的记录。然后用连接字段作为\rKey，其余部分和新加的标志作为 Value，最后进行输出。\rReduce 端的主要工作：在 Reduce 端以连接字段作为 Key 的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录进行合并即可。\r应用场景：大表 JOIN 大表。\r实现原理：将两张表的数据在 Shuffle 阶段利用 Shuffle 的分组将数据按照关联字段进行合并。\r具体使用：Hive 会自动判断是否满足 Map Join，如果不满足 Map Join，则会自动执行 Reduce Join。 Bucket Map Join # Bucket Map Join大表对小表应该使用 Map Join 来进行优化，但是如果是大表对大表，如果进行 Shuffle，那就非常可怕，第一个慢不用说，第二个容易出异常。此时就可以使用 Bucket Join 来进行优化，而 Bucket Join 又分为：\rBucket Map Join\rSort Merge Bucket Join（SMB Join）\r具体使用流程如下：\r将两张大表的数据构建分桶\r数据按照分桶的规则拆分到不同的文件中\r分桶规则 = MapReduce 分区的规则 = Key 的 Hash 取余\rKey = 分桶的字段\r只需要实现桶与桶的 JOIN 即可，减少了比较次数\r分桶本质：底层 MapReduce 的分区，桶的个数 = Reduce 个数 = 文件个数。 两张表 JOIN 的时候，小表不足以放到内存中，但是又想用 Map Join，这个时候就要用到 Bucket Map Join。其方法是两个 JOIN 表都在 Join Key上都做 Hash Bucket，并且把你打算复制的那个（相对）小表的 Bucket 数设置为大表的倍数。这样数据就会按照 Key Join 做 Hash Bucket。小表依然复制到所有节点，Map Join 的时候，小表的每一组 Bucket 加载成HashTable，与对应的一个大表 Bucket 做局部 JOIN，这样每次只需要加载部分 HashTable 就可以了。\rMap Join 条件：\rSET hive.optimize.bucketmapjoin=true; ，默认为 false\r所有要 JOIN 的表必须分桶，如果表不是 Bucket 的，则只是做普通 JOIN\r大表的 Bucket 数是小表的 Bucket 数的整数倍（或相等）\rBucket 列 == JOIN 列\r必须是应用在 Map Join 的场景中 SMB Join # SMB Join 是基于 Bucket Map Join 的有序 Bucket，可实现在 Map 端完成 JOIN 操作，只要桶内的下一条不是，就不用再比较了，有效地减少或避免 Shuffle 的数据量。\rSMB Join 的条件和 Map Join 类似但又不同：\rSET hive.optimize.bucketmapjoin=true;\rSET hive.optimize.bucketmapjoin.sortedmerge=true;\rSET hive.auto.convert.sortmerge.join=true;\r所有要 JOIN 的表必须分桶，如果表不是 Bucket 的，则只是做普通 JOIN\r大表的 Bucket 数 = 大表的 Bucket 数\rBucket 列 == JOIN 列 == SORT 列\r必须是应用在 Bucket Map Join 的场景中\t数据倾斜 # 定义 # 数据倾斜，即单个节点任务所处理的数据量远大于同类型任务所处理的数据量，导致该节点成为整个作业的瓶颈，这\r是分布式系统不可能避免的问题。\rMapReduce 模型中，数据倾斜问题是很常见的，因为同一个 Key 的 Values，在进行 GroupByKey、CountByKey、ReduceByKey、Join 等操作时一定是分配到某一个节点上的一个 Task 中进行处理的，如果某个 Key 对应的数据量特别大的话，就会造成某个节点的堵塞甚至宕机的情况。在并行计算的作业中，整个作业的进度是由运行时间最长的那个 Task 决定的，在出现数据倾斜时，整个作业的运行将会非常缓慢，甚至会发生 OOM 异常。 原因 # 从本质来说，导致数据倾斜有两种原因\r任务读取大文件，最常见的就是读取压缩的不可分割的大文件。\r任务需要处理大量相同键的数据。\r单表聚合操作，部分 Key 数据量较大，且大 Key 分布在很多不同的切片。\r两表进行 JOIN，都含有大量相同的倾斜数据键。\r数据含有大量无意义的数据，例如空值(NULL)、空字符串等。\r含有倾斜数据在进行聚合计算时无法聚合中间结果，大量数据都需要经过 Shuffle 阶段的处理，引起数据倾\t斜。\r数据在计算时做多维数据集合，导致维度膨胀引起的数据倾斜。\t解决 # 压缩引发的数据倾斜\r当对文件使用 GZIP压缩等不支持文件分割操作的压缩方式，在日后有作业涉及读取压缩后的文件时，该压缩文件只会被一个任务所读取。如果该压缩文件很大，则处理该文件的 Map 需要花费的时间会远多于读取普通文件的 Map 时间，该 Map 任务会成为作业运行的瓶颈。这种情况也就是 Map 读取文件的数据倾斜。为避免因不可拆分大文件而引发数据读取的倾斜，在数据压缩的时候可以采用 BZip2 和 Zip 或 LZO 支持文件分割的压缩算法。\r单表数据倾斜优化\r为了减少 Shuffle 数据量以及 Reduce 端的压力，通常 SQL 会在 Map 端做一个 Partial Aggregate（被称为预聚合或偏聚合），即在 Shuffle 前将同一分区内相同 Key 的记录先进行一个预计算，再将结果进行 Shuffle，发送到 Reduce 端做汇总，比如 MapTask 的提前 Combiner。\r适用场景：聚合类的 Shuffle 操作，部分 Key 数据量较大，且大 Key 的数据分布在很多不同的切片。\r解决逻辑：两阶段聚合（加盐局部聚合+去盐全局聚合）+Map-Side聚合（开启 Map端聚合或自定义Combiner）。\r首先，通过自定义函数给每个数据的 Key 添加随机数前缀，对 Key 进行打散，将原先一样的 Key 变成不一样的 Key，然后进行第一次聚合，这样就可以让原本被一个 Task 处理的数据分散到多个 Task 上去做局部聚合；随后，去除掉每个Key 的前缀，再次进行聚合。\r业务无关数据引发的数据倾斜\r实际业务中有些大量的 NULL 值（空 Key）或者一些无意义的数据参与到计算作业中，这些数据可能来自业务为了上报或因数据规范将某类数据进行归一化变成空值或空字符等形式，这些与业务无关的数据导致在进行分组聚合或者在执行表连接时发生数据倾斜。对于这类问题引发的数据倾斜，在计算过程中排除含有这类“异常”数据即可。\r空 Key 过滤使用场景：\r非 INNER JOIN 的查询\r不需要字段为 NULL 的查询\r核心思想：先过滤再 JOIN。\r如果业务需求必须包含 NULL 值返回，此时我们可以为空 Key字段赋一个随机值，使得数据随机均匀地分散到不同的 Reduce 上。\r无法消减中间结果的数据量引发的数据倾斜\r在一些操作中无法消减中间结果，例如使用 COLLECT_LIST 聚合函数（其实也是 GROUP BY优化）。在student 表中，假设 age 有数据倾斜，可以通过开启 hive.groupby.skewindata 将作业拆解成两个作业，第一个作业会尽可能将 Map 的数据平均分发到不同的 Reduce，每个 Reduce 实现预聚合，这样的处理结果是相同的 Key 会被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个作业在第一个作业处理的数据基础上，按 Key 分发到 Reduce 中，这个过程可以保证相同的 Key 会被分发到同一个 Reduce，完成最终聚合。\r多维聚合计算数据膨胀引起的数据倾斜 JOIN 数据倾斜优化\rMap JOIN\r在 Hive JOIN 策略中，当一张小表足够小并且可以先缓存到内存中时，可以使用 Map Join。Map Join 顾名思义，就是在 Map 阶段进行表之间的连接。而不需要进入到 Reduce 阶段才进行连接。这样就节省了在 Shuffle 阶段时要进行的大量数据传输。从而起到了优化作业的作用。\rMap Join 简单说就是在 Map 阶段将小表读入内存，顺序扫描大表完成 Join。\r适用场景：适用于小表 JOIN 大表。小表足够小。\r解决方案：\r在小表 JOIN 大表时如果产生数据倾斜，那么 Map JOIN 可以直接规避掉 Shuffle 阶段（默认开启）。\r大小表 JOIN\r采样倾斜 Key 并分拆 JOIN，适用场景：适用于 JOIN 时出现数据倾斜。\r解决方案：\r将存在倾斜的表，根据抽样结果，拆分为倾斜 Key（Skew 表） 和没有倾斜 Key（Normal 表）的两个数据集。\r将 Skew 表的 Key 全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集做笛卡尔乘积（即将数据量扩大 N 倍，得到 New 表）。\r打散的 Skew 表 JOIN 扩容的 New 表，普通表 JOIN 未扩容的 Old 表，最后将这两部分结果 UNION ALL。\rSkew Join\r两表进行普通的 JOIN 时，如果表的连接键存在倾斜，那么 Shuffle 阶段必然会引起数据倾斜。遇到这种情况，Hive 的通常做法是使用 Skew Join（启动两个作业），第一个作业处理没有倾斜的数据，第二个作业将倾斜的数据存到分布式缓存中，分发到各个 Map 任务的所在节点。在 Map 阶段完成 JOIN 操作，即 Map Join，这样避免了 Shuffle，从而避免了数据倾斜。\rSkew Join 是 Hive 自身对于数据倾斜的优化方案。Skew Join 是 Hive 中一种专门为了避免数据倾斜而设计的特殊的Join 过程，这种 Join 的原理是将 Map Join 和 Reduce Join 进行合并，如果某个值出现了数据倾斜，就会将产生数据倾斜的数据单独使用 Map Join 来实现，其他没有产生数据倾斜的数据由 Reduce Join 来实现，这样就避免了 Reduce Join 中产生数据倾斜的问题。最终将 Map Join 的结果和 Reduce Join 的结果进行合并。\rSkew Join 的实现原理\r首先判断 HQL 是否会产生数据倾斜；\r如果会产数据倾斜并超过 hive.skewjoin.key 阈值使用 Map Join 来处理；\r如果不会产生数据倾斜使用 Reduce Join 来处理；\r将 Map Join 的结果和 Reduce Join 的结果进行合并。\r关于 Skew Join 的相关设置如下：\r-- 是否开启 Skew Join，默认为 false\rSET hive.optimize.skewjoin=true;\r-- 当 Key 超过阈值时作为一个 Skew Join 处理，默认为 100000\rSET hive.skewjoin.key=100000;\r-- 用于处理 Skew Join 的 MapTask 的最大数量，默认为 10000\rSET hive.skewjoin.mapjoin.map.tasks=10000; 资源优化 # 向量化查询 # 向量化执行，可以简单地看作一项消除程序中循环的优化。为了实现向量化执行，需要利用 CPU 的 SIMD 指令。SIMD 的全称是 Single Instruction Multiple Data，即用单条指令操作多条数据。现代计算机系统概念中，它是通过数据并行以提高性能的一种实现方式（其他的还有指令级并行和线程级并行），它的原理是在 CPU 寄存器层面实现数据的并行操作。在计算机系统的体系结构中，存储系统是一种层次结构。存储媒介距离 CPU 越近，则访问数据的速度越快。\rSIMD：全称 Single Instruction Multiple Data，中文翻译叫单指令多数据流，可以使用一条指令同时完成多个数据的运算操作。相对的，最传统的指令架构是 SISD 就是单指令单数据流，每条指令只能对一个数据执行操作。\rHive 不仅支持将数据按列存储，而且按列进行计算。传统 OLTP 数据库通常采用按行计算，原因是事务处理中以点查为主，SQL 计算量小，实现这些技术的收益不够明显。但是在分析场景下，单个 SQL 所涉及计算量可能极大，将每行作为一个基本单元进行处理会带来严重的性能损耗：\r对每一行数据都要调用相应的函数，函数调用开销占比高；\r存储层按列存储数据，在内存中也按列组织，但是计算层按行处理，无法充分利用 CPU Cache 的预读能力，造成 CPUCache Miss 严重；\r按行处理，无法利用高效的 SIMD 指令；\rHive 实现了向量执行引擎（Vectorized Execution Engine），对内存中的列式数据，一个 Batch 调用一次 SIMD 指令（而非每一行调用一次），不仅减少了函数调用次数、降低了 Cache Miss，而且可以充分发挥 SIMD 指的并行能力，大幅缩短了计算耗时。向量执行引擎，通常能够带来数倍的性能提升。\r数据加载出来之后，每批数据中的每一列都会转成一个向量，在后续的执行过程中，数据是一个批批从一个操作符流经另一个操作符，而不是一行行的。\r总结下来就是：让计算更多的停留在函数内，而不是频繁的交互切换，提高了 CPU 的流水线并行度。数据不仅按列式存储，而且按列（Batch Data）计算。\r开启向量化查询后，Hive 会将一个普通的查询转化为向量化查询执行。它大大减少了扫描、过滤器、聚合和连接等典型查询操作的 CPU 使用。标准查询执行系统一次处理一行。向量化查询执行可以一次性处理 1024 行的数据块，以减少底层操作系统处理数据时的指令和上下文切换。\r开启向量化查询命令如下：\r-- 开启向量化查询，默认为 true\rSET hive.vectorized.execution.enabled=true;\r-- 开启 Reduce 任务的向量化执行模式，默认为 true（MR 计算引擎不支持，需要配置 Tez/Spark 计算引擎使用）\rSET hive.vectorized.execution.reduce.enabled=true;\rHive 中，向量化查询执行在 Hive 0.13.0 及以后版本可用，默认开启：\rSET hive.vectorized.execution.enabled=true;\rHive 向量化执行支持数据类型有：tinyint、smallint、int、bigint、boolean、float、double、decimal、date、timestamp、string。如果使用了其他数据类型，查询将不会使用向量化执行，而是每次只会查询一行数据。可以通过Explain 来查看查询是否使用了向量化，如果输出信息中 Execution mode 的值为 vectorized ，则使用了向量化查询 存储优化 # Hive 的存储本质还是 HDFS，而 HDFS 是不利于小文件存储的，因为每个小文件都会产生一条元数据信息，并且不利于 MapReduce 的处理。Hive 中提供了一个特殊的机制，可以自动的判断是否是小文件，如果是小文件就将小文件进行合并。\r-- 在 Map-Only 任务结束时合并小文件，默认为 true\rSET hive.merge.mapfiles=true;\r-- 在 Map-Reduce 任务结束时合并小文件，默认为 false\rSET hive.merge.mapredfiles=true;\r-- 合并文件的大小，默认约 244M\rSET hive.merge.size.per.task=256000000;\r-- 平均每个文件的大小，如果小于该值则进行合并，默认约 15M\rSET hive.merge.smallfiles.avgsize=16000000;\rHive 中提供了一个 CombineHiveInputFormat 类专门用于小文件合并（默认启用）。相关配置命令如下：\rSET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; YARN 优化 # 配置每个容器请求被分配的最小内存。如果容器请求的资源小于该值，会以 1024MB 进行分配；如果NodeManager 可被分配的内存小于该值，则该 NodeManager 将会被 ResouceManager 给关闭。默认值 1024MB。\rSET yarn.scheduler.minimum-allocation-mb=1024;\r配置每个容器请求被分配的最大内存。如果容器请求的资源超过该值，程序会抛出InvalidResourceRequestException异常。默认值 8192MB。\rSET yarn.scheduler.maximum-allocation-mb=8192;\r配置每个容器请求被分配的最小虚拟 CPU 个数，低于此值的请求将被设置为此属性的值。此外，配置为虚拟内核少于此值的 NodeManager 将被 ResouceManager 关闭。默认值 1。\rSET yarn.scheduler.minimum-allocation-vcores=1;\r配置每个容器请求被分配的最大虚拟CPU个数，高于此值的请求将抛出InvalidResourceRequestException 的异常。如果开发者所提交的作业需要处理的数据量较大，需要关注上面配置项的配置。\rSET yarn.scheduler.maximum-allocation-vcores=4;\r配置一个节点内所有容器所能使用的物理 CPU 的占比，默认为 100%。即如果一台机器有 16 核，CPU 的使用率最大为 16 核，该比值为 100%，如果该比值为 50%，则所有容器能使用的 CPU 资源为 8 核。\rSET yarn.nodemanager.resource.percentage-physical-cpu-limit=100;\r配置是否开启 CPU 的共享模式。共享模式告诉系统容器除了能够使用被分配的 CPU 资源外，还能使用空闲的 CPU 资源。默认值 false。\rSET yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false; 并行执行 # Hive 在实现 HQL 计算运行时，会解析为多个 Stage，有时候 Stage 彼此之间有依赖关系，只能挨个执行。但有些时候，很多 Stage 之间是没有依赖关系的，例如 UNION 语句，JOIN 语句等等，这些 Stage 没有依赖关系，但是 Hive 依旧会挨个执行每个 Stage，这样会导致性能非常的差。我们可以通过修改参数，开启并行执行，当多个 Stage 之间没有依赖关系时，允许多个 Stage 并行执行，提高性能\r-- 开启任务并行执行，默认为 false\rSET hive.exec.parallel=true;\r-- 同一条 SQL 的并行化线程数，默认为 8\rSET hive.exec.parallel.thread.number=8;\r需要注意的是，在共享集群中，如果 Job 中并行阶段增多，那么集群利用率就会增加。系统资源比较空闲的时候才会有优势，如果系统资源比较吃紧，没资源意味着并行也无法启动。 JVM 重用 # Hadoop 默认会为每个 Task 启动一个 JVM 来运行（JVM 启动时内存开销大）。当 Job 数据量大的时候，如果单个Task 数据量比较小，也会申请 JVM，这就导致了资源紧张及浪费的情况。JVM 重用可以使得 JVM 实例在同一个 Job 中重新使用 N 次，当一个 Task 运行结束以后，JVM 不会进行释放，而是继续供下一个 Task 运行，直到运行了 N 个Task 以后，就会释放。\r目前 Hadoop3 中已不再支持该配置，如果大家使用的是 Hadoop3 以前的版本可以通过修改 mapred-site.xml 修改该选项（默认值 1）：\r\u0026lt;!-- -1 表示一个 JVM 能够运行的任务数（来自同一个 Job）是没有限制的，通常在 10~20 之间 --\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;mapreduce.job.jvm.numtasks\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;-1\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt; 聚合优化 # GROUP BY 优化\r默认情况下，Map 阶段同一 Key 数据会分发给一个 Reduce，当一个 Key 数据过大时就会产生数据倾斜。但并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作也可以先在 Map 端进行部分聚合，最后再在 Reduce 端得出最终结果。\r开启 Map 端聚合相关选项如下：\r-- 是否开启 Map 端聚合，默认为 true\rSET hive.map.aggr=true;\r-- Map 端进行聚合操作的条目数上限，默认为 100000\rSET hive.groupby.mapaggr.checkinterval=100000;\rGROUP BY 数据倾斜的负载均衡处理：\r-- 开启该参数后，当前程序会自动通过两个作业来运行任务，实现数据倾斜的负载均衡处理，默认为 false\rSET hive.groupby.skewindata=true; ORDER BY 优化\rORDER BY 会将结果按某字段全局排序，将导致所有 Map 端数据都进入一个 Reducer 中，数据量大时可能会长时间计算不完。由于 Hive 中的 ORDER BY 对于大数据集存在性能问题，延伸出了部分排序。\r使用部分排序 SORT BY 会视情况启动多个 Reducer 进行排序，保证每个 Reducer 内局部有序，常与 DISTRIBUTE BY 搭配使用，DISTRIBUTE BY 可以保证相同 Key 的记录会被划分到一个 Reduce 中，不使用 DISTRIBUTE BY，Map 端数据就会随机分配到 Reducer。\r使用 SORT BY 可以配合指定执行的 Reduce 个数 （ SET mapred.reduce.tasks=\u0026lt;number\u0026gt; ）。SORT BY 结束后，对输出的数据再执行归并排序，即可以得到全部结果。能否利用 SORT BY 与 CLUSTER BY 达到 ORDER BY 一样的功能？ 答案是不能，因为 SORT BY 是局部有序，必须外面再嵌套一层。 COUNT(DISTINCT) 优化\r站在 MapReduce 的角度，由于 COUNT(DISTINCT) 操作需要用一个 ReduceTask 来完成，所以这个 Reduce 需要处理的数据量可能会非常大，会导致整个 Job 很难完成，所以一般 COUNT(DISTINCT) 使用先 GROUP BY 再 COUNT 的方式进行替换，但是需要注意 GROUP BY 造成的数据倾斜。\r-- 改写前\rSELECT COUNT(DISTINCT job) FROM emp;\r-- 改写后\rSELECT COUNT(*) FROM (SELECT job FROM emp GROUP BY job) e; Job 优化 # Map 优化\r通常情况下，作业通过 Input 目录会产生一个或者多个 Map 任务。主要的决定因素有：Input 的文件总个数，Input 的文件大小，集群设置的文件块大小（默认 128M）。\r是不是 Map 数越多越好？\r如果一个任务有很多小文件（远远小于块大小 128M），则每个小文件也会被当做一个块，用一个Map 任务来完成，而一个 Map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 Map 数是受限的。解决：合并小文件。\r是不是保证每个 Map 处理接近 128M 的文件块，就可以高枕无忧了？\r答案也是不一定。比如有一个 127M 的文件，正常会用一个 Map 去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果 Map 处理的逻辑比较复杂，用一个 Map 任务去做，肯定也比较耗时的。解决：Split 分片。\r控制 Map 数量需要遵循两个原则：使大数据量利用合适的 Map 数；使单个 Map 任务处理合适的数据量。\r关于切片设置 MapReduce 关键源码：\r// getFormatMinSplitSize()：一个切片最少应该拥有 1 个字节\r// getMinSplitSize(job)：读取程序员设置的切片的最小值，如果没有设置默认读取 1\rlong minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));\r// 读取程序员设置的切片的最大值，如果没有设置默认读取 Long.MAX_VALUE\rlong maxSize = getMaxSplitSize(job);\r// ...\r// 获取 Block 的大小，默认为 128M\rlong blockSize = file.getBlockSize();\r// 获取 Split 的大小，切片的默认大小为 128M\r// return Math.max(minSize, Math.min(maxSize, blockSize));\r// minSize 为 64M --\u0026gt; 最终返回 128M，minSize 为 256M --\u0026gt; 最终返回 256M\r// maxSize 为 64M --\u0026gt; 最终返回 64M，maxSize 为 256M --\u0026gt; 最终返回 128M\r// 如果需要调大切片，需要调节 minSize；如果需要调小切片，则调节 maxSize\rlong splitSize = computeSplitSize(blockSize, minSize, maxSize);\r// computeSplitSize 方法的核心代码如下：\rreturn Math.max(minSize, Math.min(maxSize, blockSize));\rminSize 设置为 64M --\u0026gt; 最终返回 128M\rminSize 设置为 256M --\u0026gt; 最终返回 256M\rmaxSize 设置为 64M --\u0026gt; 最终返回 64M\rmaxSize 设置为 256M --\u0026gt; 最终返回 128M\r因此，如果需要调大切片，则调节 minSize；如果需要调小切片，则调节 maxSize。\r-- 如果需要调小切片，则调节 maxSize，默认值 256000000\rSET mapred.max.split.size=256000000;\r-- 如果需要调大切片，需要调节 minSize，默认值 1\rSET mapred.min.split.size=1;\r-- 每个节点处理的最⼩ Split，默认值 1\rSET mapred.min.split.size.per.node=1;\r-- 每个机架处理的最⼩ Split，默认值 1\rSET mapred.min.split.size.per.rack=1;\r-- 存储优化合并小文件\rSET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; Reduce 优化\rReduce 个数的设定极大影响任务执行效率，不指定 Reduce 个数的情况下，Hive 会基于以下两个设定计算出 Reduce的个数：\r-- 每个 Reduce 处理的数据量，默认值 256000000\rSET hive.exec.reducers.bytes.per.reducer=256000000;\r-- 每个任务最大的 Reduce 数，默认值 1009\rSET hive.exec.reducers.max=1009;\r计算 Reducer 数的公式为：N=min(参数2，总输入数据量/参数1)。\r调整 Reduce 个数方法一：\rSET hive.exec.reducers.bytes.per.reducer=256000000;\r调整 Reduce 个数方法二：\r-- 设置 Reduce 的个数，默认值 -1（-1 时会通过计算得到 Reduce 个数）\rSET mapred.reduce.tasks=-1;\r关于单个 Reduce 写出文件的块的大小，可以在执行任务前通过 dfs.blocksize 来进行设置。\r-- HDFS Block 大小\rSET dfs.blocksize;\rSET dfs.block.size;\rReduce 是不是越多越好？\r同 Map 一样，启动和初始化 Reduce 也会消耗时间和资源；另外，有多少个 Reduce 就会有多少个输出文件，如果生成了很多个小文件，这些小文件又作为下一个任务的输入，则也会出现小文件过多的问题。\r什么情况下只有一个 Reduce？\r很多时候你会发现任务中不管数据量多大，不管你有没有设置调整 Reduce 个数的参数，任务中一直都只有一个\rReduce 任务。其实只有一个 Reduce 任务的情况，除了数据量小于 hive.exec.reducers.bytes.per.reducer 参数值的情况外，还有以下原因：\r没有 GROUP BY 的汇总，比如把 SELECT a, COUNT(*) FROM t WHERE a = \u0026#39;三年二班\u0026#39; GROUP BY a; 写成 SELECTCOUNT(*) FROM t WHERE a = \u0026#39;三年二班\u0026#39;;用了 ORDER BY；有笛卡尔积\r控制 Reduce 数量需要遵循两个原则：使大数据量利用合适的 Reduce 数；使单个 Reduce 任务处理合适的数据量。\rShuffle 优化\r减少IO流和磁盘流。 其他优化 # Fetch 模式\r当我们执行一个简单的 HQL 语句时，例如 SELECT * FROM emp; ，发现数据可以很快的返回，这其实涉及到 Fetch抓取的概念。相关配置为：\tSET hive.fetch.task.conversion;\r该配置默认值为 more ，如果设置为 none ，那么每次执行 HQL 都会执行 MapReduce 程序。\r多重模式\r如果碰到一堆 SQL 都是从同一个表进行扫描，然后做不同的逻辑，可以采用多重模式。例如以下 SQL：\rINSERT INTO t_user1 SELECT id, username FROM t_user;\rINSERT INTO t_user2 SELECT id, username FROM t_user;\rINSERT OVERWRITE TABLE t_user3 SELECT id, username FROM t_user;\rINSERT OVERWRITE TABLE t_user4 SELECT id, password FROM t_user;\r如果有 n 条 SQL，每个 SQL 执行都会扫描一次这张表。可以修改 SQL 为以下模式（多重模式）：\rFROM t_user\rINSERT INTO t_user1 SELECT id, username\rINSERT INTO t_user2 SELECT id, password\rINSERT OVERWRITE TABLE t_user3 SELECT id, username\rINSERT OVERWRITE TABLE t_user4 SELECT id, password;\r一次读取，多次插入，对于需要从一张表读取数据后并多次使用的场景非常管用。\r关联优化\r当一个程序中有一些操作彼此之间有关联性的时候，是可以在一个 MapReduce 中完成的，但是 Hive 会不智能的选择使用两个 MapReduce 来完成这两个操作。\r方案一：\r第一个 MapReduce 做 GROUP BY，经过 Shuffle 阶段对 id 做分组\r第二个 MapReduce 对第一个 MapReduce 的结果做 ORDER BY，经过 Shuffle 阶段对 id 进行排序\r方案二：\r因为都是对 id 处理，可以使用一个 MapReduce 的 Shuffle，既可以做分组也可以做排序\rHive 默认采用方案一来执行，这样会导致性能相对较差，通过开启相关选项可以让 Hive 按方案二来执行：\r-- 开启关联关系优化，默认为 false\rSET hive.optimize.correlation=true;\r本地模式\r使用 Hive 的过程中，有一些数据量不大的表也会转换为 MapReduce 执行。而只要提交到集群，就需要申请资源、等待资源分配、启动 JVM 进程，再运行 Task，一系列的过程比较繁琐。Hive 为了解决这个问题，延用了 MapReduce 中的设计，提供了本地计算模式，允许程序不提交给 YARN，直接在本地运行，以便于提高小数据量程序的性能。对数据量比较小的操作，就可以在本地执行，这样要比提交任务到集群执行快很多。\r开启本地模式后，Hive 会判断当前作业执行的前置环境，如果满足环境才会在本地执行。判定条件如下：\rob 的输入数据大小必须小于参数： hive.exec.mode.local.auto.inputbytes.max （默认 128M）\rJob 的 Map 数必须小于参数： hive.exec.mode.local.auto.tasks.max （默认 4）\rJob 的 Reduce 数必须为 1 或 0： mapred.reduce.tasks （默认 -1）。\r开启本地模式命令如下：SET hive.exec.mode.local.auto=true;\r严格模式\rHive 通过参数 hive.mapred.mode 来设置是否开启严格模式。目前参数值有两个：strict（严格模式）和 nostrict（非严格模式，默认）。\r开启严格模式，主要是为了禁止某些查询（这些查询可能会造成意想不到的坏结果），目前主要禁止三种类型的查\r询：\r分区表查询时，必须在 WHERE 语句后指定分区字段，否则不允许执行。因为在查询分区表时，如果不指定分区查\r询，会进行全表扫描。而分区表通常有非常大的数据量，全表扫描非常消耗资源。\rORDER BY 查询必须带有 LIMIT 语句，否则不允许执行。因为 ORDER BY 会进行全局排序，这个过程会将处理的结果分配到一个 Reduce 中进行处理，处理时间长且影响性能。\r笛卡尔积查询（多使用 JOIN 和 ON 语句查询）。数据量非常大时，笛卡尔积查询会出现不可控的情况，因此严格模式下也不允许执行。\r在开启严格模式下，进行上述三种不符合要求的查询时，通常会报类似 FAILED: Error in semantic analysis:In strict mode, XXX is not allowed. If you really want to perform the operation,+set hive.mapred.mode=nonstrict+ 的错误。\r开启严格模式命令如下：\r-- 是否开启严格模式 strict（严格模式）和 nostrict（非严格模式，默认）\rSET hive.mapred.mode=strict;\t推测执行\r推测执行(Speculative Execution)是指在集群环境下运行 MapReduce，可能是程序 Bug，负载不均或者其他的一些问题，导致在一个 Job 下的多个 Task 速度不一致，比如有的任务已经完成，但是有些任务可能只跑了 10%，根据木桶原理，这些任务将成为整个 Job 的短板，如果集群启动了推测执行，这时为了最大限度的提高短板，Hadoop 会为该 Task 启动备份任务，让 Speculative Task 与原始 Task 同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果，并且在运行完成后 Kill 掉另外一个任务。\r推测执行(Speculative Execution)是通过利用更多的资源来换取时间的一种优化策略，但是在资源很紧张的情况下，推测执行也不一定能带来时间上的优化。所以是否启用推测执行，需要根据资源情况来决定，如果在资源本身就不够的情况下，还要跑推测执行的任务，这样会导致后续启动的任务无法获取到资源，以导致无法执行。\r关于推测执行相关配置如下：\r-- 是否启用 MapTask 推测执行，默认为 true\rSET mapreduce.map.speculative=true;\r-- 是否启用 ReduceTask 推测执行，默认为 true\rSET mapreduce.reduce.speculative=true;\r-- 推测任务占当前正在运行的任务数的比例，默认为 0.1\rSET mapreduce.job.speculative.speculative-cap-running-tasks=0.1;\r-- 推测任务占全部要处理任务数的比例，默认为 0.01\rSET mapreduce.job.speculative.speculative-cap-total-tasks=0.01\r-- 最少允许同时运行的推测任务数量，默认为 10\rSET mapreduce.job.speculative.minimum-allowed-tasks=10;\r-- 本次推测没有任务下发，执行下一次推测任务的等待时间，默认为 1000（ms）\rSET mapreduce.job.speculative.retry-after-no-speculate=1000;\r-- 本次推测有任务下发，执行下一次推测任务的等待时间，默认为 15000（ms）\rSET mapreduce.job.speculative.retry-after-speculate=15000;\r-- 标准差，任务的平均进展率必须低于所有正在运行任务的平均值才会被认为是太慢的任务，默认为 1.0\rSET mapreduce.job.speculative.slowtaskthreshold=1.0;\r-- Hive 是否启用 MapReduce 推测执行，默认为 true\rSET hive.mapred.reduce.tasks.speculative.execution=true; ","externalUrl":null,"permalink":"/docs/hive/hive/","section":"Docs","summary":"Hive 3.","title":"","type":"docs"},{"content":"Linux # 配置虚拟机 # 自定义分区 # ​\t/boot引导分区，256M；swap硬盘当内存使用，2G；/剩余所有空间。\n配置网络 # ​\t输入以下命令代开配置文件：vi /etc/sysconfig/network-scripts/ifcfg-ens33。出现以下内容：\nTYPE=Ethernet # 网卡类型（通常是 Ethernet 以太网） PROXY_METHOD=none # 代理方式：为关闭状态 BROWSER_ONLY=no # 只是浏览器：否 BOOTPROTO=static # 网卡的引导协议（static：静态IP dhcp：动态IP none：不指定，不指定容易出现各种各样的网络受限） DEFROUTE=yes # 默认路由 IPV4_FAILURE_FATAL=no # 是否开启 IPV4 致命错误检测 IPV6INIT=yes # IPV6 是否自动初始化：是（现在还未用到 IPV6，不会有任何影响） IPV6_AUTOCONF=yes # IPV6 是否自动配置：是 IPV6_DEFROUTE=yes # IPV6 是否可以为默认路由：是 IPV6_FAILURE_FATAL=no # 是否开启 IPV6 致命错误检测 IPV6_ADDR_GEN_MODE=stable-privacy # IPV6 地址生成模型 NAME=ens33 # 网卡物理设备名称 UUID=070892f0-514c-46c5-8593-c379854dcce1 # 通用唯一识别码，每一个网卡都会有，不能重复，否则两台 Linux 机器只有一台可上网 DEVICE=ens33 # 网卡设备名称，必须和‘NAME’的值一样 ONBOOT=yes # 是否开机启动网络，要想网卡开机就启动或通过`systemctl restart network`控制网卡，必 须设置为 yes 修改 # ONBOOT = yes——是否开机启动网络\nBOOTPROTO = static——使用静态网络IP，dhcp表示动态获取网络IP，也就是自动获取\n删除 # UUID = xxxx——每一个网卡都会有，不能重复，否则两台Linux机器只有一台可以上网\n新增 # IPADDR=192.168.88.100 # 本机 IP NETMASK=255.255.255.0 # 子网掩码 GATEWAY=192.168.88.2 # 默认网关 DNS1=192.168.88.2 # DNS 服务器 DNS2=114.114.114.114 # DNS 服务器 DNS3=8.8.8.8 # DNS 服务器 测试 # 配置完以后输入重启网络——systemctl restart network\n然后 ping 一下百度，如果出现以下效果则表示网络配置成功（Ctrl + C 终止 ping 命令的执行）。\n防火墙 # systemctl status firewalld——查看防火墙状态\nsystemctl stop firewalld——本次服务关闭防火墙，重启后会自动开启\nsystemctl start firewalld ——启动防火墙\nststemctl restart firewalld——重启防火墙\nsystemctl disable firewalld ——禁用防火墙\nSELINUX # 使用一下命令打开SRLinux的配置文件：vi /etc/selinux/config\nenforcing：强制执行\npermissive：放行\ndisabled：禁用\n修改 SELinux=disabled ，然后 :wq 保存并退出。\n修改主机名和hosts # 修改主机名：vi /etc/hostname\n设置网络地址别名：vi /etc/hosts\n关闭虚拟机 # halt ：相当于直接拔掉电源，不推荐。\npoweroff ：直接关闭机器，但是有可能当前虚拟机有其他用户正在使用，不推荐。\nshutdown -h now ：马上关闭计算机 ，但是会给其他用户发送消息，推荐。\nreboot ：重启虚拟机。\n万事万物皆文件 # 文件目录 # /bin # ​\tBinary的缩写，存放着最经常使用的命令文件；软连接，实际地址是 /usr/bin ；普通用户和管理员都可以运行。\n/etc # ​\t存放所有的系统管理所需要的配置文件和子目录；Linux操作系统配置文件所在地。\n/home # ​\t普通用户的家目录，每个用户都有一个自己的目录，一般以用户的账号命名\n/opt # ​\t非系统盘，额外安装软件所存放的目录，默认为空。\n/root # ​\t超级管理员用户的主目录\n/usr # ​\t类似Windows系统的系统盘，用户的很多应用程序和文件都放在这个目录下\n/var # ​\t经常被修改的文件放在这里，动态数据文件，例如日志、数据库文件。\n1、/\tLinux文件系统的根目录\n2、/boot\t引导分区\n3、/dev ; /media; /mnt\t第三方挂载目录\n4、/lib-\u0026gt;/usr/lib；lib64-\u0026gt;/usr/lib64\t操作系统所需要的系统资源\n5、/proc\t内存相关文件\n6、/run\t操作系统进程相关文件\n7、/sbin-\u0026gt;/usr/sbin\t管理员才可以运行的命令文件，系统守护程序\n8、/srv\t非系统盘，服务启动后需要提取的一些数据\n9、/sys\t操作系统相关文件\n10、/tmp\t临时目录，重启时会清空\n文件操作 # Linux 命令在线查询：https://www.linuxcool.com/\ncd（change directory） # 更改当前所处的工作目录，路径可以是绝对/相对路径，不写则回到家目录\ncd 参数名\t目录名\n参数： -L 切换至符号链接所在目录 ~ 切换至用户的家目录 -p 软连接，切换到实际目录 .. 返回到当前位置的上一级目录 \u0026ndash; 回到上次所在目录 ls/ll（list） # 列出，显示目录中的文件及其属性信息。\nls\t参数\t文件名字\nll 是 “ls -l -color =auto”\t的别名\n参数 -a 显示所有文件及目录 -r 根据首字母蒋文杰反序显示 -A 显示除\u0026rsquo;.\u0026lsquo;和\u0026rsquo;..\u0026lsquo;外的所有文件 -R 地柜显示所有子文件 -d 显示目录自身的属性信息 -S 根据文件大小排序 -i 显示文件的inode属性信息 -t 依据最后修改时间按排序 -l 显示文件的详细属性信息 -X 依据拓展名将文件排序显示 -m 用逗号间隔，水平显示文件信息 -color 彩色显示信息 mkdir（make directories) # 创建目录，不会覆盖已有文件，可以一次性同时创建多个目录\nmadir\t参数\t目录名\n参数 -m 创建目录的视同设置权限 -v 显示执行过程的详细信息 -p 递归创建多级目录 -z 设置目录安全上下文 rmdir（remove directory） # 删除空目录文件，非空白需要带-R参数，递归删除要求子目录必须是空的\nrmdir\t参数\t目录名\n参数 -p 递归处理所有子文件 \u0026ndash;help 显示帮助信息 -v 显示执行过程中详细信息 \u0026ndash;version 显示版本信息 cp（copy） # 能够将一个或多个文件/目录复制到指定位置，并重命名，会直接覆盖\ncp\t参数\t原文件名\t目标文件名\n参数 -a pdr参数组合 -l 对源文件建立硬链接 -b 覆盖目标文件前先备份 -p 保留原文件或目录的所有信息 -d 复制时，目标文件也建立为链接文件 -r* 递归复制所有子文件 -f* 目标文件已存在，直接覆盖 -s 建立软连接，非复制文件 -i 目标文件存在，会询问是否覆盖 -v 显示执行过程详细信息 mv（move） # 对文件进行剪切和重命名，注意区别CP，同文件夹为重命名操作\nmv\t参数\t源文件名\t目标文件名\n参数 -b 覆盖前给目标文件创立备份 -v 显示执行过程详细信息 -f 直接覆盖目标文件，不询问 -z 设置文件安全上下文 -i 覆盖目标文件前，询问 \u0026ndash;help 显示帮助信息 -n 不覆盖已有文件 \u0026ndash;version 显示版本信息 -u 源文件比目标文件更新时覆盖 rm（remove） # 删除文件或目录，可以递归删除子文件，也可以同时删除多个\nrm\t参数\t文件名\n参数 -d 删除位子文件的空目录 -v 显示执行过程详细信息 -f 删除文件不询问 \u0026ndash;help 显示帮助信息 -i 删除前询问用户确认 \u0026ndash;version 显示版本信息 -r 递归删除目录及其子文件 touch # 创建空文件夹和修改时间戳，文件存在则会对Atime访问时间和Ctime修改时间进行修改\ntouch\t参数\t文件名\n参数 -a 设置文件的读取时间记录 -t 设置文件的时间记录 -c 不创建新文件 \u0026ndash;help 显示帮助信息 -d 设置时间和日期 \u0026ndash;version 显示版本信息 -m 设置文件的修改时间记录 stat（status） # 显示文件的状态信息，最后访问时间ATIME，最后修改时间MTIME，最后更改时间CTIME\nstat\t参数\t文件名\n参数 -c 设置显示格式 -Z 显示SElinux安全上下文值 -f 显示文件系统信息 \u0026ndash;help 显示帮助信息 -L 支持符号链接 \u0026ndash;version 显示版本信息 -t 设置以简洁方式显示 ln（link） # 在另一个位置建立同步连接，分为软连接和硬链接。软连接相当于快捷方式，硬链接复制了文件的inode属性。\nln\t参数\t原文件名\t目标文件名\n参数 -b 为已存在的目标文件创建备份 -s 对源文件创建软连接 -f 强制创建连接而不询问 读取文件信息 # cat（concatenate files and print） # 在终端设备上显示文件内容，适合内容较少的纯文本。tac反向cat\ncat\t参数\t文件名\n参数 -b 显示行数， -E 每行结束处显示$符号 -n 显示行数，包括空行 -s 显示行数，多个空行算一个 # less\nmore分页显示文本文件。Enter下一行，Space下一页不可以回退。less可以从后向前浏览\nmore\t参数\t文件名\nless\t参数\t文件名\nmore -p 清屏后显示文本文件内容 -s 多个空行压缩成一行显示 less -y 设置向前滚最大行数 head # 显示文件开头的内容，默认为前10行\nhead\t参数\t文件名\n参数 -c 设置显示头部内容的字符数 -v 显示文件名的头信息 -n 设置显示行数 -q 不现实文件名头信息 tail # 查看文件尾部内容，默认在终端显示指定文件末尾10行。可以使用管道|。inode变化后-f会失效，-F不会。\ntail\t参数\t文件名\n参数 -f 持续显示文件尾部最新内容 -c 设置显示文件尾部的字符数 -n 设置显示文件尾部的行数 find # 根据给定的路径和条件找茬相关文件，模糊搜索\nfind\t路径\t条件\t文件名\n参数 -name 匹配文件名 -user 匹配文件所属主 -type 匹配文件类型 -size 匹配文件大小 -mtime 匹配最后修改文件内容时间 -atime 匹配最后读取文件时间 vi and vim # vi\t文件名；进入文件命令模式，i插入；a追加；I行首；A行末；o下一行；O上一行。\n命令模式 # 指令 G：跳转文件末尾 gg：跳转文件第一行 数字gg：跳转指定行 ^:跳转行首 $：跳转行末 w：调到下个单词 yw：复制一个单词 yy：复制一行 p：黏贴 dw：删除一个单词 dd：删除一行 u：回退上一步 ctrl+r：回退u执行的 .：重复上一步 x：剪切 r：替换 编辑模式 # 低行模式 # 指令 :set nu 打开行号 :set nonu 关闭行号 /要查找的内容 :w 保存 ;q 退出 :wq 保存并退出 q! 强制退出 s/要查找的内容/替换为的内容/修饰符\t/i忽略大小写\t/g替换当前行所有\n打开文件夹 # vi 文件名 ：正常打开。 vi +8 文件名 ：打开文件并跳转至第 8 行。 vi + 文件名 ：打开文件并跳转至最后一行。 vi +/要查找的内容 文件名 ：打开文件并搜索指定的字符串。\tn ：查找下一个。\tN ：查找上一个\n数据传输 # scp（secure copy） # 基于SSH协议远程复制文件，可以在Linux系统之间复制文件，数据全部加密。源数据地址失灵一台服务器，可以把别的服务器文件拷贝到自己本机。不同步，一端删除不影响另一端。\nscp\t参数\t源数据地址\t远程服务器信息\n参数 -C 允许传输过程中压缩 -r 递归传输整个目录 -p 保留文件的stat属性 -P 制定远程主机的端口号 -l 设置宽带限制 -v 显示执行过程详细信息 rsync（remote sync） # 远程数据同步，传送前进行文件比较，只传送内容有差异的部分。使用前需要先安装\nyum -y stall sync rsync\t参数\t源数据地址\t远程服务器信息\n参数 -a 归档模式，保持所有文件属性 -v 显示过程中的详细信息 -z 传输时进行压缩 -P \u0026ndash;partial恢复终端的传输 两个参数 \u0026ndash;progress显示传输进度 区别： 1、scp总是安全的，rsync不是加密传输，只有通过ssh传输才能安全。\n2、传输大文件，在传输之前断开时，rsync会从中断的地方继续传输，scp不会\n3、rsync默认是只拷贝有改变的文件，scp是全覆盖。\nscp和rsync在文件夹均不存在时，执行时间相差不大，文件夹存在的情况下想差异很大。原因是scp是复制后强制全部覆盖。rsync是tongue，先比较两边文件差异，再将有差异的文件进行更新。\n文件压缩 # tar # 功能是压缩和解压缩，7以后解压缩不填价格是参数。\ntar\t参数\t压缩包名\t文件或目录名\ntar\t-zxf\tfile1\t-C\t目录地址\t解压缩文件file1到目录地址\ntar\t-zcf\t压缩后的名字\t源文件\n参数 -z 使用gzip压缩格式 -x 从压缩包中解压缩 -v 显示执行过程详细信息 -f 制定压缩包文件 -c 压缩 -C 制定解压缩的文件目录 zip和unzip # yum -y install zip unzip 压缩和解压缩；使用\u0026ndash;help\nLinux网络信息 # 主机名称 # hostname\tbdp\t临时修改\nvi /etc/hostname\t长久修改\n修改主机域名 # vi /etc/hosts\t192.168.10.100\t别名\n服务器之间相互免密钥 # 散列算法 -\u0026gt; 不可逆 -\u0026gt; 完整性校验 -\u0026gt; 账户密码加密 -\u0026gt; 虚拟货币加密\n对称算法 -\u0026gt; 密钥解密 -\u0026gt; 不安全\n非对称算法 -\u0026gt; 一对公私钥 -\u0026gt; 公钥加密，私钥解密 -\u0026gt; 私钥加密，公钥解密\n生成密钥对 # ssh-keygen -t rsa -P \u0026#39;\u0026#39; -f ~/.ssh/id_rsa 运行以上命令后会在 ~/.ssh/ 目录下生成一对密钥对。\n拷贝公钥 # ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.88.102 时间和日期 # 时间命令 # 查看时区：\tll /etc/loclatime\n设置时区：\tln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\n查看当前系统时间：\tdate\n查看日历：\tcal\n修改时间：\tdate -s 2019-11-11 11:11:11\n日期自动同步 # 安装时间同步的服务：\tyum -y install ntp\n自动同步网络时间中心：\tntpdate cn.ntp.org.cn\n用户-组-权限 # 用户 # 命令可以用来创建新用户或更改用户信息。账号建好之后再用设定账号的密码，建立的账号保存在/etc/passwd文本中\nuseradd [选项] [用户名]\n选项 -D 改变新用户的默认值 -c 添加备注文字 -d 设置新用户家目录 -e 用户终止日期 -f 过期几天后停止权利 -g 用户对应用户组 ","externalUrl":null,"permalink":"/docs/linux/linux/","section":"Docs","summary":"\u003ch1 class=\"relative group\"\u003eLinux \n    \u003cdiv id=\"linux\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#linux\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\n\n\u003ch2 class=\"relative group\"\u003e配置虚拟机 \n    \u003cdiv id=\"%E9%85%8D%E7%BD%AE%E8%99%9A%E6%8B%9F%E6%9C%BA\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E9%85%8D%E7%BD%AE%E8%99%9A%E6%8B%9F%E6%9C%BA\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e自定义分区 \n    \u003cdiv id=\"%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003e​\t\t/boot引导分区，256M；swap硬盘当内存使用，2G；/剩余所有空间。\u003c/p\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e配置网络 \n    \u003cdiv id=\"%E9%85%8D%E7%BD%AE%E7%BD%91%E7%BB%9C\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E9%85%8D%E7%BD%AE%E7%BD%91%E7%BB%9C\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003e​\t\t输入以下命令代开配置文件：vi /etc/sysconfig/network-scripts/ifcfg-ens33。出现以下内容：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-vim\" data-lang=\"vim\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eTYPE\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eEthernet\u003c/span\u003e     # \u003cspan class=\"nx\"\u003e网卡类型\u003c/span\u003e（\u003cspan class=\"nx\"\u003e通常是\u003c/span\u003e \u003cspan class=\"nx\"\u003eEthernet\u003c/span\u003e \u003cspan class=\"nx\"\u003e以太网\u003c/span\u003e）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003ePROXY_METHOD\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003enone\u003c/span\u003e   # \u003cspan class=\"nx\"\u003e代理方式\u003c/span\u003e：\u003cspan class=\"nx\"\u003e为关闭状态\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eBROWSER_ONLY\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eno\u003c/span\u003e    # \u003cspan class=\"nx\"\u003e只是浏览器\u003c/span\u003e：\u003cspan class=\"nx\"\u003e否\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eBOOTPROTO\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003estatic\u003c/span\u003e   # \u003cspan class=\"nx\"\u003e网卡的引导协议\u003c/span\u003e（\u003cspan class=\"nx\"\u003estatic\u003c/span\u003e：\u003cspan class=\"nx\"\u003e静态IP\u003c/span\u003e \u003cspan class=\"nx\"\u003edhcp\u003c/span\u003e：\u003cspan class=\"nx\"\u003e动态IP\u003c/span\u003e \u003cspan class=\"nx\"\u003enone\u003c/span\u003e：\u003cspan class=\"nx\"\u003e不指定\u003c/span\u003e，\u003cspan class=\"nx\"\u003e不指定容易出现各种各样的网络受限\u003c/span\u003e）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eDEFROUTE\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eyes\u003c/span\u003e     # \u003cspan class=\"nx\"\u003e默认路由\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eIPV4_FAILURE_FATAL\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eno\u003c/span\u003e # \u003cspan class=\"nx\"\u003e是否开启\u003c/span\u003e \u003cspan class=\"nx\"\u003eIPV4\u003c/span\u003e \u003cspan class=\"nx\"\u003e致命错误检测\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eIPV6INIT\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eyes\u003c/span\u003e     # \u003cspan class=\"nx\"\u003eIPV6\u003c/span\u003e \u003cspan class=\"nx\"\u003e是否自动初始化\u003c/span\u003e：\u003cspan class=\"nx\"\u003e是\u003c/span\u003e（\u003cspan class=\"nx\"\u003e现在还未用到\u003c/span\u003e \u003cspan class=\"nx\"\u003eIPV6\u003c/span\u003e，\u003cspan class=\"nx\"\u003e不会有任何影响\u003c/span\u003e）\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eIPV6_AUTOCONF\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eyes\u003c/span\u003e   # \u003cspan class=\"nx\"\u003eIPV6\u003c/span\u003e \u003cspan class=\"nx\"\u003e是否自动配置\u003c/span\u003e：\u003cspan class=\"nx\"\u003e是\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eIPV6_DEFROUTE\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eyes\u003c/span\u003e   # \u003cspan class=\"nx\"\u003eIPV6\u003c/span\u003e \u003cspan class=\"nx\"\u003e是否可以为默认路由\u003c/span\u003e：\u003cspan class=\"nx\"\u003e是\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eIPV6_FAILURE_FATAL\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eno\u003c/span\u003e # \u003cspan class=\"nx\"\u003e是否开启\u003c/span\u003e \u003cspan class=\"nx\"\u003eIPV6\u003c/span\u003e \u003cspan class=\"nx\"\u003e致命错误检测\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eIPV6_ADDR_GEN_MODE\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003estable\u003c/span\u003e\u003cspan class=\"p\"\u003e-\u003c/span\u003e\u003cspan class=\"nx\"\u003eprivacy\u003c/span\u003e # \u003cspan class=\"nx\"\u003eIPV6\u003c/span\u003e \u003cspan class=\"nx\"\u003e地址生成模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eNAME\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eens33\u003c/span\u003e      # \u003cspan class=\"nx\"\u003e网卡物理设备名称\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eUUID\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e070892\u003c/span\u003ef\u003cspan class=\"m\"\u003e0-514\u003c/span\u003ec\u003cspan class=\"m\"\u003e-46\u003c/span\u003ec\u003cspan class=\"m\"\u003e5-8593\u003c/span\u003e\u003cspan class=\"p\"\u003e-\u003c/span\u003e\u003cspan class=\"nx\"\u003ec379854dcce1\u003c/span\u003e # \u003cspan class=\"nx\"\u003e通用唯一识别码\u003c/span\u003e，\u003cspan class=\"nx\"\u003e每一个网卡都会有\u003c/span\u003e，\u003cspan class=\"nx\"\u003e不能重复\u003c/span\u003e，\u003cspan class=\"nx\"\u003e否则两台\u003c/span\u003e \u003cspan class=\"nx\"\u003eLinux\u003c/span\u003e \u003cspan class=\"nx\"\u003e机器只有一台可上网\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eDEVICE\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eens33\u003c/span\u003e     # \u003cspan class=\"nx\"\u003e网卡设备名称\u003c/span\u003e，\u003cspan class=\"nx\"\u003e必须和\u003c/span\u003e‘\u003cspan class=\"nx\"\u003eNAME\u003c/span\u003e’\u003cspan class=\"nx\"\u003e的值一样\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eONBOOT\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"nx\"\u003eyes\u003c/span\u003e      # \u003cspan class=\"nx\"\u003e是否开机启动网络\u003c/span\u003e，\u003cspan class=\"nx\"\u003e要想网卡开机就启动或通过\u003c/span\u003e`\u003cspan class=\"nx\"\u003esystemctl\u003c/span\u003e \u003cspan class=\"nx\"\u003erestart\u003c/span\u003e \u003cspan class=\"nx\"\u003enetwork\u003c/span\u003e`\u003cspan class=\"nx\"\u003e控制网卡\u003c/span\u003e，\u003cspan class=\"nx\"\u003e必\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003e须设置为\u003c/span\u003e \u003cspan class=\"nx\"\u003eyes\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\n\u003ch4 class=\"relative group\"\u003e修改 \n    \u003cdiv id=\"%E4%BF%AE%E6%94%B9\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%BF%AE%E6%94%B9\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eONBOOT = yes——是否开机启动网络\u003c/p\u003e\n\u003cp\u003eBOOTPROTO = static——使用静态网络IP，dhcp表示动态获取网络IP，也就是自动获取\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e删除 \n    \u003cdiv id=\"%E5%88%A0%E9%99%A4\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E5%88%A0%E9%99%A4\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003eUUID = xxxx——每一个网卡都会有，不能重复，否则两台Linux机器只有一台可以上网\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e新增 \n    \u003cdiv id=\"%E6%96%B0%E5%A2%9E\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E6%96%B0%E5%A2%9E\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-vim\" data-lang=\"vim\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eIPADDR\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e192\u003c/span\u003e.\u003cspan class=\"m\"\u003e168\u003c/span\u003e.\u003cspan class=\"m\"\u003e88\u003c/span\u003e.\u003cspan class=\"m\"\u003e100\u003c/span\u003e # \u003cspan class=\"nx\"\u003e本机\u003c/span\u003e \u003cspan class=\"nx\"\u003eIP\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eNETMASK\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e255\u003c/span\u003e.\u003cspan class=\"m\"\u003e255\u003c/span\u003e.\u003cspan class=\"m\"\u003e255\u003c/span\u003e.\u003cspan class=\"m\"\u003e0\u003c/span\u003e # \u003cspan class=\"nx\"\u003e子网掩码\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eGATEWAY\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e192\u003c/span\u003e.\u003cspan class=\"m\"\u003e168\u003c/span\u003e.\u003cspan class=\"m\"\u003e88\u003c/span\u003e.\u003cspan class=\"m\"\u003e2\u003c/span\u003e # \u003cspan class=\"nx\"\u003e默认网关\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eDNS1\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e192\u003c/span\u003e.\u003cspan class=\"m\"\u003e168\u003c/span\u003e.\u003cspan class=\"m\"\u003e88\u003c/span\u003e.\u003cspan class=\"m\"\u003e2\u003c/span\u003e   # \u003cspan class=\"nx\"\u003eDNS\u003c/span\u003e \u003cspan class=\"nx\"\u003e服务器\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eDNS2\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e114\u003c/span\u003e.\u003cspan class=\"m\"\u003e114\u003c/span\u003e.\u003cspan class=\"m\"\u003e114\u003c/span\u003e.\u003cspan class=\"m\"\u003e114\u003c/span\u003e  # \u003cspan class=\"nx\"\u003eDNS\u003c/span\u003e \u003cspan class=\"nx\"\u003e服务器\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eDNS3\u003c/span\u003e\u003cspan class=\"p\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e8\u003c/span\u003e.\u003cspan class=\"m\"\u003e8\u003c/span\u003e.\u003cspan class=\"m\"\u003e8\u003c/span\u003e.\u003cspan class=\"m\"\u003e8\u003c/span\u003e      # \u003cspan class=\"nx\"\u003eDNS\u003c/span\u003e \u003cspan class=\"nx\"\u003e服务器\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\n\u003ch4 class=\"relative group\"\u003e测试 \n    \u003cdiv id=\"%E6%B5%8B%E8%AF%95\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E6%B5%8B%E8%AF%95\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e配置完以后输入重启网络——systemctl restart network\u003c/p\u003e\n\u003cp\u003e然后 ping 一下百度，如果出现以下效果则表示网络配置成功（Ctrl + C 终止 ping 命令的执行）。\u003c/p\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e防火墙 \n    \u003cdiv id=\"%E9%98%B2%E7%81%AB%E5%A2%99\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E9%98%B2%E7%81%AB%E5%A2%99\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003esystemctl status firewalld——查看防火墙状态\u003c/p\u003e\n\u003cp\u003esystemctl stop firewalld——本次服务关闭防火墙，重启后会自动开启\u003c/p\u003e\n\u003cp\u003esystemctl start firewalld ——启动防火墙\u003c/p\u003e\n\u003cp\u003eststemctl restart firewalld——重启防火墙\u003c/p\u003e\n\u003cp\u003esystemctl disable firewalld ——禁用防火墙\u003c/p\u003e\n\n\n\u003ch3 class=\"relative group\"\u003eSELINUX \n    \u003cdiv id=\"selinux\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#selinux\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003e使用一下命令打开SRLinux的配置文件：vi /etc/selinux/config\u003c/p\u003e\n\u003cp\u003eenforcing：强制执行\u003c/p\u003e\n\u003cp\u003epermissive：放行\u003c/p\u003e\n\u003cp\u003edisabled：禁用\u003c/p\u003e\n\u003cp\u003e修改 SELinux=disabled ，然后 :wq 保存并退出。\u003c/p\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e修改主机名和hosts \n    \u003cdiv id=\"%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D%E5%92%8Chosts\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D%E5%92%8Chosts\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003e修改主机名：vi /etc/hostname\u003c/p\u003e\n\u003cp\u003e设置网络地址别名：vi /etc/hosts\u003c/p\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e关闭虚拟机 \n    \u003cdiv id=\"%E5%85%B3%E9%97%AD%E8%99%9A%E6%8B%9F%E6%9C%BA\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E5%85%B3%E9%97%AD%E8%99%9A%E6%8B%9F%E6%9C%BA\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003ehalt ：相当于直接拔掉电源，不推荐。\u003c/p\u003e\n\u003cp\u003epoweroff ：直接关闭机器，但是有可能当前虚拟机有其他用户正在使用，不推荐。\u003c/p\u003e\n\u003cp\u003eshutdown -h now ：马上关闭计算机 ，但是会给其他用户发送消息，推荐。\u003c/p\u003e\n\u003cp\u003ereboot ：重启虚拟机。\u003c/p\u003e\n\n\n\u003ch2 class=\"relative group\"\u003e万事万物皆文件 \n    \u003cdiv id=\"%E4%B8%87%E4%BA%8B%E4%B8%87%E7%89%A9%E7%9A%86%E6%96%87%E4%BB%B6\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%B8%87%E4%BA%8B%E4%B8%87%E7%89%A9%E7%9A%86%E6%96%87%E4%BB%B6\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e文件目录 \n    \u003cdiv id=\"%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e/bin \n    \u003cdiv id=\"bin\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#bin\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e​\t\tBinary的缩写，存放着最经常使用的命令文件；软连接，实际地址是 /usr/bin ；普通用户和管理员都可以运行。\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e/etc \n    \u003cdiv id=\"etc\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#etc\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e​\t\t存放所有的系统管理所需要的配置文件和子目录；Linux操作系统配置文件所在地。\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e/home \n    \u003cdiv id=\"home\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#home\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e​\t\t普通用户的家目录，每个用户都有一个自己的目录，一般以用户的账号命名\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e/opt \n    \u003cdiv id=\"opt\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#opt\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e​\t\t非系统盘，额外安装软件所存放的目录，默认为空。\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e/root \n    \u003cdiv id=\"root\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#root\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e​\t\t超级管理员用户的主目录\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e/usr \n    \u003cdiv id=\"usr\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#usr\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e​\t\t类似Windows系统的系统盘，用户的很多应用程序和文件都放在这个目录下\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003e/var \n    \u003cdiv id=\"var\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#var\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e​\t\t经常被修改的文件放在这里，动态数据文件，例如日志、数据库文件。\u003c/p\u003e\n\u003cp\u003e1、/\tLinux文件系统的根目录\u003c/p\u003e\n\u003cp\u003e2、/boot\t引导分区\u003c/p\u003e\n\u003cp\u003e3、/dev ; /media; /mnt\t第三方挂载目录\u003c/p\u003e\n\u003cp\u003e4、/lib-\u0026gt;/usr/lib；lib64-\u0026gt;/usr/lib64\t操作系统所需要的系统资源\u003c/p\u003e\n\u003cp\u003e5、/proc\t内存相关文件\u003c/p\u003e\n\u003cp\u003e6、/run\t操作系统进程相关文件\u003c/p\u003e\n\u003cp\u003e7、/sbin-\u0026gt;/usr/sbin\t管理员才可以运行的命令文件，系统守护程序\u003c/p\u003e\n\u003cp\u003e8、/srv\t非系统盘，服务启动后需要提取的一些数据\u003c/p\u003e\n\u003cp\u003e9、/sys\t操作系统相关文件\u003c/p\u003e\n\u003cp\u003e10、/tmp\t临时目录，重启时会清空\u003c/p\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e文件操作 \n    \u003cdiv id=\"%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003eLinux 命令在线查询：https://www.linuxcool.com/\u003c/p\u003e\n\n\n\u003ch4 class=\"relative group\"\u003ecd（change directory） \n    \u003cdiv id=\"cdchange-directory\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#cdchange-directory\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e更改当前所处的工作目录，路径可以是绝对/相对路径，不写则回到家目录\u003c/p\u003e\n\u003cp\u003ecd \t参数名\t目录名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数：\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-L\u003c/td\u003e\n\u003ctd\u003e切换至符号链接所在目录\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e~\u003c/td\u003e\n\u003ctd\u003e切换至用户的家目录\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-p\u003c/td\u003e\n\u003ctd\u003e软连接，切换到实际目录\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e..\u003c/td\u003e\n\u003ctd\u003e返回到当前位置的上一级目录\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u0026ndash;\u003c/td\u003e\n\u003ctd\u003e回到上次所在目录\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003els/ll（list） \n    \u003cdiv id=\"lslllist\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#lslllist\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e列出，显示目录中的文件及其属性信息。\u003c/p\u003e\n\u003cp\u003els\t参数\t文件名字\u003c/p\u003e\n\u003cp\u003ell 是 “ls -l -color =auto”\t的别名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-a\u003c/td\u003e\n\u003ctd\u003e显示所有文件及目录\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e-r\u003c/td\u003e\n\u003ctd\u003e根据首字母蒋文杰反序显示\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-A\u003c/td\u003e\n\u003ctd\u003e显示除\u0026rsquo;.\u0026lsquo;和\u0026rsquo;..\u0026lsquo;外的所有文件\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e-R\u003c/td\u003e\n\u003ctd\u003e地柜显示所有子文件\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-d\u003c/td\u003e\n\u003ctd\u003e显示目录自身的属性信息\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e-S\u003c/td\u003e\n\u003ctd\u003e根据文件大小排序\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-i\u003c/td\u003e\n\u003ctd\u003e显示文件的inode属性信息\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e-t\u003c/td\u003e\n\u003ctd\u003e依据最后修改时间按排序\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-l\u003c/td\u003e\n\u003ctd\u003e显示文件的详细属性信息\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e-X\u003c/td\u003e\n\u003ctd\u003e依据拓展名将文件排序显示\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-m\u003c/td\u003e\n\u003ctd\u003e用逗号间隔，水平显示文件信息\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e-color\u003c/td\u003e\n\u003ctd\u003e彩色显示信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003emkdir（make directories) \n    \u003cdiv id=\"mkdirmake-directories\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#mkdirmake-directories\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e创建目录，不会覆盖已有文件，可以一次性同时创建多个目录\u003c/p\u003e\n\u003cp\u003emadir\t参数\t目录名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-m\u003c/td\u003e\n\u003ctd\u003e创建目录的视同设置权限\u003c/td\u003e\n\u003ctd\u003e-v\u003c/td\u003e\n\u003ctd\u003e显示执行过程的详细信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-p\u003c/td\u003e\n\u003ctd\u003e递归创建多级目录\u003c/td\u003e\n\u003ctd\u003e-z\u003c/td\u003e\n\u003ctd\u003e设置目录安全上下文\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003ermdir（remove directory） \n    \u003cdiv id=\"rmdirremove-directory\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#rmdirremove-directory\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e删除空目录文件，非空白需要带-R参数，递归删除要求子目录必须是空的\u003c/p\u003e\n\u003cp\u003ermdir\t参数\t目录名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-p\u003c/td\u003e\n\u003ctd\u003e递归处理所有子文件\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;help\u003c/td\u003e\n\u003ctd\u003e显示帮助信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-v\u003c/td\u003e\n\u003ctd\u003e显示执行过程中详细信息\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;version\u003c/td\u003e\n\u003ctd\u003e显示版本信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003ecp（copy） \n    \u003cdiv id=\"cpcopy\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#cpcopy\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e能够将一个或多个文件/目录复制到指定位置，并重命名，会直接覆盖\u003c/p\u003e\n\u003cp\u003ecp\t参数\t原文件名\t目标文件名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-a\u003c/td\u003e\n\u003ctd\u003epdr参数组合\u003c/td\u003e\n\u003ctd\u003e-l\u003c/td\u003e\n\u003ctd\u003e对源文件建立硬链接\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-b\u003c/td\u003e\n\u003ctd\u003e覆盖目标文件前先备份\u003c/td\u003e\n\u003ctd\u003e-p\u003c/td\u003e\n\u003ctd\u003e保留原文件或目录的所有信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-d\u003c/td\u003e\n\u003ctd\u003e复制时，目标文件也建立为链接文件\u003c/td\u003e\n\u003ctd\u003e-r*\u003c/td\u003e\n\u003ctd\u003e递归复制所有子文件\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-f*\u003c/td\u003e\n\u003ctd\u003e目标文件已存在，直接覆盖\u003c/td\u003e\n\u003ctd\u003e-s\u003c/td\u003e\n\u003ctd\u003e建立软连接，非复制文件\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-i\u003c/td\u003e\n\u003ctd\u003e目标文件存在，会询问是否覆盖\u003c/td\u003e\n\u003ctd\u003e-v\u003c/td\u003e\n\u003ctd\u003e显示执行过程详细信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003emv（move） \n    \u003cdiv id=\"mvmove\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#mvmove\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e对文件进行剪切和重命名，注意区别CP，同文件夹为重命名操作\u003c/p\u003e\n\u003cp\u003emv\t参数\t源文件名\t目标文件名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-b\u003c/td\u003e\n\u003ctd\u003e覆盖前给目标文件创立备份\u003c/td\u003e\n\u003ctd\u003e-v\u003c/td\u003e\n\u003ctd\u003e显示执行过程详细信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-f\u003c/td\u003e\n\u003ctd\u003e直接覆盖目标文件，不询问\u003c/td\u003e\n\u003ctd\u003e-z\u003c/td\u003e\n\u003ctd\u003e设置文件安全上下文\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-i\u003c/td\u003e\n\u003ctd\u003e覆盖目标文件前，询问\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;help\u003c/td\u003e\n\u003ctd\u003e显示帮助信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-n\u003c/td\u003e\n\u003ctd\u003e不覆盖已有文件\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;version\u003c/td\u003e\n\u003ctd\u003e显示版本信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-u\u003c/td\u003e\n\u003ctd\u003e源文件比目标文件更新时覆盖\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003erm（remove） \n    \u003cdiv id=\"rmremove\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#rmremove\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e删除文件或目录，可以递归删除子文件，也可以同时删除多个\u003c/p\u003e\n\u003cp\u003erm\t参数\t文件名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-d\u003c/td\u003e\n\u003ctd\u003e删除位子文件的空目录\u003c/td\u003e\n\u003ctd\u003e-v\u003c/td\u003e\n\u003ctd\u003e显示执行过程详细信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-f\u003c/td\u003e\n\u003ctd\u003e删除文件不询问\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;help\u003c/td\u003e\n\u003ctd\u003e显示帮助信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-i\u003c/td\u003e\n\u003ctd\u003e删除前询问用户确认\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;version\u003c/td\u003e\n\u003ctd\u003e显示版本信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-r\u003c/td\u003e\n\u003ctd\u003e递归删除目录及其子文件\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003etouch \n    \u003cdiv id=\"touch\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#touch\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e创建空文件夹和修改时间戳，文件存在则会对Atime访问时间和Ctime修改时间进行修改\u003c/p\u003e\n\u003cp\u003etouch\t参数\t文件名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-a\u003c/td\u003e\n\u003ctd\u003e设置文件的读取时间记录\u003c/td\u003e\n\u003ctd\u003e-t\u003c/td\u003e\n\u003ctd\u003e设置文件的时间记录\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-c\u003c/td\u003e\n\u003ctd\u003e不创建新文件\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;help\u003c/td\u003e\n\u003ctd\u003e显示帮助信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-d\u003c/td\u003e\n\u003ctd\u003e设置时间和日期\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;version\u003c/td\u003e\n\u003ctd\u003e显示版本信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-m\u003c/td\u003e\n\u003ctd\u003e设置文件的修改时间记录\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003estat（status） \n    \u003cdiv id=\"statstatus\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#statstatus\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e显示文件的状态信息，最后访问时间ATIME，最后修改时间MTIME，最后更改时间CTIME\u003c/p\u003e\n\u003cp\u003estat\t参数\t文件名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-c\u003c/td\u003e\n\u003ctd\u003e设置显示格式\u003c/td\u003e\n\u003ctd\u003e-Z\u003c/td\u003e\n\u003ctd\u003e显示SElinux安全上下文值\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-f\u003c/td\u003e\n\u003ctd\u003e显示文件系统信息\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;help\u003c/td\u003e\n\u003ctd\u003e显示帮助信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-L\u003c/td\u003e\n\u003ctd\u003e支持符号链接\u003c/td\u003e\n\u003ctd\u003e\u0026ndash;version\u003c/td\u003e\n\u003ctd\u003e显示版本信息\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-t\u003c/td\u003e\n\u003ctd\u003e设置以简洁方式显示\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch4 class=\"relative group\"\u003eln（link） \n    \u003cdiv id=\"lnlink\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#lnlink\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e在另一个位置建立同步连接，分为软连接和硬链接。软连接相当于快捷方式，硬链接复制了文件的inode属性。\u003c/p\u003e\n\u003cp\u003eln\t参数\t原文件名\t目标文件名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-b\u003c/td\u003e\n\u003ctd\u003e为已存在的目标文件创建备份\u003c/td\u003e\n\u003ctd\u003e-s\u003c/td\u003e\n\u003ctd\u003e对源文件创建软连接\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-f\u003c/td\u003e\n\u003ctd\u003e强制创建连接而不询问\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e读取文件信息 \n    \u003cdiv id=\"%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\n\n\u003ch4 class=\"relative group\"\u003ecat（concatenate files and print） \n    \u003cdiv id=\"catconcatenate-files-and-print\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#catconcatenate-files-and-print\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e在终端设备上显示文件内容，适合内容较少的纯文本。tac反向cat\u003c/p\u003e\n\u003cp\u003ecat\t参数\t文件名\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e参数\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e-b\u003c/td\u003e\n\u003ctd\u003e显示行数，\u003c/td\u003e\n\u003ctd\u003e-E\u003c/td\u003e\n\u003ctd\u003e每行结束处显示$符号\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-n\u003c/td\u003e\n\u003ctd\u003e显示行数，包括空行\u003c/td\u003e\n\u003ctd\u003e-s\u003c/td\u003e\n\u003ctd\u003e显示行数，多个空行算一个\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n\u003ch3 class=\"relative group\"\u003e \n    \u003cdiv id=\"heading\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#heading\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e","title":"","type":"docs"},{"content":" Zookeeper # https://www.processon.com/view/link/64f1a21db08bb5658e7f6b05 集群和分布式 # ​\t集群：建一个任务部署在多个服务器，每个服务器都能独立完成该任务。\n​\t分布式：将一个任务拆分成若干个子任务，每个服务器只能完成某个特定的子任务。\n技术架构的演变 # 单一应用架构 # ​\t通俗的讲（monolith application）就是将应用程序的所有功能都打包成一个独立的单元。\n特点：\r所有的功能集成在一个项目工程中；\r所有的功能打一个 war 包部署到服务器；\r应用与数据库分开部署；\r通过部署应用集群和数据库集群来提高系统的性能。\r优点：\r开发简单：一个 IDE 就可以快速构建单体应用；\r便于共享：单个归档文件包含所有功能，便于在团队之间以及不同的部署阶段之间共享；\r易于测试：单体应用一旦部署，所有的服务或特性就都可以使用了，这简化了测试过程，因为没有额外的依赖，每项测试都可以在部署完成后立刻开始；容易部署：整个项目就一个 war 包，Tomcat 安装好之后，应用扔上去就行了。群化部署也很容易，多个 Tomcat + 一个 Nginx 分分钟搞定。\r缺点：\r妨碍持续交付：随着时间的推移，单体应用可能会变得比较大，构建和部署时间也相应地延长，不利于频繁部署，阻碍持续交付。在移动应用开发中，这个问题会显得尤为严重；\r不够灵活：随着项目的逐渐变大，整个开发流程的时间也会变得很长，即使在仅仅更改了一行代码的情况下，软件开发人员需要花费几十分钟甚至超过一个小时的时间对所有代码进行编译，并接下来花费大量的时间重新部署刚刚生成的产品，以验证自己的更改是否正确。如果多个开发人员共同开发一个应用程序，那么还要等待其他开发人员完成了各自的开发。这降低了团队的灵活性和功能交付频率；\r受技术栈限制：项目变得越来越大的同时，我们的应用所使用的技术也会变得越来越多。这些技术有些是不兼容的，就比如在一个项目中大范围地混合使用 C++ 和 Java 几乎是不可能的事情。在这种情况下，我们就需要抛弃对某些不兼容技术的使用，而选择一种不是那么适合的技术来实现特定的功能；\r可靠性差：某个环节出现了死循环，导致内存溢出，会影响整个项目挂掉；\r伸缩性差：系统的扩容只能针对应用进行扩容，不能做到对某个功能进行扩容，扩容后必然带来资源浪费的问题；\r技术债务：假设我的代码库中有一个混乱的模块结构。此时，我需要添加一个新功能。如果这个模块结构清晰，可能我只需要 2 天时间就可以添加好这个功能，但是如今这个模块的结构很混乱，所以我需要 4 天时间。多出来的这两天就是债务利息。随着时间推移、人员变动，技术债务必然也会随之增多。 垂直应用架构 # ​\t当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。\n特点：\r以单体结构规模的项目为单位进行垂直划分，就是将一个大项目拆分成一个一个单体结构项目；\r项目与项目之间存在数据冗余，耦合性较大，比如上图中三个项目都存在用户信息；\r项目之间的接口多为数据同步功能，如：数据库之间的数据库，通过网络接口进行数据库同步。\r优点：\r开发成本低，架构简单；\r避免单体应用的无限扩大；\r系统拆分实现了流量分担，解决了并发问题；\r可以针对不同系统进行扩容、优化；\r方便水平扩展，负载均衡，容错率提高；\r不同的项目可采用不同的技术；\r系统间相互独立\r缺点：\r系统之间相互调用，如果某个系统的端口或者 IP 地址发生改变，调用系统需要手动变更；\r垂直架构中相同逻辑代码需要不断的复制，不能复用；\r系统性能扩展只能通过扩展集群结点，成本高、有瓶颈。 SOA面向服务架构 # ​\t当垂直应用越来越多，应用之间交互不可避免，将核心业务抽离出来，作为独立的服务，逐渐形成稳定的服务中心。当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需要添加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。\n特点：\r基于 SOA 的架构思想将重复公用的功能抽取为组件，以服务的形式给各系统提供服务；\r各项目（系统）与服务之间采用 WebService、RPC 等方式进行通信；\r使用 ESB 企业服务总线作为项目与服务之间通信的桥梁\r优点：\r将重复的功能抽取为服务，提高开发效率，提高系统的可重用性、可维护性；\r可以针对不同服务的特点制定集群及优化方案；\r采用 ESB 减少系统中的接口耦合。\r缺点：\r系统与服务的界限模糊，不利于开发及维护；\r虽然使用了 ESB，但是服务的接口协议不固定，种类繁多，不利于系统维护；\r抽取的服务的粒度过大，系统与服务之间耦合性高；\r涉及多种中间件，对开发人员技术栈要求高；\r服务关系复杂，运维、测试部署困难。 微服务架构 # ​\t微服务就是讲一个单体架构的应用按业务划分为一个个的独立运行的程序集服务，他们之间通过HTTP协议进行通信，可以采用不同的编程化语言，使用不同的存储技术，自动化部署减少人为控制，降低出错率，服务数量越来越多，管理起来越来越复杂，因此采用集中化管理。\n​\t微服务是一种架构风格，架构是为了解耦，实际使用的是分布式系统开发。一个大型的复杂软件应用，有一个或多个微服务组成。系统中各个为服务可以被单独部署，各个微服务之间是松耦合的。每个微服务仅关注完成一件任务并很好地完成。\n特点：\r将系统服务层完全独立出来，并将服务层抽取为一个一个的微服务；\r微服务中每一个服务都对应唯一的业务能力，遵循单一原则；\r微服务之间采用 RESTful 等轻量协议传输。\r优点：\r团队独立：每个服务都是一个独立的开发团队，这个小团队可以是 2 到 5 人的开发人员组成；\r技术独立：采用去中心化思想，服务之间采用 RESTful 等轻量协议通信，使用什么技术什么语言开发，别人无需干涉；\r前后端分离：采用前后端分离开发，提供统一 Rest 接口，后端不用再为 PC、移动端开发不同接口；\r数据库分离：每个微服务都有自己的存储能力，可以有自己的数据库。也可以有统一数据库；\r服务拆分粒度更细，有利于资源重复利用，提高开发效率；\r一个团队的新成员能够更快投入生产；\r微服务易于被一个开发人员理解，修改和维护，这样小团队能够更关注自己的工作成果。无需通过合作才能体现价值；\r可以更加精准的制定每个服务的优化方案（比如扩展），提高系统可维护性；\r适用于互联网时代，产品迭代周期更短。\r缺点：\r微服务过多，服务治理成本高，不利于系统维护；\r分布式系统开发的技术成本高（网络问题、容错问题、调用关系、分布式事务等），对团队挑战大；\r微服务将原来的函数式调用改为服务调用，不管是用 rpc，还是 http rest 方式，都会增大系统整体延迟。这个是再所难免的，这个就需要我们将原来的串行编程改为并发编程甚至异步编程，增加了技术门槛；多服务运维难度，随着服务的增加，运维的压力也在增大；测试的难度提升。服务和服务之间通过接口来交互，当接口有改变的时候，对所有的调用方都是有影响的，这时自动化测试就显得非常重要了，如果要靠人工一个个接口去测试，那工作量就太大了，所以 API 文档的管理尤为重要。 CAP原则 # 定理 # ​\tCAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得\n特性 定理 Consistency 一致性：也叫做数据源原子性。在分布式系统中，更新操作执行成功后所有的用户访问的都是最新值，这样的系统被认为具有后强一致性，等同于所有节点访问同一份最新的数据。 Availability 可用性：每一个操作总是能够在一定的时间内返回及时结果，一定时间内是指在可以容忍的范围内返回结果，结果可以是成功或失败，且不保证获取的数据是最新数据。 Partition tolerance 分区容错性：分布式系统在遇到任何网络分区故障的时候，仍能够对外提供满足一致性和可用性的服务，除非整个网络环境瘫痪，理解为是否可以对数据进行分区考虑性能和伸缩性。 取舍策略 # CA # 舍弃P（不允许分区），则强一致性和可用性可以保证。分布式节点受限制，无法部署子节点。\nCP # 舍弃A（可用），相当于每个请求都需要在服务器之间保持强一致性，强一致性会导致等待所有数据同步之后才能正常访问服务。即发生网络故障或信息丢失时，会牺牲用户体验，等待所有数据全部同步之后再让用户访问系统。对分布式数据库来说，数据一致性是最基本的要求，因为达不到这个标准，直接采用关系型数据库就好，没必要浪费资源部署分布式。\nAP # 高可用并允许分区，放弃一致性。节点之间可能失去联系，为了高可用性，每个节点只能用本地数据提供服务，这样会导致全局数据的不一致性。先在A方面保证系统可以正常服务，然后在数据一致性方面做了牺牲。\n数据的第一性 # 定义 # 分布式系统通过复制数据来提高系统的可靠性和兼容性，将数据的不同副本存放在不同的机器中。\n在数据有多副本的情况下，如果网络、服务器或者软件出现故障，会导致部分副本写入成功，部分失败。以至于副本之间数据不一致，数据内容出现冲突\n模型 # 强一致性：要求无论更新操作是在哪一个副本执行，之后所有的读操作都要能获得最新的数据\n弱一致性：用户读到某一操作对系统特定数据的更新需要一段时间，这段是时间是“不一致窗口”\n最终一致性：弱一致性的特例，保证用户最终能够读取到某操作对系统特定数据的更新\n​\t从客户端看，可能暂时获取的数据不是最新数据，但是最终还是能访问到最新的\n​\t从服务端看，数据存储复制到整个系统超过半输的节点，保证数据最终一致\nPaxos算法 # 简介 # Paxos算法是Leslie Lamport宗师提出的一种基于消息传递的分布式一致性算法，使其获得2013年图灵奖。 Paxos在1990年提出，被广泛应用于分布式计算中，Google的Chubby，Apache的Zookeeper都是基于它的理论来实现的 Paxos算法解决的问题是分布式一致性问题，即一个分布式系统中的各个进程如何就某个值（决议）达成一致。 传统节点间通信存在着两种通讯模型：共享内存（Shared memory）、消息传递（Messages passing），Paxos是一个基于消息传递的一 致性算法。\nPaxos判断 # 小岛(Island) 服务器集群 议员(Senator) 单台服务器 议员的总数(Senator Count)是确定的 提议(Proposal) 每一次对集群中的数据进行修改 每个提议都有一个编号(PID)，这个编号是一直增长的 每个提议都需要超过半数((Senator Count)/2 +1)的议员同意才能生效 每个议员只会同意大于当前编号的提议 每个议员在自己记事本上面记录的编号，他不断更新这个编号 整个议会不能保证所有议员记事本上的编号总是相同的 议会有一个目标：保证所有的议员对于提议都能达成一致的看法。 前期投票(\u0026gt;1/2)，后期广播(all) Paxos算法 数据的全量备份 弱一致性 =\u0026gt; 最终一致性\n议员总数不能更改；事务变更需要通过一个提议Proposal，每个提议都有个不断增长的PID编号；超过半数议员同意才生效并广播；\n有主模式 # ​\t集群初始化选主和宕机后重新选主\n1、不能太慢，无脑投给编号最大的节点\n2、主的数据一定是集群中最全的，不处于宕机状态（PID和公共相同且处于运行状态）\n3、投票过半原则解决脑裂问题。\n二阶段\n1、询问：发起提议，询问大家是否都接受\n2、广播阶段：询问阶段票数超总服务器过二分之一\n主诞生以后\n1、广播自己的数据至其他节点，解决其他节点数据落后的问题\n2、公共朝代标记递增1\nRaft算法 # 简介 # Raft 适用于一个管理日志一致性的协议，相比于 Paxos 协议 Raft 更易于理解和去实现它。 Raft 将一致性算法分为了几个部分，包括领导选取（leader selection）、日志复制（log replication）、安全（safety） Raft 算法的官网：https://raft.github.io/ 中文版：https://acehi.github.io/thesecretlivesofdata-cn/raft/ 英文版：http://thesecretlivesofdata.com/raft/\n角色分配 # Raft算法将Server划分为3种状态，或者也可以称作角色： Leader 负责Client交互和log复制，同一时刻系统中最多存在1个。 Follower 被动响应请求RPC，从不主动发起请求RPC。 Candidate 一种临时的角色，只存在于leader的选举阶段，某个节点想要变成leader，那么就发起投票请求，同时自己变\t成candidate\n算法流程 # Term\rTerm的概念类比中国历史上的朝代更替，Raft 算法将时间划分成为任意不同长度的任期（term）。\r任期用连续的数字进行表示。每一个任期的开始都是一次选举（election），一个或多个候选人会试图成为领导人。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。Raft 算法保证在给定的一个任期最多只有一个领导人\rRPC\rRaft 算法中服务器节点之间通信使用远程过程调用（RPCs）\r基本的一致性算法只需要两种类型的 RPCs，为了在服务器之间传输快照增加了第三种 RPC。\rRequestVote RPC：候选人在选举期间发起\rAppendEntries RPC：领导人发起的一种心跳机制，复制日志也在该命令中完成\rInstallSnapshot RPC: 领导者使用该RPC来发送快照给太落后的追随者\r日志复制（Log Replication）\r主要用于保证节点的一致性，这阶段所做的操作也是为了保证一致性与高可用性。\r当Leader选举出来后便开始负责客户端的请求，所有事务（更新操作）请求都必须先经过Leader处理\r日志复制（Log Replication）就是为了保证执行相同的操作序列所做的工作。\r在Raft中当接收到客户端的日志（事务请求）后先把该日志追加到本地的Log中\r然后通过heartbeat把该Entry同步给其他Follower，Follower接收到日志后记录日志然后向Leader发送ACK\r当Leader收到大多数（n/2+1）Follower的ACK信息后将该日志设置为已提交并追加到本地磁盘中\r通知客户端并在下个heartbeat中Leader将通知所有的Follower将该日志存储在自己的本地磁盘中。 ZAB协议 # 简介 # ZAB 协议，全称 ZooKeeper Atomic Broadcast（ZooKeeper 原子广播协议）。它是专门为分布式协调服务——ZooKeeper 设计的一种支持崩溃恢复和原子广播的协议。\n​\tZAB 协议借鉴了 Paxos 算法，而 ZooKeeper 正是通过 ZAB 协议来保证分布式事务的最终一致性。基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。\n​\t主备系统架构模型，就是指只有一个 Leader 节点负责处理外部的写事务请求，然后 Leader 节点将数据同步到其他 Follower 节点和Observer 节点。接下来我们详细介绍一下这三种角色。\n三种角色 # Follow 服务器和 Observer 服务器又统称为 Learner 服务器\nLeader # ​\t负责整个ZooKeeper集群工作机制中的核心，主要工作：实物请求的唯一调度和处理者，保证集群事务处理的顺序性，集群内部个服务器的调度者\nFollower # ​\tLeader的追随者，主要工作：处理客户端的非事务请求，转发事务请求给Leader服务器参与事务请求Proposal的投票，参与Leader选举投票\nObserver # ​\t不参与实物请求Proposal的 投票，也不参与Leader选举投票，只提供非事务的服务（查询），通常在不影响集群事务处理能力的前提下提升集群的非事务处理能力。\n两种模式 # 崩溃恢复之数据恢复 # 当整个集群正在启动时，或者当 Leader 节点出现网络中断、崩溃等情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader，当Leader 服务器选举出来后，并且集群中有过半的机器和该 Leader 节点完成数据同步后（同步指的是数据同步，用来保证集群中过半的机器能够和 Leader 服务器的数据状态保持一致），ZAB 协议就会退出恢复模式。\r当集群中已经有过半的 Follower 节点完成了和 Leader 状态同步以后，那么整个集群就进入了消息广播模式。这个时候，在 Leader 节点正常工作时，启动一台新的服务器加入到集群，那这个服务器会直接进入数据恢复模式，和 Leader 节点进行数据同步。同步完成后即可正常对外提供非事务请求的处理。\r在整个消息广播中，Leader 会将每一个事务请求转换成对应的 Proposal 来进行广播，并且在广播事务 Proposal 之前，Leader 服务器会首先为这个事务 Proposal 分配一个全局单递增的唯一ID，称之为事务 ID（即 ZXID），由于 ZAB 协议需要保证每一个消息的严格的顺序关系，因此必须将每一个 Proposal 按照其 ZXID 的先后顺序进行排序和处理 消息广播之原子广播 # 在 ZooKeeper 集群中，数据副本的传递策略就是采用消息广播模式。Leader 服务器将客户端事务请求转化成一个 Prososal（提议），并将该 Proposal 分发给集群中所有的 Follower 服务器。也就是向所有 Follower 节点发送数据广播请求（或数据复制）。\rZooKeeper 中数据副本的同步方式与二段提交相似，但是却又不同。二段提交要求协调者必须等到所有的参与者全部反馈 ACK 确认消息后，再发送 Commit 消息。要求所有的参与者要么全部成功，要么全部失败。二段提交会产生严重的阻塞问题。\r而在 ZAB 协议中 Leader 等待 Follower 的 ACK 反馈消息“只要半数以上的 Follower 成功反馈即可，不需要收到全部 Follower 的反馈”。 总结 # 整个 ZAB 协议一共定义了三个阶段： 发现：要求 ZooKeeper 集群必须选举出一个 Leader，同时 Leader 会维护一个 Follower 可用列表。将来客户端可以和这些 Follower 节点进行通信。 同步：Leader 要负责将本身的数据与 Follower 完成同步，做到多副本存储。这样便体现了 CAP 中的一致性和分区容错。Follower 将队列中未处理完的请求消费完成后，写入本地事务日志中。 广播：Leader 可以接受客户端新的事务 Proposal 请求，将新的 Proposal 请求广播给所有的 Follower。\n​\t三个阶段执行完为一个周期，在 ZooKeeper 集群的整个生命周期中，这三个阶段会不断进行，如果 Leader 崩溃或因其它原因导致 Leader缺失，ZAB 协议会再次进入阶段一\nZookeeper集群搭建 # Node01 # 解压\n[root@node01 ~]# tar -zxvf apache-zookeeper-3.6.3-bin.tar.gz -C /opt/yjx/ [root@node01 ~]# rm apache-zookeeper-3.6.3-bin.tar.gz -rf [root@node01 ~]# mkdir -p /var/yjx/zookeeper/data [root@node01 ~]# mkdir -p /opt/yjx/apache-zookeeper-3.6.3-bin/logs [root@node01 ~]# cd /opt/yjx/apache-zookeeper-3.6.3-bin/ [root@node01 apache-zookeeper-3.6.3-bin]# cp conf/zoo_sample.cfg conf/zoo.cfg [root@node01 apache-zookeeper-3.6.3-bin]# vim conf/zoo.cfg 创建数据目录、日志目录。\n[root@node01 ~]# mkdir -p /var/yjx/zookeeper/data\r[root@node01 ~]# mkdir -p /opt/yjx/apache-zookeeper-3.6.3-bin/logs 修改配置文件，ZooKeeper 启动时默认加载的配置文件名为 zoo.cfg 。\n[root@node01 ~]# cd /opt/yjx/apache-zookeeper-3.6.3-bin/\r[root@node01 apache-zookeeper-3.6.3-bin]# cp conf/zoo_sample.cfg conf/zoo.cfg\r[root@node01 apache-zookeeper-3.6.3-bin]# vim conf/zoo.cfg 主要修改以下内容（数字表示行号）：server.1 中的 1 是 myid 文件中的内容，2888 用于集群内部通信，3888 用于选举 Leader\n12 dataDir=/var/yjx/zookeeper/data\r13 dataLogDir=/opt/yjx/apache-zookeeper-3.6.3-bin/logs\r15 clientPort=2181\r37 server.1=node01:2888:3888\r38 server.2=node02:2888:3888\r39 server.3=node03:2888:3888 node02/03 # 接下来将 node01 的 ZooKeeper 所有文件拷贝至 node02 和 node03。推荐从 node02 和 node03 拷贝\n[root@node02 ~]# scp -r root@node01:/opt/yjx/apache-zookeeper-3.6.3-bin /opt/yjx/\r[root@node03 ~]# scp -r root@node01:/opt/yjx/apache-zookeeper-3.6.3-bin /opt/yjx/\r# 或者使用分发脚本\r[root@node01 ~]# yjxrsync /opt/yjx/apache-zookeeper-3.6.3-bin node01/02/03 # 然后在三台机器的 /var/yjx/zookeeper/data 目录下分别创建 myid 文件，内容分别为 1，2，3\nnode01：\n[root@node01 ~]# echo 1 \u0026gt; /var/yjx/zookeeper/data/myid node02：\n[root@node02 ~]# mkdir -p /var/yjx/zookeeper/data\r[root@node02 ~]# echo 2 \u0026gt; /var/yjx/zookeeper/data/myid node03：\n[root@node03 ~]# mkdir -p /var/yjx/zookeeper/data\r[root@node03 ~]# echo 3 \u0026gt; /var/yjx/zookeeper/data/myid 环境变量 # 最后 vim /etc/profile 配置环境变量，环境搭建结束。\nexport ZOOKEEPER_HOME=/opt/yjx/apache-zookeeper-3.6.3-bin\rexport PATH=$ZOOKEEPER_HOME/bin:$PATH 配完环境变量后 source /etc/profile 重新加载环境变量。\nZookeeper监听机制 # ","externalUrl":null,"permalink":"/docs/zookeeper/zookeeper/","section":"Docs","summary":"Zookeeper # https://www.","title":"","type":"docs"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]